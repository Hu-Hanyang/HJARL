diff --git a/ceshi_game.py b/ceshi_game.py
index 5994a62..d2c1509 100644
--- a/ceshi_game.py
+++ b/ceshi_game.py
@@ -1,9 +1,65 @@
+import  numpy as np
 from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
 
+initial_attacker = np.array([[-0.1, 0.0]])
+initial_defender = np.array([[0.0, 0.0]])
+env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
+# print(f"The initial state is {env.}. \n")
+# obs = env.reset()
+# print(f"The state space of the env is {env.observation_space}. \n")
+# print(f"The action space of the env is {env.action_space}. \n")
+# print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
 
-env = ReachAvoidGameEnv()
-obs = env.reset()
-print(f"The state space of the env is {env.observation_space}. \n")
-print(f"The action space of the env is {env.action_space}. \n")
-print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
\ No newline at end of file
+# obs = env.reset()
+# print(f"The state space of the env is {env.observation_space}. \n")
+# print(f"The action space of the env is {env.action_space}. \n")
+# print(f"The obs is {obs}. \n")
+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
+
+# for i in range(10):
+#     obs = env.reset()
+#     print(f"The initial player seed is {env.initial_players_seed}. \n")
+#     print(f"The obs is {obs}. \n")
+
+
+action = np.array([-0.1, 0.0])
+
+for i in range(10):
+    obs, reward, terminated, truncated, info = env.step(action)
+    print(f"The obs is {obs}. \n")
+    print(f"The reward is {reward}. \n")
+    # print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
+    if terminated or truncated:
+        print(f"The game is terminated: {terminated} and the game is truncated: {truncated} at the step {i}. \n")
+        break
+
+# current_attackers = np.array([[0.0, 0.0]])
+# current_defenders = np.array([[3.0, 4.0]])
+# distance = np.linalg.norm(current_attackers[0] - current_defenders[0])
+# print(f"The distance between the attacker and the defender is {distance}. \n")
+
+# obstacles = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}
+
+# def _check_area(state, area):
+#         """Check if the state is inside the area.
+
+#         Parameters:
+#             state (np.ndarray): the state to check
+#             area (dict): the area dictionary to be checked.
+        
+#         Returns:
+#             bool: True if the state is inside the area, False otherwise.
+#         """
+#         x, y = state  # Unpack the state assuming it's a 2D coordinate
+
+#         for bounds in area.values():
+#             x_lower, x_upper, y_lower, y_upper = bounds
+#             if x_lower <= x <= x_upper and y_lower <= y <= y_upper:
+#                 return True
+
+#         return False
+
+# print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
+# reward = 0.0
+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
\ No newline at end of file
diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
index 5e7b3f0..d1032b2 100644
--- a/safe_control_gym/envs/__init__.py
+++ b/safe_control_gym/envs/__init__.py
@@ -61,4 +61,7 @@ register(idx='quadrotor_random',
 
 register(idx='quadrotor_randomhj',
          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
-         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
\ No newline at end of file
+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
+
+register(idx='reach_avoid',
+         entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
\ No newline at end of file
diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
index 0ea88d5..956edfb 100644
--- a/safe_control_gym/envs/gym_game/BaseGame.py
+++ b/safe_control_gym/envs/gym_game/BaseGame.py
@@ -14,6 +14,7 @@ class Dynamics:
     SIG = {'id': 'sig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.0}           # Base single integrator dynamics
     FSIG = {'id': 'fsig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.5}         # Faster single integrator dynamics with feedback
     
+    
 class BaseGameEnv(gym.Env):
     """Base class for the multi-agent reach-avoid game Gym environments."""
     
@@ -26,6 +27,7 @@ class BaseGameEnv(gym.Env):
                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
                  ctrl_freq: int = 200,
                  seed: int = None,
+                 random_init: bool = True,
                  output_folder='results',
                  ):
         """Initialization of a generic aviary environment.
@@ -47,6 +49,7 @@ class BaseGameEnv(gym.Env):
         ctrl_freq : int, optional
             The control frequency of the environment.
         seed : int, optional
+        random_init : bool, optional
         output_folder : str, optional
             The folder where to save logs.
 
@@ -67,15 +70,12 @@ class BaseGameEnv(gym.Env):
         #### Input initial states ####################################
         self.init_attackers = initial_attacker
         self.init_defenders = initial_defender
-        #### Create action and observation spaces ##################
-        self.action_space = self._actionSpace()
-        self.observation_space = self._observationSpace()
         #### Housekeeping ##########################################
+        self.random_init = random_init
         self._housekeeping()
         #### Update and all players' information #####
         self._updateAndLog()
     
-    ################################################################################
 
     def _housekeeping(self):
         """Housekeeping function.
@@ -84,8 +84,10 @@ class BaseGameEnv(gym.Env):
         in the `reset()` function.
 
         """
-        if self.init_attackers is None and self.init_defenders is None:
-            self.init_attackers, self.init_defenders = self.initial_players()            
+        if self.random_init:
+            self.init_attackers, self.init_defenders = self.initial_players()
+        else:
+            assert self.init_attackers is not None and self.init_defenders is not None, "Need to provide initial positions for all players."     
         #### Set attackers and defenders ##########################
         self.attackers = make_agents(self.ATTACKER_PHYSICS, self.NUM_ATTACKERS, self.init_attackers, self.CTRL_FREQ)
         self.defenders = make_agents(self.DEFENDER_PHYSICS, self.NUM_DEFENDERS, self.init_defenders, self.CTRL_FREQ)
@@ -96,21 +98,26 @@ class BaseGameEnv(gym.Env):
         self.attackers_status = []  # 0 stands for free, -1 stands for captured, 1 stands for arrived 
         self.attackers_actions = []
         self.defenders_actions = []
+        # self.last_relative_distance = np.zeros((self.NUM_ATTACKERS, self.NUM_DEFENDERS))
 
-    ################################################################################
 
     def _updateAndLog(self):
         """Update and log all players' information after inialization, reset(), or step.
 
         """
         # Update the state
-        self.state = np.vstack([self.attackers._get_state().copy(), self.defenders._get_state().copy()])
+        current_attackers = self.attackers._get_state().copy()
+        current_defenders = self.defenders._get_state().copy()
+        
+        self.state = np.vstack([current_attackers, current_defenders])
         # Log the state and trajectory information
-        self.attackers_traj.append(self.attackers._get_state().copy())
-        self.defenders_traj.append(self.defenders._get_state().copy())
+        self.attackers_traj.append(current_attackers)
+        self.defenders_traj.append(current_defenders)
         self.attackers_status.append(self._getAttackersStatus().copy())
+        # for i in range(self.NUM_ATTACKERS):
+        #     for j in range(self.NUM_DEFENDERS):
+        #         self.last_relative_distance[i, j] = np.linalg.norm(current_attackers[i] - current_defenders[j])
     
-    ################################################################################
     
     def initial_players(self):
         '''Set the initial positions for all players.
@@ -122,7 +129,7 @@ class BaseGameEnv(gym.Env):
         np.random.seed(self.initial_players_seed)
     
         # Map boundaries
-        min_val, max_val = -1.0, 1.0
+        min_val, max_val = -0.99, 0.99
         
         # Obstacles and target areas
         obstacles = [
@@ -170,7 +177,6 @@ class BaseGameEnv(gym.Env):
         
         return np.array([attacker_pos]), np.array([defender_pos])
 
-    ################################################################################
     
     def reset(self, seed : int = None):
         """Resets the environment.
@@ -201,62 +207,6 @@ class BaseGameEnv(gym.Env):
         
         return obs
     
-    ################################################################################
-
-    def step(self,action):
-        #TODO: Hanyang: change the action only for the defender
-        """Advances the environment by one simulation step.
-
-        Parameters
-        ----------
-        action : ndarray | (dim_action, )
-            The input action for the defender.
-
-        Returns
-        -------
-        ndarray | dict[..]
-            The step's observation, check the specific implementation of `_computeObs()`
-            in each subclass for its format.
-        float | dict[..]
-            The step's reward value(s), check the specific implementation of `_computeReward()`
-            in each subclass for its format.
-        bool | dict[..]
-            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
-            in each subclass for its format.
-        bool | dict[..]
-            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
-            in each subclass for its format.
-        bool | dict[..]
-            Whether the current episode is trunacted, always false.
-        dict[..]
-            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-            in each subclass for its format.
-
-        """
-        
-        #### Step the simulation using the desired physics update ##        
-        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
-        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
-        self.attackers.step(attackers_action)
-        self.defenders.step(defenders_action)
-        #### Update and all players' information #####
-        self._updateAndLog()
-        #### Prepare the return values #############################
-        obs = self._computeObs()
-        reward = self._computeReward()
-        terminated = self._computeTerminated()
-        truncated = self._computeTruncated()
-        info = self._computeInfo()
-        
-        #### Advance the step counter ##############################
-        self.step_counter += 1
-        #### Log the actions taken by the attackers and defenders ################
-        self.attackers_actions.append(attackers_action)
-        self.defenders_actions.append(defenders_action)
-        
-        return obs, reward, terminated, truncated, info
-    
-    ################################################################################
 
     def _getAttackersStatus(self):
         """Returns the current status of all attackers.
@@ -266,37 +216,15 @@ class BaseGameEnv(gym.Env):
         """
         raise NotImplementedError
     
-    ################################################################################
-    
-    def _actionSpace(self):
-        """Returns the action space of the environment.
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-           
-    ################################################################################
-
-    def _observationSpace(self):
-        """Returns the observation space of the environment.
-
-        Must be implemented in a subclass.
-
-        """
-        raise NotImplementedError
-    
-    ################################################################################
     
     def _computeObs(self):
         """Returns the current observation of the environment.
 
-        Must be implemented in a subclass.
-
         """
-        raise NotImplementedError
+        obs = self.state.flatten()
+        
+        return obs
     
-    ################################################################################
 
     def _computeReward(self):
         """Computes the current reward value(s).
@@ -311,7 +239,6 @@ class BaseGameEnv(gym.Env):
         """
         raise NotImplementedError
 
-    ################################################################################
 
     def _computeTerminated(self):
         """Computes the current terminated value(s).
@@ -321,7 +248,6 @@ class BaseGameEnv(gym.Env):
         """
         raise NotImplementedError
     
-    ################################################################################
 
     def _computeTruncated(self):
         """Computes the current truncated value(s).
@@ -331,22 +257,11 @@ class BaseGameEnv(gym.Env):
         """
         raise NotImplementedError
 
-    ################################################################################
 
     def _computeInfo(self):
         """Computes the current info dict(s).
 
         Must be implemented in a subclass.
 
-        """
-        raise NotImplementedError
-
-    ################################################################################
-
-    def _computeAttackerActions(self):
-        """Computes the current actions of the attackers.
-
-        Must be implemented in a subclass.
-
         """
         raise NotImplementedError
\ No newline at end of file
diff --git a/safe_control_gym/envs/gym_game/BaseRLGame.py b/safe_control_gym/envs/gym_game/BaseRLGame.py
index cdd42ca..c62dd6e 100644
--- a/safe_control_gym/envs/gym_game/BaseRLGame.py
+++ b/safe_control_gym/envs/gym_game/BaseRLGame.py
@@ -26,6 +26,7 @@ class BaseRLGameEnv(BaseGameEnv):
                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
                  ctrl_freq: int = 200,
                  seed = 42,
+                 random_init = True,
                  output_folder='results',
                  ):
         """Initialization of a generic aviary environment.
@@ -47,6 +48,7 @@ class BaseRLGameEnv(BaseGameEnv):
         ctrl_freq : int, optional
             The control frequency of the environment.
         seed : int, optional
+        random_init: bool, optional
         output_folder : str, optional
             The folder where to save logs.
 
@@ -55,32 +57,33 @@ class BaseRLGameEnv(BaseGameEnv):
         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
-                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
                          )
+        
+        #### Create action and observation spaces ##################
+        self.action_space = self._actionSpace()
+        self.observation_space = self._observationSpace()
        
-
-    ################################################################################
-
    
     def _actionSpace(self):
         """Returns the action space of the environment.
-        Formulation: [attackers' action spaces, defenders' action spaces]
+        Formulation: [defenders' action spaces]
         Returns
         -------
         spaces.Box
-            A Box of size NUM_PLAYERS x 2, or 1, depending on the action type.
+            A Box of size NUM_DEFENDERS x 2, or 1, depending on the action type.
 
         """
         
-        if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
-            attacker_lower_bound = np.array([-1.0, -1.0])
-            attacker_upper_bound = np.array([+1.0, +1.0])
-        elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
-            attacker_lower_bound = np.array([-1.0])
-            attacker_upper_bound = np.array([+1.0])
-        else:
-            print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
-            exit()
+        # if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
+        #     attacker_lower_bound = np.array([-1.0, -1.0])
+        #     attacker_upper_bound = np.array([+1.0, +1.0])
+        # elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
+        #     attacker_lower_bound = np.array([-1.0])
+        #     attacker_upper_bound = np.array([+1.0])
+        # else:
+        #     print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
+        #     exit()
         
         if self.DEFENDER_PHYSICS == Dynamics.SIG or self.DEFENDER_PHYSICS == Dynamics.FSIG:
             defender_lower_bound = np.array([-1.0, -1.0])
@@ -92,28 +95,28 @@ class BaseRLGameEnv(BaseGameEnv):
             print("[ERROR] in Defender Action Space, BaseRLGameEnv._actionSpace()")
             exit()
         
-        attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
-        attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
+        # attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
+        # attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
 
-        if self.NUM_DEFENDERS > 0:
-            defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-            defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
+        # if self.NUM_DEFENDERS > 0:
+        #     defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
+        #     defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
             
-            act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
-            act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
-        else:
-            act_lower_bound = attackers_lower_bound
-            act_upper_bound = attackers_upper_bound
-
+        #     act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
+        #     act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
+        # else:
+        #     act_lower_bound = attackers_lower_bound
+        #     act_upper_bound = attackers_upper_bound
+            
+        defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
+        defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
         # Flatten the lower and upper bounds to ensure the action space shape is (4,)
-        act_lower_bound = act_lower_bound.flatten()
-        act_upper_bound = act_upper_bound.flatten()
+        act_lower_bound = defenders_lower_bound.flatten()
+        act_upper_bound = defenders_upper_bound.flatten()
 
         return spaces.Box(low=act_lower_bound, high=act_upper_bound, dtype=np.float32)
  
 
-    ################################################################################
-
     def _observationSpace(self):
         """Returns the observation space of the environment.
         Formulation: [attackers' obs spaces, defenders' obs spaces]
diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
index b5344ff..0ea357c 100644
--- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
@@ -24,13 +24,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
                  ctrl_freq: int = 200,
                  seed = 42,
+                 random_init = True,
                  uMode="min", 
                  dMode="max",
                  output_folder='results',
                  game_length_sec=20,
-                 map={'map': [-1., 1., -1., 1.]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
+                 map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                 obstacles: dict = None,  
+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
                  ):
         """Initialization of a generic aviary environment.
 
@@ -51,6 +52,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         ctrl_freq : int, optional
             The control frequency of the environment.
         seed : int, optional
+        random_init: bool, optional
         uMode : str, optional
             The mode of the attacker, default is "min".
         dMode : str, optional
@@ -71,7 +73,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
-                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
                          )
         
         assert map is not None, "Map must be provided in the game."
@@ -84,13 +86,82 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         self.uMode = uMode
         self.dMode = dMode
         # Load necessary values for the attacker control
-        #TODO: Hanyang: not finished
         self.grid1vs0 = Grid(np.array([-1.0, -1.0]), np.array([1.0, 1.0]), 2, np.array([100, 100])) 
-        self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
-        self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
+        # self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
+        # self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
         self.value1vs0 = np.load('safe_control_gym/envs/gym_game/values/1vs0Attacker.npy')
 
-    ################################################################################
+    
+    def step(self, action):
+        """Advances the environment by one simulation step.
+
+        Parameters
+        ----------
+        action : ndarray | (dim_action, )
+            The input action for the defender.
+
+        Returns
+        -------
+        ndarray | dict[..]
+            The step's observation, check the specific implementation of `_computeObs()`
+            in each subclass for its format.
+        float | dict[..]
+            The step's reward value(s), check the specific implementation of `_computeReward()`
+            in each subclass for its format.
+        bool | dict[..]
+            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
+            in each subclass for its format.
+        bool | dict[..]
+            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
+            in each subclass for its format.
+        bool | dict[..]
+            Whether the current episode is trunacted, always false.
+        dict[..]
+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
+            in each subclass for its format.
+
+        """
+        
+        #### Step the simulation using the desired physics update ##        
+        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
+        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
+        self.attackers.step(attackers_action)
+        self.defenders.step(defenders_action)
+        #### Update and all players' information #####
+        self._updateAndLog()
+        #### Prepare the return values #############################
+        obs = self._computeObs()
+        reward = self._computeReward()
+        terminated = self._computeTerminated()
+        truncated = self._computeTruncated()
+        info = self._computeInfo()
+        
+        #### Advance the step counter ##############################
+        self.step_counter += 1
+        #### Log the actions taken by the attackers and defenders ################
+        self.attackers_actions.append(attackers_action)
+        self.defenders_actions.append(defenders_action)
+        
+        return obs, reward, terminated, truncated, info
+    
+    
+    def _computeAttackerActions(self):
+        """Computes the current actions of the attackers.
+
+        Must be implemented in a subclass.
+
+        """
+        current_attacker_state = self.attackers._get_state().copy()
+        control_attackers = np.zeros((self.NUM_ATTACKERS, 2))
+        for i in range(self.NUM_ATTACKERS):
+            neg2pos, pos2neg = find_sign_change1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i])
+            if len(neg2pos):
+                control_attackers[i] = self.attacker_control_1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i], neg2pos)
+            else:
+                control_attackers[i] = (0.0, 0.0)
+                
+        return control_attackers
+    
     
     def _getAttackersStatus(self):
         """Returns the current status of all attackers.
@@ -126,7 +197,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
                                 break
 
             return new_status
-    ################################################################################
+        
 
     def _check_area(self, state, area):
         """Check if the state is inside the area.
@@ -147,7 +218,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
 
         return False
     
-    ################################################################################
 
     def _computeObs(self):
         """Returns the current observation of the environment.
@@ -162,16 +232,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
 
         return obs
     
-    ################################################################################
     
     def _computeReward(self):
-        #TODO: Hanyang: not finished
         """Computes the current reward value.
 
-        One attacker is captured: +100
-        One attacker arrived at the goal: -100
+        Once the attacker is captured: +100
+        Once the attacker arrived at the goal: -100
         The defender hits the obstacle: -100
-        One step and nothing happens: 
+        One step and nothing happens: maybe use the distance between the attacker and the defender as a sign?
         In status, 0 stands for free, -1 stands for captured, 1 stands for arrived
 
         Returns
@@ -182,15 +250,24 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         """
         last_attacker_status = self.attackers_status[-2]
         current_attacker_status = self.attackers_status[-1]
-        reward = -1.0
+        reward = 0.0
+        # check the attacker status
         for num in range(self.NUM_ATTACKERS):
-            reward += (current_attacker_status[num] - last_attacker_status[num]) * -10
-            
+            reward += (current_attacker_status[num] - last_attacker_status[num]) * (-200)
+        # check the defender status
+        current_defender_state = self.defenders._get_state().copy()
+        reward += -200 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
+        # check the relative distance difference or relative distance
+        current_attacker_state = self.attackers._get_state().copy()
+        current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
+        last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
+        # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
+        reward += -current_relative_distance
+        
         return reward
 
     
     def _computeTerminated(self):
-        #TODO: Hanyang: not finished
         """Computes the current done value.
         done = True if all attackers have arrived or been captured.
 
@@ -200,9 +277,15 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
             Whether the current episode is done.
 
         """
-        
+        # defender hits the obstacle or the attacker is captured or the attacker has arrived or the attacker hits the obstacle
+        # check the attacker status
         current_attacker_status = self.attackers_status[-1]
-        done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
+        attacker_done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
+        # check the defender status: hit the obstacle, or the attacker is captured
+        current_defender_state = self.defenders._get_state().copy()
+        defender_done = self._check_area(current_defender_state[0], self.obstacles)
+        # final done
+        done = True if attacker_done or defender_done else False
         
         return done
         
@@ -223,7 +306,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
 
     
     def _computeInfo(self):
-        #TODO: Hanyang: not finished
         """Computes the current info dict(s).
 
         Unused.
@@ -240,10 +322,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         
         return info 
     
-    ################################################################################
 
     def _computeAttackerActions(self):
-        #TODO: Hanyang: not finished
         """Computes the current actions of the attackers.
 
         """
@@ -296,9 +376,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
 
         return (opt_a1, opt_a2)
     
-    ################################################################################
     
-    def optCrtl_1vs1(self, spat_deriv):
+    def optCtrl_1vs1(self, spat_deriv):
         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 1 game.
         
         Parameters:
@@ -329,7 +408,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
 
         return (opt_u1, opt_u2)
 
-    ################################################################################
 
     def optCtrl_1vs0(self, spat_deriv):
         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 0 game.
@@ -360,50 +438,4 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
                 opt_a1 = self.attackers.speed * deriv1 / ctrl_len
                 opt_a2 = self.attackers.speed * deriv2 / ctrl_len
 
-        return (opt_a1, opt_a2)
-        """Computes the optimal control (disturbance) for the attacker in a 1 vs. 2 game.
-        
-        Parameters:
-            spat_deriv (tuple): spatial derivative in all dimensions
-        
-        Returns:
-            tuple: a tuple of optimal control of the defender (disturbances)
-        """
-        opt_d1 = self.defenders.uMax
-        opt_d2 = self.defenders.uMax
-        opt_d3 = self.defenders.uMax
-        opt_d4 = self.defenders.uMax
-        deriv3 = spat_deriv[2]
-        deriv4 = spat_deriv[3]
-        deriv5 = spat_deriv[4]
-        deriv6 = spat_deriv[5]
-        distb_len1 = np.sqrt(deriv3*deriv3 + deriv4*deriv4)
-        distb_len2 = np.sqrt(deriv5*deriv5 + deriv6*deriv6)
-        if self.dMode == "max":
-            if distb_len1 == 0:
-                opt_d1 = 0.0
-                opt_d2 = 0.0
-            else:
-                opt_d1 = self.defenders.speed*deriv3 / distb_len1
-                opt_d2 = self.defenders.speed*deriv4 / distb_len1
-            if distb_len2 == 0:
-                opt_d3 = 0.0
-                opt_d4 = 0.0
-            else:
-                opt_d3 = self.defenders.speed*deriv5 / distb_len2
-                opt_d4 = self.defenders.speed*deriv6 / distb_len2
-        else:
-            if distb_len1 == 0:
-                opt_d1 = 0.0
-                opt_d2 = 0.0
-            else:
-                opt_d1 = -self.defenders.speed*deriv3 / distb_len1
-                opt_d2 = -self.defenders.speed*deriv4 / distb_len1
-            if distb_len2 == 0:
-                opt_d3 = 0.0
-                opt_d4 = 0.0
-            else:
-                opt_d3 = -self.defenders.speed*deriv5 / distb_len2
-                opt_d4 = -self.defenders.speed*deriv6 / distb_len2
-
-        return (opt_d1, opt_d2, opt_d3, opt_d4)
\ No newline at end of file
+        return (opt_a1, opt_a2)
\ No newline at end of file
