diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
index 4dc12e1..f915f41 100644
--- a/safe_control_gym/envs/gym_game/BaseGame.py
+++ b/safe_control_gym/envs/gym_game/BaseGame.py
@@ -128,7 +128,6 @@ class BaseGameEnv(gym.Env):
             defenders (np.ndarray): the initial positions of the defenders
         '''
         np.random.seed(self.initial_players_seed)
-        print(f"========== self.call_counter: {self.call_counter} in BaseGame.py. ==========\n")
         # Map boundaries
         min_val, max_val = -0.9, 0.9
         
@@ -192,18 +191,33 @@ class BaseGameEnv(gym.Env):
                     return (new_point_x, new_point_y)
         
         # Calculate desired distance based on the counter
-        if self.call_counter < 500:  # 2e5 steps 
+        stage = -1
+        if self.call_counter < 3000:  # [0.10, 0.20]
             distance = 0.15
             r = 0.05
-        elif self.call_counter < 1000:  # around 4e5 steps
+            stage = 0
+        elif self.call_counter < 7000:  # [0.20, 0.50]
             distance = 0.35
             r = 0.15
-        elif self.call_counter < 1800:
+            stage = 1
+        elif self.call_counter < 12000:  # [0.50, 1.00]
             distance = 0.75
             r = 0.25
-        else:
+            stage = 2
+        elif self.call_counter < 18000:  # [1.00, 2.00]
+            distance = 1.50
+            r = 0.50
+            stage = 3
+        elif self.call_counter < 25000:  # [2.00, 2.80]
+            distance = 2.40
+            r = 0.40
+            stage = 4
+        else:  # [0.10, 2.80]
             distance = 1.45
             r = 1.35
+            stage = 5
+        
+        print(f"========== self.call_counter: {self.call_counter} and stage: {stage} in BaseGame.py. ==========\n")
         
         attacker_seed = self.initial_players_seed
         defender_seed = self.initial_players_seed + 1
diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
index 78b8656..faa6980 100644
--- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
@@ -34,7 +34,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
                  game_length_sec=10,
                  map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
                  ):
         """Initialization of a generic aviary environment.
 
@@ -261,20 +261,20 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
         # reward 2:
         status_change = current_attacker_status[0] - last_attacker_status[0]
         if status_change == 1:  # attacker arrived
-            reward += -500
+            reward += -100
         elif status_change == -1:  # attacker is captured
-            reward += 500
+            reward += 100
         else:  # attacker is free
             reward += 0.0
         # check the defender status
         current_defender_state = self.defenders._get_state().copy()
-        reward += -500 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
+        reward += -100 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
         # check the relative distance difference or relative distance
         current_attacker_state = self.attackers._get_state().copy()
         current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
         # last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
         # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
-        reward += -(current_relative_distance*5)
+        reward += -(current_relative_distance)
         
         return reward
 
diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
index e991d35..23c923e 100644
--- a/safe_control_gym/experiments/train_game.py
+++ b/safe_control_gym/experiments/train_game.py
@@ -45,7 +45,7 @@ class Args:
     # Algorithm specific arguments
     env_id: str = "reach_avoid"
     """the id of the environment"""
-    total_timesteps: int = 2e7
+    total_timesteps: int = 1e7
     """total timesteps of the experiments"""
     learning_rate: float = 3e-4
     """the learning rate of the optimizer"""
diff --git a/training_results/game/ppo/42/20000000.0/events.out.tfevents.1719259765.cs-mars-14.16617.0 b/training_results/game/ppo/42/20000000.0/events.out.tfevents.1719259765.cs-mars-14.16617.0
deleted file mode 100644
index d11bbaf..0000000
Binary files a/training_results/game/ppo/42/20000000.0/events.out.tfevents.1719259765.cs-mars-14.16617.0 and /dev/null differ
diff --git a/training_results/game/ppo/42/20000000.0/reward6.png b/training_results/game/ppo/42/20000000.0/reward6.png
deleted file mode 100644
index 460b1ad..0000000
Binary files a/training_results/game/ppo/42/20000000.0/reward6.png and /dev/null differ
diff --git a/training_results/game/ppo/42/20000000.0/reward6_init.png b/training_results/game/ppo/42/20000000.0/reward6_init.png
deleted file mode 100644
index 96345f7..0000000
Binary files a/training_results/game/ppo/42/20000000.0/reward6_init.png and /dev/null differ
diff --git a/training_results/game/ppo/42/20000000.0/train_game.cleanrl_model b/training_results/game/ppo/42/20000000.0/train_game.cleanrl_model
deleted file mode 100644
index 8d7c8ce..0000000
Binary files a/training_results/game/ppo/42/20000000.0/train_game.cleanrl_model and /dev/null differ
diff --git a/wandb/latest-run b/wandb/latest-run
index b7e7f2f..85cd6ea 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240624_130921-tkjintz8
\ No newline at end of file
+run-20240626_122706-6hu5vmst
\ No newline at end of file
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/code/safe_control_gym/experiments/train_game_controller.py b/wandb/run-20240615_110134-fpms1nm3/files/code/safe_control_gym/experiments/train_game_controller.py
deleted file mode 100644
index 7211aae..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/code/safe_control_gym/experiments/train_game_controller.py
+++ /dev/null
@@ -1,362 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e6
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/' +f'{args.seed}' + f'{datetime.now().strftime("%Y.%m.%d_%H:%M")}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"runs/{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-        # from cleanrl_utils.evals.ppo_eval import evaluate
-
-        # episodic_returns = evaluate(
-        #     model_path,
-        #     make_env,
-        #     args.env_id,
-        #     eval_episodes=10,
-        #     run_name=f"{run_name}-eval",
-        #     Model=Agent,
-        #     device=device,
-        #     gamma=args.gamma,
-        # )
-        # for idx, episodic_return in enumerate(episodic_returns):
-        #     writer.add_scalar("eval/episodic_return", episodic_return, idx)
-
-        # if args.upload_model:
-        #     from cleanrl_utils.huggingface import push_to_hub
-
-        #     repo_name = f"{args.env_id}-{args.exp_name}-seed{args.seed}"
-        #     repo_id = f"{args.hf_entity}/{repo_name}" if args.hf_entity else repo_name
-        #     push_to_hub(args, episodic_returns, repo_id, "PPO", f"runs/{run_name}", f"videos/{run_name}-eval")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/conda-environment.yaml b/wandb/run-20240615_110134-fpms1nm3/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/config.yaml b/wandb/run-20240615_110134-fpms1nm3/files/config.yaml
deleted file mode 100644
index 750971f..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game_controller
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 1000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 244.0
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game_controller.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718474495.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/diff.patch b/wandb/run-20240615_110134-fpms1nm3/files/diff.patch
deleted file mode 100644
index d29e714..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/diff.patch
+++ /dev/null
@@ -1,758 +0,0 @@
-diff --git a/ceshi_game.py b/ceshi_game.py
-index 5994a62..d2c1509 100644
---- a/ceshi_game.py
-+++ b/ceshi_game.py
-@@ -1,9 +1,65 @@
-+import  numpy as np
- from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
- 
-+initial_attacker = np.array([[-0.1, 0.0]])
-+initial_defender = np.array([[0.0, 0.0]])
-+env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
-+# print(f"The initial state is {env.}. \n")
-+# obs = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
- 
--env = ReachAvoidGameEnv()
--obs = env.reset()
--print(f"The state space of the env is {env.observation_space}. \n")
--print(f"The action space of the env is {env.action_space}. \n")
--print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
--print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-\ No newline at end of file
-+# obs = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+
-+# for i in range(10):
-+#     obs = env.reset()
-+#     print(f"The initial player seed is {env.initial_players_seed}. \n")
-+#     print(f"The obs is {obs}. \n")
-+
-+
-+action = np.array([-0.1, 0.0])
-+
-+for i in range(10):
-+    obs, reward, terminated, truncated, info = env.step(action)
-+    print(f"The obs is {obs}. \n")
-+    print(f"The reward is {reward}. \n")
-+    # print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+    if terminated or truncated:
-+        print(f"The game is terminated: {terminated} and the game is truncated: {truncated} at the step {i}. \n")
-+        break
-+
-+# current_attackers = np.array([[0.0, 0.0]])
-+# current_defenders = np.array([[3.0, 4.0]])
-+# distance = np.linalg.norm(current_attackers[0] - current_defenders[0])
-+# print(f"The distance between the attacker and the defender is {distance}. \n")
-+
-+# obstacles = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}
-+
-+# def _check_area(state, area):
-+#         """Check if the state is inside the area.
-+
-+#         Parameters:
-+#             state (np.ndarray): the state to check
-+#             area (dict): the area dictionary to be checked.
-+        
-+#         Returns:
-+#             bool: True if the state is inside the area, False otherwise.
-+#         """
-+#         x, y = state  # Unpack the state assuming it's a 2D coordinate
-+
-+#         for bounds in area.values():
-+#             x_lower, x_upper, y_lower, y_upper = bounds
-+#             if x_lower <= x <= x_upper and y_lower <= y <= y_upper:
-+#                 return True
-+
-+#         return False
-+
-+# print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
-+# reward = 0.0
-+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
-index 5e7b3f0..d1032b2 100644
---- a/safe_control_gym/envs/__init__.py
-+++ b/safe_control_gym/envs/__init__.py
-@@ -61,4 +61,7 @@ register(idx='quadrotor_random',
- 
- register(idx='quadrotor_randomhj',
-          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
--         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-\ No newline at end of file
-+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-+
-+register(idx='reach_avoid',
-+         entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 0ea88d5..956edfb 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -14,6 +14,7 @@ class Dynamics:
-     SIG = {'id': 'sig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.0}           # Base single integrator dynamics
-     FSIG = {'id': 'fsig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.5}         # Faster single integrator dynamics with feedback
-     
-+    
- class BaseGameEnv(gym.Env):
-     """Base class for the multi-agent reach-avoid game Gym environments."""
-     
-@@ -26,6 +27,7 @@ class BaseGameEnv(gym.Env):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed: int = None,
-+                 random_init: bool = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +49,7 @@ class BaseGameEnv(gym.Env):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init : bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -67,15 +70,12 @@ class BaseGameEnv(gym.Env):
-         #### Input initial states ####################################
-         self.init_attackers = initial_attacker
-         self.init_defenders = initial_defender
--        #### Create action and observation spaces ##################
--        self.action_space = self._actionSpace()
--        self.observation_space = self._observationSpace()
-         #### Housekeeping ##########################################
-+        self.random_init = random_init
-         self._housekeeping()
-         #### Update and all players' information #####
-         self._updateAndLog()
-     
--    ################################################################################
- 
-     def _housekeeping(self):
-         """Housekeeping function.
-@@ -84,8 +84,10 @@ class BaseGameEnv(gym.Env):
-         in the `reset()` function.
- 
-         """
--        if self.init_attackers is None and self.init_defenders is None:
--            self.init_attackers, self.init_defenders = self.initial_players()            
-+        if self.random_init:
-+            self.init_attackers, self.init_defenders = self.initial_players()
-+        else:
-+            assert self.init_attackers is not None and self.init_defenders is not None, "Need to provide initial positions for all players."     
-         #### Set attackers and defenders ##########################
-         self.attackers = make_agents(self.ATTACKER_PHYSICS, self.NUM_ATTACKERS, self.init_attackers, self.CTRL_FREQ)
-         self.defenders = make_agents(self.DEFENDER_PHYSICS, self.NUM_DEFENDERS, self.init_defenders, self.CTRL_FREQ)
-@@ -96,21 +98,26 @@ class BaseGameEnv(gym.Env):
-         self.attackers_status = []  # 0 stands for free, -1 stands for captured, 1 stands for arrived 
-         self.attackers_actions = []
-         self.defenders_actions = []
-+        # self.last_relative_distance = np.zeros((self.NUM_ATTACKERS, self.NUM_DEFENDERS))
- 
--    ################################################################################
- 
-     def _updateAndLog(self):
-         """Update and log all players' information after inialization, reset(), or step.
- 
-         """
-         # Update the state
--        self.state = np.vstack([self.attackers._get_state().copy(), self.defenders._get_state().copy()])
-+        current_attackers = self.attackers._get_state().copy()
-+        current_defenders = self.defenders._get_state().copy()
-+        
-+        self.state = np.vstack([current_attackers, current_defenders])
-         # Log the state and trajectory information
--        self.attackers_traj.append(self.attackers._get_state().copy())
--        self.defenders_traj.append(self.defenders._get_state().copy())
-+        self.attackers_traj.append(current_attackers)
-+        self.defenders_traj.append(current_defenders)
-         self.attackers_status.append(self._getAttackersStatus().copy())
-+        # for i in range(self.NUM_ATTACKERS):
-+        #     for j in range(self.NUM_DEFENDERS):
-+        #         self.last_relative_distance[i, j] = np.linalg.norm(current_attackers[i] - current_defenders[j])
-     
--    ################################################################################
-     
-     def initial_players(self):
-         '''Set the initial positions for all players.
-@@ -122,7 +129,7 @@ class BaseGameEnv(gym.Env):
-         np.random.seed(self.initial_players_seed)
-     
-         # Map boundaries
--        min_val, max_val = -1.0, 1.0
-+        min_val, max_val = -0.99, 0.99
-         
-         # Obstacles and target areas
-         obstacles = [
-@@ -170,7 +177,6 @@ class BaseGameEnv(gym.Env):
-         
-         return np.array([attacker_pos]), np.array([defender_pos])
- 
--    ################################################################################
-     
-     def reset(self, seed : int = None):
-         """Resets the environment.
-@@ -201,62 +207,6 @@ class BaseGameEnv(gym.Env):
-         
-         return obs
-     
--    ################################################################################
--
--    def step(self,action):
--        #TODO: Hanyang: change the action only for the defender
--        """Advances the environment by one simulation step.
--
--        Parameters
--        ----------
--        action : ndarray | (dim_action, )
--            The input action for the defender.
--
--        Returns
--        -------
--        ndarray | dict[..]
--            The step's observation, check the specific implementation of `_computeObs()`
--            in each subclass for its format.
--        float | dict[..]
--            The step's reward value(s), check the specific implementation of `_computeReward()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is trunacted, always false.
--        dict[..]
--            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
--            in each subclass for its format.
--
--        """
--        
--        #### Step the simulation using the desired physics update ##        
--        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
--        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
--        self.attackers.step(attackers_action)
--        self.defenders.step(defenders_action)
--        #### Update and all players' information #####
--        self._updateAndLog()
--        #### Prepare the return values #############################
--        obs = self._computeObs()
--        reward = self._computeReward()
--        terminated = self._computeTerminated()
--        truncated = self._computeTruncated()
--        info = self._computeInfo()
--        
--        #### Advance the step counter ##############################
--        self.step_counter += 1
--        #### Log the actions taken by the attackers and defenders ################
--        self.attackers_actions.append(attackers_action)
--        self.defenders_actions.append(defenders_action)
--        
--        return obs, reward, terminated, truncated, info
--    
--    ################################################################################
- 
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -266,37 +216,15 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
--    
--    def _actionSpace(self):
--        """Returns the action space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--           
--    ################################################################################
--
--    def _observationSpace(self):
--        """Returns the observation space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--    
--    ################################################################################
-     
-     def _computeObs(self):
-         """Returns the current observation of the environment.
- 
--        Must be implemented in a subclass.
--
-         """
--        raise NotImplementedError
-+        obs = self.state.flatten()
-+        
-+        return obs
-     
--    ################################################################################
- 
-     def _computeReward(self):
-         """Computes the current reward value(s).
-@@ -311,7 +239,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeTerminated(self):
-         """Computes the current terminated value(s).
-@@ -321,7 +248,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
- 
-     def _computeTruncated(self):
-         """Computes the current truncated value(s).
-@@ -331,22 +257,11 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeInfo(self):
-         """Computes the current info dict(s).
- 
-         Must be implemented in a subclass.
- 
--        """
--        raise NotImplementedError
--
--    ################################################################################
--
--    def _computeAttackerActions(self):
--        """Computes the current actions of the attackers.
--
--        Must be implemented in a subclass.
--
-         """
-         raise NotImplementedError
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseRLGame.py b/safe_control_gym/envs/gym_game/BaseRLGame.py
-index cdd42ca..c62dd6e 100644
---- a/safe_control_gym/envs/gym_game/BaseRLGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseRLGame.py
-@@ -26,6 +26,7 @@ class BaseRLGameEnv(BaseGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +48,7 @@ class BaseRLGameEnv(BaseGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -55,32 +57,33 @@ class BaseRLGameEnv(BaseGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-+        
-+        #### Create action and observation spaces ##################
-+        self.action_space = self._actionSpace()
-+        self.observation_space = self._observationSpace()
-        
--
--    ################################################################################
--
-    
-     def _actionSpace(self):
-         """Returns the action space of the environment.
--        Formulation: [attackers' action spaces, defenders' action spaces]
-+        Formulation: [defenders' action spaces]
-         Returns
-         -------
-         spaces.Box
--            A Box of size NUM_PLAYERS x 2, or 1, depending on the action type.
-+            A Box of size NUM_DEFENDERS x 2, or 1, depending on the action type.
- 
-         """
-         
--        if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
--            attacker_lower_bound = np.array([-1.0, -1.0])
--            attacker_upper_bound = np.array([+1.0, +1.0])
--        elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
--            attacker_lower_bound = np.array([-1.0])
--            attacker_upper_bound = np.array([+1.0])
--        else:
--            print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
--            exit()
-+        # if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
-+        #     attacker_lower_bound = np.array([-1.0, -1.0])
-+        #     attacker_upper_bound = np.array([+1.0, +1.0])
-+        # elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
-+        #     attacker_lower_bound = np.array([-1.0])
-+        #     attacker_upper_bound = np.array([+1.0])
-+        # else:
-+        #     print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
-+        #     exit()
-         
-         if self.DEFENDER_PHYSICS == Dynamics.SIG or self.DEFENDER_PHYSICS == Dynamics.FSIG:
-             defender_lower_bound = np.array([-1.0, -1.0])
-@@ -92,28 +95,28 @@ class BaseRLGameEnv(BaseGameEnv):
-             print("[ERROR] in Defender Action Space, BaseRLGameEnv._actionSpace()")
-             exit()
-         
--        attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
--        attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
- 
--        if self.NUM_DEFENDERS > 0:
--            defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
--            defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-+        # if self.NUM_DEFENDERS > 0:
-+        #     defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        #     defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-             
--            act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
--            act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
--        else:
--            act_lower_bound = attackers_lower_bound
--            act_upper_bound = attackers_upper_bound
--
-+        #     act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
-+        #     act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
-+        # else:
-+        #     act_lower_bound = attackers_lower_bound
-+        #     act_upper_bound = attackers_upper_bound
-+            
-+        defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-         # Flatten the lower and upper bounds to ensure the action space shape is (4,)
--        act_lower_bound = act_lower_bound.flatten()
--        act_upper_bound = act_upper_bound.flatten()
-+        act_lower_bound = defenders_lower_bound.flatten()
-+        act_upper_bound = defenders_upper_bound.flatten()
- 
-         return spaces.Box(low=act_lower_bound, high=act_upper_bound, dtype=np.float32)
-  
- 
--    ################################################################################
--
-     def _observationSpace(self):
-         """Returns the observation space of the environment.
-         Formulation: [attackers' obs spaces, defenders' obs spaces]
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index b5344ff..0ea357c 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -24,13 +24,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  uMode="min", 
-                  dMode="max",
-                  output_folder='results',
-                  game_length_sec=20,
--                 map={'map': [-1., 1., -1., 1.]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-+                 map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
--                 obstacles: dict = None,  
-+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  ):
-         """Initialization of a generic aviary environment.
- 
-@@ -51,6 +52,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         uMode : str, optional
-             The mode of the attacker, default is "min".
-         dMode : str, optional
-@@ -71,7 +73,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-         
-         assert map is not None, "Map must be provided in the game."
-@@ -84,13 +86,82 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         self.uMode = uMode
-         self.dMode = dMode
-         # Load necessary values for the attacker control
--        #TODO: Hanyang: not finished
-         self.grid1vs0 = Grid(np.array([-1.0, -1.0]), np.array([1.0, 1.0]), 2, np.array([100, 100])) 
--        self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
--        self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-+        # self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
-+        # self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-         self.value1vs0 = np.load('safe_control_gym/envs/gym_game/values/1vs0Attacker.npy')
- 
--    ################################################################################
-+    
-+    def step(self, action):
-+        """Advances the environment by one simulation step.
-+
-+        Parameters
-+        ----------
-+        action : ndarray | (dim_action, )
-+            The input action for the defender.
-+
-+        Returns
-+        -------
-+        ndarray | dict[..]
-+            The step's observation, check the specific implementation of `_computeObs()`
-+            in each subclass for its format.
-+        float | dict[..]
-+            The step's reward value(s), check the specific implementation of `_computeReward()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is trunacted, always false.
-+        dict[..]
-+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-+            in each subclass for its format.
-+
-+        """
-+        
-+        #### Step the simulation using the desired physics update ##        
-+        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
-+        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
-+        self.attackers.step(attackers_action)
-+        self.defenders.step(defenders_action)
-+        #### Update and all players' information #####
-+        self._updateAndLog()
-+        #### Prepare the return values #############################
-+        obs = self._computeObs()
-+        reward = self._computeReward()
-+        terminated = self._computeTerminated()
-+        truncated = self._computeTruncated()
-+        info = self._computeInfo()
-+        
-+        #### Advance the step counter ##############################
-+        self.step_counter += 1
-+        #### Log the actions taken by the attackers and defenders ################
-+        self.attackers_actions.append(attackers_action)
-+        self.defenders_actions.append(defenders_action)
-+        
-+        return obs, reward, terminated, truncated, info
-+    
-+    
-+    def _computeAttackerActions(self):
-+        """Computes the current actions of the attackers.
-+
-+        Must be implemented in a subclass.
-+
-+        """
-+        current_attacker_state = self.attackers._get_state().copy()
-+        control_attackers = np.zeros((self.NUM_ATTACKERS, 2))
-+        for i in range(self.NUM_ATTACKERS):
-+            neg2pos, pos2neg = find_sign_change1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i])
-+            if len(neg2pos):
-+                control_attackers[i] = self.attacker_control_1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i], neg2pos)
-+            else:
-+                control_attackers[i] = (0.0, 0.0)
-+                
-+        return control_attackers
-+    
-     
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -126,7 +197,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                                 break
- 
-             return new_status
--    ################################################################################
-+        
- 
-     def _check_area(self, state, area):
-         """Check if the state is inside the area.
-@@ -147,7 +218,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return False
-     
--    ################################################################################
- 
-     def _computeObs(self):
-         """Returns the current observation of the environment.
-@@ -162,16 +232,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return obs
-     
--    ################################################################################
-     
-     def _computeReward(self):
--        #TODO: Hanyang: not finished
-         """Computes the current reward value.
- 
--        One attacker is captured: +100
--        One attacker arrived at the goal: -100
-+        Once the attacker is captured: +100
-+        Once the attacker arrived at the goal: -100
-         The defender hits the obstacle: -100
--        One step and nothing happens: 
-+        One step and nothing happens: maybe use the distance between the attacker and the defender as a sign?
-         In status, 0 stands for free, -1 stands for captured, 1 stands for arrived
- 
-         Returns
-@@ -182,15 +250,24 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         """
-         last_attacker_status = self.attackers_status[-2]
-         current_attacker_status = self.attackers_status[-1]
--        reward = -1.0
-+        reward = 0.0
-+        # check the attacker status
-         for num in range(self.NUM_ATTACKERS):
--            reward += (current_attacker_status[num] - last_attacker_status[num]) * -10
--            
-+            reward += (current_attacker_status[num] - last_attacker_status[num]) * (-200)
-+        # check the defender status
-+        current_defender_state = self.defenders._get_state().copy()
-+        reward += -200 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-+        # check the relative distance difference or relative distance
-+        current_attacker_state = self.attackers._get_state().copy()
-+        current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
-+        last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
-+        # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
-+        reward += -current_relative_distance
-+        
-         return reward
- 
-     
-     def _computeTerminated(self):
--        #TODO: Hanyang: not finished
-         """Computes the current done value.
-         done = True if all attackers have arrived or been captured.
- 
-@@ -200,9 +277,15 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-             Whether the current episode is done.
- 
-         """
--        
-+        # defender hits the obstacle or the attacker is captured or the attacker has arrived or the attacker hits the obstacle
-+        # check the attacker status
-         current_attacker_status = self.attackers_status[-1]
--        done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        attacker_done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        # check the defender status: hit the obstacle, or the attacker is captured
-+        current_defender_state = self.defenders._get_state().copy()
-+        defender_done = self._check_area(current_defender_state[0], self.obstacles)
-+        # final done
-+        done = True if attacker_done or defender_done else False
-         
-         return done
-         
-@@ -223,7 +306,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-     
-     def _computeInfo(self):
--        #TODO: Hanyang: not finished
-         """Computes the current info dict(s).
- 
-         Unused.
-@@ -240,10 +322,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         
-         return info 
-     
--    ################################################################################
- 
-     def _computeAttackerActions(self):
--        #TODO: Hanyang: not finished
-         """Computes the current actions of the attackers.
- 
-         """
-@@ -296,9 +376,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_a1, opt_a2)
-     
--    ################################################################################
-     
--    def optCrtl_1vs1(self, spat_deriv):
-+    def optCtrl_1vs1(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 1 game.
-         
-         Parameters:
-@@ -329,7 +408,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_u1, opt_u2)
- 
--    ################################################################################
- 
-     def optCtrl_1vs0(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 0 game.
-@@ -360,50 +438,4 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                 opt_a1 = self.attackers.speed * deriv1 / ctrl_len
-                 opt_a2 = self.attackers.speed * deriv2 / ctrl_len
- 
--        return (opt_a1, opt_a2)
--        """Computes the optimal control (disturbance) for the attacker in a 1 vs. 2 game.
--        
--        Parameters:
--            spat_deriv (tuple): spatial derivative in all dimensions
--        
--        Returns:
--            tuple: a tuple of optimal control of the defender (disturbances)
--        """
--        opt_d1 = self.defenders.uMax
--        opt_d2 = self.defenders.uMax
--        opt_d3 = self.defenders.uMax
--        opt_d4 = self.defenders.uMax
--        deriv3 = spat_deriv[2]
--        deriv4 = spat_deriv[3]
--        deriv5 = spat_deriv[4]
--        deriv6 = spat_deriv[5]
--        distb_len1 = np.sqrt(deriv3*deriv3 + deriv4*deriv4)
--        distb_len2 = np.sqrt(deriv5*deriv5 + deriv6*deriv6)
--        if self.dMode == "max":
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = self.defenders.speed*deriv6 / distb_len2
--        else:
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = -self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = -self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = -self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = -self.defenders.speed*deriv6 / distb_len2
--
--        return (opt_d1, opt_d2, opt_d3, opt_d4)
-\ No newline at end of file
-+        return (opt_a1, opt_a2)
-\ No newline at end of file
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/events.out.tfevents.1718474498.cs-mars-14.27385.0 b/wandb/run-20240615_110134-fpms1nm3/files/events.out.tfevents.1718474498.cs-mars-14.27385.0
deleted file mode 120000
index 3d93a25..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/events.out.tfevents.1718474498.cs-mars-14.27385.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/runs/training_results/game/20242024.06.15_11:01/events.out.tfevents.1718474498.cs-mars-14.27385.0
\ No newline at end of file
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/requirements.txt b/wandb/run-20240615_110134-fpms1nm3/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/wandb-metadata.json b/wandb/run-20240615_110134-fpms1nm3/files/wandb-metadata.json
deleted file mode 100644
index ff339ef..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-15T18:01:35.315878",
-    "startedAt": "2024-06-15T18:01:34.998771",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game_controller.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game_controller.py",
-    "codePath": "safe_control_gym/experiments/train_game_controller.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "47ee2a533ec86c45ae7cc45eb6c514cc4d3acdf7"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3641.9372812500005,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3687.657,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3562.598,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3699.514,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2724.205,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3655.236,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3634.161,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2633.58,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4589.683,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4626.416,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3688.706,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3689.357,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3697.158,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3699.04,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3685.233,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3698.338,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3697.376,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2849.205,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3685.325,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3638.693,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2876.884,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3638.6,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2748.381,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3404.072,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4620.768,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4609.073,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3685.062,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3700.732,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3677.485,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3677.025,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3683.354,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3679.058,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3700.018,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26454544067383
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240615_110134-fpms1nm3/files/wandb-summary.json b/wandb/run-20240615_110134-fpms1nm3/files/wandb-summary.json
deleted file mode 100644
index f6a8b45..0000000
--- a/wandb/run-20240615_110134-fpms1nm3/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb": {"runtime": 4}, "global_step": 0, "_timestamp": 1718474498.9507837, "_runtime": 3.9488227367401123, "_step": 0}
\ No newline at end of file
diff --git a/wandb/run-20240615_110134-fpms1nm3/run-fpms1nm3.wandb b/wandb/run-20240615_110134-fpms1nm3/run-fpms1nm3.wandb
deleted file mode 100644
index 43c5e8e..0000000
Binary files a/wandb/run-20240615_110134-fpms1nm3/run-fpms1nm3.wandb and /dev/null differ
diff --git a/wandb/run-20240615_110630-ju4besh8/files/code/safe_control_gym/experiments/train_game_controller.py b/wandb/run-20240615_110630-ju4besh8/files/code/safe_control_gym/experiments/train_game_controller.py
deleted file mode 100644
index 8bc475d..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/code/safe_control_gym/experiments/train_game_controller.py
+++ /dev/null
@@ -1,362 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e6
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/' +f'{args.seed}' + f'{datetime.now().strftime("%Y.%m.%d_%H:%M")}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-        # from cleanrl_utils.evals.ppo_eval import evaluate
-
-        # episodic_returns = evaluate(
-        #     model_path,
-        #     make_env,
-        #     args.env_id,
-        #     eval_episodes=10,
-        #     run_name=f"{run_name}-eval",
-        #     Model=Agent,
-        #     device=device,
-        #     gamma=args.gamma,
-        # )
-        # for idx, episodic_return in enumerate(episodic_returns):
-        #     writer.add_scalar("eval/episodic_return", episodic_return, idx)
-
-        # if args.upload_model:
-        #     from cleanrl_utils.huggingface import push_to_hub
-
-        #     repo_name = f"{args.env_id}-{args.exp_name}-seed{args.seed}"
-        #     repo_id = f"{args.hf_entity}/{repo_name}" if args.hf_entity else repo_name
-        #     push_to_hub(args, episodic_returns, repo_id, "PPO", f"runs/{run_name}", f"videos/{run_name}-eval")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240615_110630-ju4besh8/files/conda-environment.yaml b/wandb/run-20240615_110630-ju4besh8/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240615_110630-ju4besh8/files/config.yaml b/wandb/run-20240615_110630-ju4besh8/files/config.yaml
deleted file mode 100644
index 9b585d3..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game_controller
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 1000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 244.0
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game_controller.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718474790.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240615_110630-ju4besh8/files/diff.patch b/wandb/run-20240615_110630-ju4besh8/files/diff.patch
deleted file mode 100644
index aa22c08..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/diff.patch
+++ /dev/null
@@ -1,762 +0,0 @@
-diff --git a/ceshi_game.py b/ceshi_game.py
-index 5994a62..4d5bd4b 100644
---- a/ceshi_game.py
-+++ b/ceshi_game.py
-@@ -1,9 +1,65 @@
-+import  numpy as np
- from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
- 
-+initial_attacker = np.array([[-0.1, 0.0]])
-+initial_defender = np.array([[0.0, 0.0]])
-+env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
-+# print(f"The initial state is {env.}. \n")
-+# obs, info = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
- 
--env = ReachAvoidGameEnv()
--obs = env.reset()
--print(f"The state space of the env is {env.observation_space}. \n")
--print(f"The action space of the env is {env.action_space}. \n")
--print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
--print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-\ No newline at end of file
-+# obs = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+
-+# for i in range(10):
-+#     obs = env.reset()
-+#     print(f"The initial player seed is {env.initial_players_seed}. \n")
-+#     print(f"The obs is {obs}. \n")
-+
-+
-+action = np.array([-0.1, 0.0])
-+
-+for i in range(10):
-+    obs, reward, terminated, truncated, info = env.step(action)
-+    print(f"The obs is {obs}. \n")
-+    print(f"The reward is {reward}. \n")
-+    # print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+    if terminated or truncated:
-+        print(f"The game is terminated: {terminated} and the game is truncated: {truncated} at the step {i}. \n")
-+        break
-+
-+# current_attackers = np.array([[0.0, 0.0]])
-+# current_defenders = np.array([[3.0, 4.0]])
-+# distance = np.linalg.norm(current_attackers[0] - current_defenders[0])
-+# print(f"The distance between the attacker and the defender is {distance}. \n")
-+
-+# obstacles = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}
-+
-+# def _check_area(state, area):
-+#         """Check if the state is inside the area.
-+
-+#         Parameters:
-+#             state (np.ndarray): the state to check
-+#             area (dict): the area dictionary to be checked.
-+        
-+#         Returns:
-+#             bool: True if the state is inside the area, False otherwise.
-+#         """
-+#         x, y = state  # Unpack the state assuming it's a 2D coordinate
-+
-+#         for bounds in area.values():
-+#             x_lower, x_upper, y_lower, y_upper = bounds
-+#             if x_lower <= x <= x_upper and y_lower <= y <= y_upper:
-+#                 return True
-+
-+#         return False
-+
-+# print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
-+# reward = 0.0
-+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
-index 5e7b3f0..d1032b2 100644
---- a/safe_control_gym/envs/__init__.py
-+++ b/safe_control_gym/envs/__init__.py
-@@ -61,4 +61,7 @@ register(idx='quadrotor_random',
- 
- register(idx='quadrotor_randomhj',
-          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
--         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-\ No newline at end of file
-+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-+
-+register(idx='reach_avoid',
-+         entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 0ea88d5..ce2dc6b 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -14,6 +14,7 @@ class Dynamics:
-     SIG = {'id': 'sig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.0}           # Base single integrator dynamics
-     FSIG = {'id': 'fsig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.5}         # Faster single integrator dynamics with feedback
-     
-+    
- class BaseGameEnv(gym.Env):
-     """Base class for the multi-agent reach-avoid game Gym environments."""
-     
-@@ -26,6 +27,7 @@ class BaseGameEnv(gym.Env):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed: int = None,
-+                 random_init: bool = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +49,7 @@ class BaseGameEnv(gym.Env):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init : bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -67,15 +70,12 @@ class BaseGameEnv(gym.Env):
-         #### Input initial states ####################################
-         self.init_attackers = initial_attacker
-         self.init_defenders = initial_defender
--        #### Create action and observation spaces ##################
--        self.action_space = self._actionSpace()
--        self.observation_space = self._observationSpace()
-         #### Housekeeping ##########################################
-+        self.random_init = random_init
-         self._housekeeping()
-         #### Update and all players' information #####
-         self._updateAndLog()
-     
--    ################################################################################
- 
-     def _housekeeping(self):
-         """Housekeeping function.
-@@ -84,8 +84,10 @@ class BaseGameEnv(gym.Env):
-         in the `reset()` function.
- 
-         """
--        if self.init_attackers is None and self.init_defenders is None:
--            self.init_attackers, self.init_defenders = self.initial_players()            
-+        if self.random_init:
-+            self.init_attackers, self.init_defenders = self.initial_players()
-+        else:
-+            assert self.init_attackers is not None and self.init_defenders is not None, "Need to provide initial positions for all players."     
-         #### Set attackers and defenders ##########################
-         self.attackers = make_agents(self.ATTACKER_PHYSICS, self.NUM_ATTACKERS, self.init_attackers, self.CTRL_FREQ)
-         self.defenders = make_agents(self.DEFENDER_PHYSICS, self.NUM_DEFENDERS, self.init_defenders, self.CTRL_FREQ)
-@@ -96,21 +98,26 @@ class BaseGameEnv(gym.Env):
-         self.attackers_status = []  # 0 stands for free, -1 stands for captured, 1 stands for arrived 
-         self.attackers_actions = []
-         self.defenders_actions = []
-+        # self.last_relative_distance = np.zeros((self.NUM_ATTACKERS, self.NUM_DEFENDERS))
- 
--    ################################################################################
- 
-     def _updateAndLog(self):
-         """Update and log all players' information after inialization, reset(), or step.
- 
-         """
-         # Update the state
--        self.state = np.vstack([self.attackers._get_state().copy(), self.defenders._get_state().copy()])
-+        current_attackers = self.attackers._get_state().copy()
-+        current_defenders = self.defenders._get_state().copy()
-+        
-+        self.state = np.vstack([current_attackers, current_defenders])
-         # Log the state and trajectory information
--        self.attackers_traj.append(self.attackers._get_state().copy())
--        self.defenders_traj.append(self.defenders._get_state().copy())
-+        self.attackers_traj.append(current_attackers)
-+        self.defenders_traj.append(current_defenders)
-         self.attackers_status.append(self._getAttackersStatus().copy())
-+        # for i in range(self.NUM_ATTACKERS):
-+        #     for j in range(self.NUM_DEFENDERS):
-+        #         self.last_relative_distance[i, j] = np.linalg.norm(current_attackers[i] - current_defenders[j])
-     
--    ################################################################################
-     
-     def initial_players(self):
-         '''Set the initial positions for all players.
-@@ -122,7 +129,7 @@ class BaseGameEnv(gym.Env):
-         np.random.seed(self.initial_players_seed)
-     
-         # Map boundaries
--        min_val, max_val = -1.0, 1.0
-+        min_val, max_val = -0.99, 0.99
-         
-         # Obstacles and target areas
-         obstacles = [
-@@ -170,7 +177,6 @@ class BaseGameEnv(gym.Env):
-         
-         return np.array([attacker_pos]), np.array([defender_pos])
- 
--    ################################################################################
-     
-     def reset(self, seed : int = None):
-         """Resets the environment.
-@@ -198,65 +204,10 @@ class BaseGameEnv(gym.Env):
-         self._updateAndLog()
-         #### Prepare the observation #############################
-         obs = self._computeObs()
--        
--        return obs
--    
--    ################################################################################
--
--    def step(self,action):
--        #TODO: Hanyang: change the action only for the defender
--        """Advances the environment by one simulation step.
--
--        Parameters
--        ----------
--        action : ndarray | (dim_action, )
--            The input action for the defender.
--
--        Returns
--        -------
--        ndarray | dict[..]
--            The step's observation, check the specific implementation of `_computeObs()`
--            in each subclass for its format.
--        float | dict[..]
--            The step's reward value(s), check the specific implementation of `_computeReward()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is trunacted, always false.
--        dict[..]
--            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
--            in each subclass for its format.
--
--        """
--        
--        #### Step the simulation using the desired physics update ##        
--        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
--        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
--        self.attackers.step(attackers_action)
--        self.defenders.step(defenders_action)
--        #### Update and all players' information #####
--        self._updateAndLog()
--        #### Prepare the return values #############################
--        obs = self._computeObs()
--        reward = self._computeReward()
--        terminated = self._computeTerminated()
--        truncated = self._computeTruncated()
-         info = self._computeInfo()
-         
--        #### Advance the step counter ##############################
--        self.step_counter += 1
--        #### Log the actions taken by the attackers and defenders ################
--        self.attackers_actions.append(attackers_action)
--        self.defenders_actions.append(defenders_action)
--        
--        return obs, reward, terminated, truncated, info
-+        return obs, info
-     
--    ################################################################################
- 
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -266,37 +217,15 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
--    
--    def _actionSpace(self):
--        """Returns the action space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--           
--    ################################################################################
--
--    def _observationSpace(self):
--        """Returns the observation space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--    
--    ################################################################################
-     
-     def _computeObs(self):
-         """Returns the current observation of the environment.
- 
--        Must be implemented in a subclass.
--
-         """
--        raise NotImplementedError
-+        obs = self.state.flatten()
-+        
-+        return obs
-     
--    ################################################################################
- 
-     def _computeReward(self):
-         """Computes the current reward value(s).
-@@ -311,7 +240,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeTerminated(self):
-         """Computes the current terminated value(s).
-@@ -321,7 +249,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
- 
-     def _computeTruncated(self):
-         """Computes the current truncated value(s).
-@@ -331,22 +258,11 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeInfo(self):
-         """Computes the current info dict(s).
- 
-         Must be implemented in a subclass.
- 
--        """
--        raise NotImplementedError
--
--    ################################################################################
--
--    def _computeAttackerActions(self):
--        """Computes the current actions of the attackers.
--
--        Must be implemented in a subclass.
--
-         """
-         raise NotImplementedError
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseRLGame.py b/safe_control_gym/envs/gym_game/BaseRLGame.py
-index cdd42ca..c62dd6e 100644
---- a/safe_control_gym/envs/gym_game/BaseRLGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseRLGame.py
-@@ -26,6 +26,7 @@ class BaseRLGameEnv(BaseGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +48,7 @@ class BaseRLGameEnv(BaseGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -55,32 +57,33 @@ class BaseRLGameEnv(BaseGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-+        
-+        #### Create action and observation spaces ##################
-+        self.action_space = self._actionSpace()
-+        self.observation_space = self._observationSpace()
-        
--
--    ################################################################################
--
-    
-     def _actionSpace(self):
-         """Returns the action space of the environment.
--        Formulation: [attackers' action spaces, defenders' action spaces]
-+        Formulation: [defenders' action spaces]
-         Returns
-         -------
-         spaces.Box
--            A Box of size NUM_PLAYERS x 2, or 1, depending on the action type.
-+            A Box of size NUM_DEFENDERS x 2, or 1, depending on the action type.
- 
-         """
-         
--        if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
--            attacker_lower_bound = np.array([-1.0, -1.0])
--            attacker_upper_bound = np.array([+1.0, +1.0])
--        elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
--            attacker_lower_bound = np.array([-1.0])
--            attacker_upper_bound = np.array([+1.0])
--        else:
--            print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
--            exit()
-+        # if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
-+        #     attacker_lower_bound = np.array([-1.0, -1.0])
-+        #     attacker_upper_bound = np.array([+1.0, +1.0])
-+        # elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
-+        #     attacker_lower_bound = np.array([-1.0])
-+        #     attacker_upper_bound = np.array([+1.0])
-+        # else:
-+        #     print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
-+        #     exit()
-         
-         if self.DEFENDER_PHYSICS == Dynamics.SIG or self.DEFENDER_PHYSICS == Dynamics.FSIG:
-             defender_lower_bound = np.array([-1.0, -1.0])
-@@ -92,28 +95,28 @@ class BaseRLGameEnv(BaseGameEnv):
-             print("[ERROR] in Defender Action Space, BaseRLGameEnv._actionSpace()")
-             exit()
-         
--        attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
--        attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
- 
--        if self.NUM_DEFENDERS > 0:
--            defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
--            defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-+        # if self.NUM_DEFENDERS > 0:
-+        #     defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        #     defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-             
--            act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
--            act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
--        else:
--            act_lower_bound = attackers_lower_bound
--            act_upper_bound = attackers_upper_bound
--
-+        #     act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
-+        #     act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
-+        # else:
-+        #     act_lower_bound = attackers_lower_bound
-+        #     act_upper_bound = attackers_upper_bound
-+            
-+        defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-         # Flatten the lower and upper bounds to ensure the action space shape is (4,)
--        act_lower_bound = act_lower_bound.flatten()
--        act_upper_bound = act_upper_bound.flatten()
-+        act_lower_bound = defenders_lower_bound.flatten()
-+        act_upper_bound = defenders_upper_bound.flatten()
- 
-         return spaces.Box(low=act_lower_bound, high=act_upper_bound, dtype=np.float32)
-  
- 
--    ################################################################################
--
-     def _observationSpace(self):
-         """Returns the observation space of the environment.
-         Formulation: [attackers' obs spaces, defenders' obs spaces]
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index b5344ff..0ea357c 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -24,13 +24,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  uMode="min", 
-                  dMode="max",
-                  output_folder='results',
-                  game_length_sec=20,
--                 map={'map': [-1., 1., -1., 1.]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-+                 map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
--                 obstacles: dict = None,  
-+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  ):
-         """Initialization of a generic aviary environment.
- 
-@@ -51,6 +52,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         uMode : str, optional
-             The mode of the attacker, default is "min".
-         dMode : str, optional
-@@ -71,7 +73,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-         
-         assert map is not None, "Map must be provided in the game."
-@@ -84,13 +86,82 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         self.uMode = uMode
-         self.dMode = dMode
-         # Load necessary values for the attacker control
--        #TODO: Hanyang: not finished
-         self.grid1vs0 = Grid(np.array([-1.0, -1.0]), np.array([1.0, 1.0]), 2, np.array([100, 100])) 
--        self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
--        self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-+        # self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
-+        # self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-         self.value1vs0 = np.load('safe_control_gym/envs/gym_game/values/1vs0Attacker.npy')
- 
--    ################################################################################
-+    
-+    def step(self, action):
-+        """Advances the environment by one simulation step.
-+
-+        Parameters
-+        ----------
-+        action : ndarray | (dim_action, )
-+            The input action for the defender.
-+
-+        Returns
-+        -------
-+        ndarray | dict[..]
-+            The step's observation, check the specific implementation of `_computeObs()`
-+            in each subclass for its format.
-+        float | dict[..]
-+            The step's reward value(s), check the specific implementation of `_computeReward()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is trunacted, always false.
-+        dict[..]
-+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-+            in each subclass for its format.
-+
-+        """
-+        
-+        #### Step the simulation using the desired physics update ##        
-+        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
-+        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
-+        self.attackers.step(attackers_action)
-+        self.defenders.step(defenders_action)
-+        #### Update and all players' information #####
-+        self._updateAndLog()
-+        #### Prepare the return values #############################
-+        obs = self._computeObs()
-+        reward = self._computeReward()
-+        terminated = self._computeTerminated()
-+        truncated = self._computeTruncated()
-+        info = self._computeInfo()
-+        
-+        #### Advance the step counter ##############################
-+        self.step_counter += 1
-+        #### Log the actions taken by the attackers and defenders ################
-+        self.attackers_actions.append(attackers_action)
-+        self.defenders_actions.append(defenders_action)
-+        
-+        return obs, reward, terminated, truncated, info
-+    
-+    
-+    def _computeAttackerActions(self):
-+        """Computes the current actions of the attackers.
-+
-+        Must be implemented in a subclass.
-+
-+        """
-+        current_attacker_state = self.attackers._get_state().copy()
-+        control_attackers = np.zeros((self.NUM_ATTACKERS, 2))
-+        for i in range(self.NUM_ATTACKERS):
-+            neg2pos, pos2neg = find_sign_change1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i])
-+            if len(neg2pos):
-+                control_attackers[i] = self.attacker_control_1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i], neg2pos)
-+            else:
-+                control_attackers[i] = (0.0, 0.0)
-+                
-+        return control_attackers
-+    
-     
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -126,7 +197,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                                 break
- 
-             return new_status
--    ################################################################################
-+        
- 
-     def _check_area(self, state, area):
-         """Check if the state is inside the area.
-@@ -147,7 +218,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return False
-     
--    ################################################################################
- 
-     def _computeObs(self):
-         """Returns the current observation of the environment.
-@@ -162,16 +232,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return obs
-     
--    ################################################################################
-     
-     def _computeReward(self):
--        #TODO: Hanyang: not finished
-         """Computes the current reward value.
- 
--        One attacker is captured: +100
--        One attacker arrived at the goal: -100
-+        Once the attacker is captured: +100
-+        Once the attacker arrived at the goal: -100
-         The defender hits the obstacle: -100
--        One step and nothing happens: 
-+        One step and nothing happens: maybe use the distance between the attacker and the defender as a sign?
-         In status, 0 stands for free, -1 stands for captured, 1 stands for arrived
- 
-         Returns
-@@ -182,15 +250,24 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         """
-         last_attacker_status = self.attackers_status[-2]
-         current_attacker_status = self.attackers_status[-1]
--        reward = -1.0
-+        reward = 0.0
-+        # check the attacker status
-         for num in range(self.NUM_ATTACKERS):
--            reward += (current_attacker_status[num] - last_attacker_status[num]) * -10
--            
-+            reward += (current_attacker_status[num] - last_attacker_status[num]) * (-200)
-+        # check the defender status
-+        current_defender_state = self.defenders._get_state().copy()
-+        reward += -200 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-+        # check the relative distance difference or relative distance
-+        current_attacker_state = self.attackers._get_state().copy()
-+        current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
-+        last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
-+        # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
-+        reward += -current_relative_distance
-+        
-         return reward
- 
-     
-     def _computeTerminated(self):
--        #TODO: Hanyang: not finished
-         """Computes the current done value.
-         done = True if all attackers have arrived or been captured.
- 
-@@ -200,9 +277,15 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-             Whether the current episode is done.
- 
-         """
--        
-+        # defender hits the obstacle or the attacker is captured or the attacker has arrived or the attacker hits the obstacle
-+        # check the attacker status
-         current_attacker_status = self.attackers_status[-1]
--        done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        attacker_done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        # check the defender status: hit the obstacle, or the attacker is captured
-+        current_defender_state = self.defenders._get_state().copy()
-+        defender_done = self._check_area(current_defender_state[0], self.obstacles)
-+        # final done
-+        done = True if attacker_done or defender_done else False
-         
-         return done
-         
-@@ -223,7 +306,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-     
-     def _computeInfo(self):
--        #TODO: Hanyang: not finished
-         """Computes the current info dict(s).
- 
-         Unused.
-@@ -240,10 +322,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         
-         return info 
-     
--    ################################################################################
- 
-     def _computeAttackerActions(self):
--        #TODO: Hanyang: not finished
-         """Computes the current actions of the attackers.
- 
-         """
-@@ -296,9 +376,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_a1, opt_a2)
-     
--    ################################################################################
-     
--    def optCrtl_1vs1(self, spat_deriv):
-+    def optCtrl_1vs1(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 1 game.
-         
-         Parameters:
-@@ -329,7 +408,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_u1, opt_u2)
- 
--    ################################################################################
- 
-     def optCtrl_1vs0(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 0 game.
-@@ -360,50 +438,4 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                 opt_a1 = self.attackers.speed * deriv1 / ctrl_len
-                 opt_a2 = self.attackers.speed * deriv2 / ctrl_len
- 
--        return (opt_a1, opt_a2)
--        """Computes the optimal control (disturbance) for the attacker in a 1 vs. 2 game.
--        
--        Parameters:
--            spat_deriv (tuple): spatial derivative in all dimensions
--        
--        Returns:
--            tuple: a tuple of optimal control of the defender (disturbances)
--        """
--        opt_d1 = self.defenders.uMax
--        opt_d2 = self.defenders.uMax
--        opt_d3 = self.defenders.uMax
--        opt_d4 = self.defenders.uMax
--        deriv3 = spat_deriv[2]
--        deriv4 = spat_deriv[3]
--        deriv5 = spat_deriv[4]
--        deriv6 = spat_deriv[5]
--        distb_len1 = np.sqrt(deriv3*deriv3 + deriv4*deriv4)
--        distb_len2 = np.sqrt(deriv5*deriv5 + deriv6*deriv6)
--        if self.dMode == "max":
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = self.defenders.speed*deriv6 / distb_len2
--        else:
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = -self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = -self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = -self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = -self.defenders.speed*deriv6 / distb_len2
--
--        return (opt_d1, opt_d2, opt_d3, opt_d4)
-\ No newline at end of file
-+        return (opt_a1, opt_a2)
-\ No newline at end of file
diff --git a/wandb/run-20240615_110630-ju4besh8/files/events.out.tfevents.1718474794.cs-mars-14.28158.0 b/wandb/run-20240615_110630-ju4besh8/files/events.out.tfevents.1718474794.cs-mars-14.28158.0
deleted file mode 120000
index ca5b7e0..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/events.out.tfevents.1718474794.cs-mars-14.28158.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/runs/training_results/game/20242024.06.15_11:06/events.out.tfevents.1718474794.cs-mars-14.28158.0
\ No newline at end of file
diff --git a/wandb/run-20240615_110630-ju4besh8/files/requirements.txt b/wandb/run-20240615_110630-ju4besh8/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240615_110630-ju4besh8/files/wandb-metadata.json b/wandb/run-20240615_110630-ju4besh8/files/wandb-metadata.json
deleted file mode 100644
index 54ee91f..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-15T18:06:30.795642",
-    "startedAt": "2024-06-15T18:06:30.456551",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game_controller.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game_controller.py",
-    "codePath": "safe_control_gym/experiments/train_game_controller.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "47ee2a533ec86c45ae7cc45eb6c514cc4d3acdf7"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3100.36325,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 4807.404,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3844.019,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3853.879,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3839.631,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3841.5,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3839.783,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3844.159,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3842.729,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2200.338,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.434,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2217.383,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2235.783,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2509.259,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2222.662,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.639,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2219.355,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4800.855,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3834.6,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3835.407,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3841.035,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3835.354,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3834.911,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3837.366,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3840.401,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2222.715,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.375,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2214.898,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2201.034,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2212.099,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2218.624,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.652,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2236.341,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26460647583008
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240615_110630-ju4besh8/files/wandb-summary.json b/wandb/run-20240615_110630-ju4besh8/files/wandb-summary.json
deleted file mode 100644
index dce0732..0000000
--- a/wandb/run-20240615_110630-ju4besh8/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb": {"runtime": 4}, "global_step": 0, "_timestamp": 1718474794.373307, "_runtime": 3.913583993911743, "_step": 0}
\ No newline at end of file
diff --git a/wandb/run-20240615_110630-ju4besh8/run-ju4besh8.wandb b/wandb/run-20240615_110630-ju4besh8/run-ju4besh8.wandb
deleted file mode 100644
index c2007a3..0000000
Binary files a/wandb/run-20240615_110630-ju4besh8/run-ju4besh8.wandb and /dev/null differ
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/code/safe_control_gym/experiments/train_game_controller.py b/wandb/run-20240615_111015-lmlvxua0/files/code/safe_control_gym/experiments/train_game_controller.py
deleted file mode 100644
index a782df9..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/code/safe_control_gym/experiments/train_game_controller.py
+++ /dev/null
@@ -1,362 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e6
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{datetime.now().strftime("%Y.%m.%d_%H:%M")}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-        # from cleanrl_utils.evals.ppo_eval import evaluate
-
-        # episodic_returns = evaluate(
-        #     model_path,
-        #     make_env,
-        #     args.env_id,
-        #     eval_episodes=10,
-        #     run_name=f"{run_name}-eval",
-        #     Model=Agent,
-        #     device=device,
-        #     gamma=args.gamma,
-        # )
-        # for idx, episodic_return in enumerate(episodic_returns):
-        #     writer.add_scalar("eval/episodic_return", episodic_return, idx)
-
-        # if args.upload_model:
-        #     from cleanrl_utils.huggingface import push_to_hub
-
-        #     repo_name = f"{args.env_id}-{args.exp_name}-seed{args.seed}"
-        #     repo_id = f"{args.hf_entity}/{repo_name}" if args.hf_entity else repo_name
-        #     push_to_hub(args, episodic_returns, repo_id, "PPO", f"runs/{run_name}", f"videos/{run_name}-eval")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/conda-environment.yaml b/wandb/run-20240615_111015-lmlvxua0/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/config.yaml b/wandb/run-20240615_111015-lmlvxua0/files/config.yaml
deleted file mode 100644
index 777707e..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game_controller
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 1000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 244.0
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game_controller.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718475015.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/diff.patch b/wandb/run-20240615_111015-lmlvxua0/files/diff.patch
deleted file mode 100644
index 9ddbe93..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/diff.patch
+++ /dev/null
@@ -1,766 +0,0 @@
-diff --git a/ceshi_game.py b/ceshi_game.py
-index 5994a62..4d5bd4b 100644
---- a/ceshi_game.py
-+++ b/ceshi_game.py
-@@ -1,9 +1,65 @@
-+import  numpy as np
- from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
- 
-+initial_attacker = np.array([[-0.1, 0.0]])
-+initial_defender = np.array([[0.0, 0.0]])
-+env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
-+# print(f"The initial state is {env.}. \n")
-+# obs, info = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
- 
--env = ReachAvoidGameEnv()
--obs = env.reset()
--print(f"The state space of the env is {env.observation_space}. \n")
--print(f"The action space of the env is {env.action_space}. \n")
--print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
--print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-\ No newline at end of file
-+# obs = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+
-+# for i in range(10):
-+#     obs = env.reset()
-+#     print(f"The initial player seed is {env.initial_players_seed}. \n")
-+#     print(f"The obs is {obs}. \n")
-+
-+
-+action = np.array([-0.1, 0.0])
-+
-+for i in range(10):
-+    obs, reward, terminated, truncated, info = env.step(action)
-+    print(f"The obs is {obs}. \n")
-+    print(f"The reward is {reward}. \n")
-+    # print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+    if terminated or truncated:
-+        print(f"The game is terminated: {terminated} and the game is truncated: {truncated} at the step {i}. \n")
-+        break
-+
-+# current_attackers = np.array([[0.0, 0.0]])
-+# current_defenders = np.array([[3.0, 4.0]])
-+# distance = np.linalg.norm(current_attackers[0] - current_defenders[0])
-+# print(f"The distance between the attacker and the defender is {distance}. \n")
-+
-+# obstacles = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}
-+
-+# def _check_area(state, area):
-+#         """Check if the state is inside the area.
-+
-+#         Parameters:
-+#             state (np.ndarray): the state to check
-+#             area (dict): the area dictionary to be checked.
-+        
-+#         Returns:
-+#             bool: True if the state is inside the area, False otherwise.
-+#         """
-+#         x, y = state  # Unpack the state assuming it's a 2D coordinate
-+
-+#         for bounds in area.values():
-+#             x_lower, x_upper, y_lower, y_upper = bounds
-+#             if x_lower <= x <= x_upper and y_lower <= y <= y_upper:
-+#                 return True
-+
-+#         return False
-+
-+# print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
-+# reward = 0.0
-+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
-index 5e7b3f0..d1032b2 100644
---- a/safe_control_gym/envs/__init__.py
-+++ b/safe_control_gym/envs/__init__.py
-@@ -61,4 +61,7 @@ register(idx='quadrotor_random',
- 
- register(idx='quadrotor_randomhj',
-          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
--         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-\ No newline at end of file
-+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-+
-+register(idx='reach_avoid',
-+         entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 0ea88d5..169382c 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -14,6 +14,7 @@ class Dynamics:
-     SIG = {'id': 'sig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.0}           # Base single integrator dynamics
-     FSIG = {'id': 'fsig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.5}         # Faster single integrator dynamics with feedback
-     
-+    
- class BaseGameEnv(gym.Env):
-     """Base class for the multi-agent reach-avoid game Gym environments."""
-     
-@@ -26,6 +27,7 @@ class BaseGameEnv(gym.Env):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed: int = None,
-+                 random_init: bool = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +49,7 @@ class BaseGameEnv(gym.Env):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init : bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -67,15 +70,12 @@ class BaseGameEnv(gym.Env):
-         #### Input initial states ####################################
-         self.init_attackers = initial_attacker
-         self.init_defenders = initial_defender
--        #### Create action and observation spaces ##################
--        self.action_space = self._actionSpace()
--        self.observation_space = self._observationSpace()
-         #### Housekeeping ##########################################
-+        self.random_init = random_init
-         self._housekeeping()
-         #### Update and all players' information #####
-         self._updateAndLog()
-     
--    ################################################################################
- 
-     def _housekeeping(self):
-         """Housekeeping function.
-@@ -84,8 +84,10 @@ class BaseGameEnv(gym.Env):
-         in the `reset()` function.
- 
-         """
--        if self.init_attackers is None and self.init_defenders is None:
--            self.init_attackers, self.init_defenders = self.initial_players()            
-+        if self.random_init:
-+            self.init_attackers, self.init_defenders = self.initial_players()
-+        else:
-+            assert self.init_attackers is not None and self.init_defenders is not None, "Need to provide initial positions for all players."     
-         #### Set attackers and defenders ##########################
-         self.attackers = make_agents(self.ATTACKER_PHYSICS, self.NUM_ATTACKERS, self.init_attackers, self.CTRL_FREQ)
-         self.defenders = make_agents(self.DEFENDER_PHYSICS, self.NUM_DEFENDERS, self.init_defenders, self.CTRL_FREQ)
-@@ -96,21 +98,26 @@ class BaseGameEnv(gym.Env):
-         self.attackers_status = []  # 0 stands for free, -1 stands for captured, 1 stands for arrived 
-         self.attackers_actions = []
-         self.defenders_actions = []
-+        # self.last_relative_distance = np.zeros((self.NUM_ATTACKERS, self.NUM_DEFENDERS))
- 
--    ################################################################################
- 
-     def _updateAndLog(self):
-         """Update and log all players' information after inialization, reset(), or step.
- 
-         """
-         # Update the state
--        self.state = np.vstack([self.attackers._get_state().copy(), self.defenders._get_state().copy()])
-+        current_attackers = self.attackers._get_state().copy()
-+        current_defenders = self.defenders._get_state().copy()
-+        
-+        self.state = np.vstack([current_attackers, current_defenders])
-         # Log the state and trajectory information
--        self.attackers_traj.append(self.attackers._get_state().copy())
--        self.defenders_traj.append(self.defenders._get_state().copy())
-+        self.attackers_traj.append(current_attackers)
-+        self.defenders_traj.append(current_defenders)
-         self.attackers_status.append(self._getAttackersStatus().copy())
-+        # for i in range(self.NUM_ATTACKERS):
-+        #     for j in range(self.NUM_DEFENDERS):
-+        #         self.last_relative_distance[i, j] = np.linalg.norm(current_attackers[i] - current_defenders[j])
-     
--    ################################################################################
-     
-     def initial_players(self):
-         '''Set the initial positions for all players.
-@@ -122,7 +129,7 @@ class BaseGameEnv(gym.Env):
-         np.random.seed(self.initial_players_seed)
-     
-         # Map boundaries
--        min_val, max_val = -1.0, 1.0
-+        min_val, max_val = -0.99, 0.99
-         
-         # Obstacles and target areas
-         obstacles = [
-@@ -170,9 +177,9 @@ class BaseGameEnv(gym.Env):
-         
-         return np.array([attacker_pos]), np.array([defender_pos])
- 
--    ################################################################################
-     
--    def reset(self, seed : int = None):
-+    def reset(self, seed : int = None,
-+              options : dict = None):
-         """Resets the environment.
- 
-         Parameters
-@@ -198,65 +205,10 @@ class BaseGameEnv(gym.Env):
-         self._updateAndLog()
-         #### Prepare the observation #############################
-         obs = self._computeObs()
--        
--        return obs
--    
--    ################################################################################
--
--    def step(self,action):
--        #TODO: Hanyang: change the action only for the defender
--        """Advances the environment by one simulation step.
--
--        Parameters
--        ----------
--        action : ndarray | (dim_action, )
--            The input action for the defender.
--
--        Returns
--        -------
--        ndarray | dict[..]
--            The step's observation, check the specific implementation of `_computeObs()`
--            in each subclass for its format.
--        float | dict[..]
--            The step's reward value(s), check the specific implementation of `_computeReward()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is trunacted, always false.
--        dict[..]
--            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
--            in each subclass for its format.
--
--        """
--        
--        #### Step the simulation using the desired physics update ##        
--        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
--        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
--        self.attackers.step(attackers_action)
--        self.defenders.step(defenders_action)
--        #### Update and all players' information #####
--        self._updateAndLog()
--        #### Prepare the return values #############################
--        obs = self._computeObs()
--        reward = self._computeReward()
--        terminated = self._computeTerminated()
--        truncated = self._computeTruncated()
-         info = self._computeInfo()
-         
--        #### Advance the step counter ##############################
--        self.step_counter += 1
--        #### Log the actions taken by the attackers and defenders ################
--        self.attackers_actions.append(attackers_action)
--        self.defenders_actions.append(defenders_action)
--        
--        return obs, reward, terminated, truncated, info
-+        return obs, info
-     
--    ################################################################################
- 
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -266,37 +218,15 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
--    
--    def _actionSpace(self):
--        """Returns the action space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--           
--    ################################################################################
--
--    def _observationSpace(self):
--        """Returns the observation space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--    
--    ################################################################################
-     
-     def _computeObs(self):
-         """Returns the current observation of the environment.
- 
--        Must be implemented in a subclass.
--
-         """
--        raise NotImplementedError
-+        obs = self.state.flatten()
-+        
-+        return obs
-     
--    ################################################################################
- 
-     def _computeReward(self):
-         """Computes the current reward value(s).
-@@ -311,7 +241,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeTerminated(self):
-         """Computes the current terminated value(s).
-@@ -321,7 +250,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
- 
-     def _computeTruncated(self):
-         """Computes the current truncated value(s).
-@@ -331,22 +259,11 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeInfo(self):
-         """Computes the current info dict(s).
- 
-         Must be implemented in a subclass.
- 
--        """
--        raise NotImplementedError
--
--    ################################################################################
--
--    def _computeAttackerActions(self):
--        """Computes the current actions of the attackers.
--
--        Must be implemented in a subclass.
--
-         """
-         raise NotImplementedError
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseRLGame.py b/safe_control_gym/envs/gym_game/BaseRLGame.py
-index cdd42ca..c62dd6e 100644
---- a/safe_control_gym/envs/gym_game/BaseRLGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseRLGame.py
-@@ -26,6 +26,7 @@ class BaseRLGameEnv(BaseGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +48,7 @@ class BaseRLGameEnv(BaseGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -55,32 +57,33 @@ class BaseRLGameEnv(BaseGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-+        
-+        #### Create action and observation spaces ##################
-+        self.action_space = self._actionSpace()
-+        self.observation_space = self._observationSpace()
-        
--
--    ################################################################################
--
-    
-     def _actionSpace(self):
-         """Returns the action space of the environment.
--        Formulation: [attackers' action spaces, defenders' action spaces]
-+        Formulation: [defenders' action spaces]
-         Returns
-         -------
-         spaces.Box
--            A Box of size NUM_PLAYERS x 2, or 1, depending on the action type.
-+            A Box of size NUM_DEFENDERS x 2, or 1, depending on the action type.
- 
-         """
-         
--        if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
--            attacker_lower_bound = np.array([-1.0, -1.0])
--            attacker_upper_bound = np.array([+1.0, +1.0])
--        elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
--            attacker_lower_bound = np.array([-1.0])
--            attacker_upper_bound = np.array([+1.0])
--        else:
--            print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
--            exit()
-+        # if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
-+        #     attacker_lower_bound = np.array([-1.0, -1.0])
-+        #     attacker_upper_bound = np.array([+1.0, +1.0])
-+        # elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
-+        #     attacker_lower_bound = np.array([-1.0])
-+        #     attacker_upper_bound = np.array([+1.0])
-+        # else:
-+        #     print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
-+        #     exit()
-         
-         if self.DEFENDER_PHYSICS == Dynamics.SIG or self.DEFENDER_PHYSICS == Dynamics.FSIG:
-             defender_lower_bound = np.array([-1.0, -1.0])
-@@ -92,28 +95,28 @@ class BaseRLGameEnv(BaseGameEnv):
-             print("[ERROR] in Defender Action Space, BaseRLGameEnv._actionSpace()")
-             exit()
-         
--        attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
--        attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
- 
--        if self.NUM_DEFENDERS > 0:
--            defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
--            defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-+        # if self.NUM_DEFENDERS > 0:
-+        #     defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        #     defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-             
--            act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
--            act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
--        else:
--            act_lower_bound = attackers_lower_bound
--            act_upper_bound = attackers_upper_bound
--
-+        #     act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
-+        #     act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
-+        # else:
-+        #     act_lower_bound = attackers_lower_bound
-+        #     act_upper_bound = attackers_upper_bound
-+            
-+        defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-         # Flatten the lower and upper bounds to ensure the action space shape is (4,)
--        act_lower_bound = act_lower_bound.flatten()
--        act_upper_bound = act_upper_bound.flatten()
-+        act_lower_bound = defenders_lower_bound.flatten()
-+        act_upper_bound = defenders_upper_bound.flatten()
- 
-         return spaces.Box(low=act_lower_bound, high=act_upper_bound, dtype=np.float32)
-  
- 
--    ################################################################################
--
-     def _observationSpace(self):
-         """Returns the observation space of the environment.
-         Formulation: [attackers' obs spaces, defenders' obs spaces]
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index b5344ff..0ea357c 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -24,13 +24,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  uMode="min", 
-                  dMode="max",
-                  output_folder='results',
-                  game_length_sec=20,
--                 map={'map': [-1., 1., -1., 1.]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-+                 map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
--                 obstacles: dict = None,  
-+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  ):
-         """Initialization of a generic aviary environment.
- 
-@@ -51,6 +52,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         uMode : str, optional
-             The mode of the attacker, default is "min".
-         dMode : str, optional
-@@ -71,7 +73,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-         
-         assert map is not None, "Map must be provided in the game."
-@@ -84,13 +86,82 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         self.uMode = uMode
-         self.dMode = dMode
-         # Load necessary values for the attacker control
--        #TODO: Hanyang: not finished
-         self.grid1vs0 = Grid(np.array([-1.0, -1.0]), np.array([1.0, 1.0]), 2, np.array([100, 100])) 
--        self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
--        self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-+        # self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
-+        # self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-         self.value1vs0 = np.load('safe_control_gym/envs/gym_game/values/1vs0Attacker.npy')
- 
--    ################################################################################
-+    
-+    def step(self, action):
-+        """Advances the environment by one simulation step.
-+
-+        Parameters
-+        ----------
-+        action : ndarray | (dim_action, )
-+            The input action for the defender.
-+
-+        Returns
-+        -------
-+        ndarray | dict[..]
-+            The step's observation, check the specific implementation of `_computeObs()`
-+            in each subclass for its format.
-+        float | dict[..]
-+            The step's reward value(s), check the specific implementation of `_computeReward()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is trunacted, always false.
-+        dict[..]
-+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-+            in each subclass for its format.
-+
-+        """
-+        
-+        #### Step the simulation using the desired physics update ##        
-+        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
-+        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
-+        self.attackers.step(attackers_action)
-+        self.defenders.step(defenders_action)
-+        #### Update and all players' information #####
-+        self._updateAndLog()
-+        #### Prepare the return values #############################
-+        obs = self._computeObs()
-+        reward = self._computeReward()
-+        terminated = self._computeTerminated()
-+        truncated = self._computeTruncated()
-+        info = self._computeInfo()
-+        
-+        #### Advance the step counter ##############################
-+        self.step_counter += 1
-+        #### Log the actions taken by the attackers and defenders ################
-+        self.attackers_actions.append(attackers_action)
-+        self.defenders_actions.append(defenders_action)
-+        
-+        return obs, reward, terminated, truncated, info
-+    
-+    
-+    def _computeAttackerActions(self):
-+        """Computes the current actions of the attackers.
-+
-+        Must be implemented in a subclass.
-+
-+        """
-+        current_attacker_state = self.attackers._get_state().copy()
-+        control_attackers = np.zeros((self.NUM_ATTACKERS, 2))
-+        for i in range(self.NUM_ATTACKERS):
-+            neg2pos, pos2neg = find_sign_change1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i])
-+            if len(neg2pos):
-+                control_attackers[i] = self.attacker_control_1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i], neg2pos)
-+            else:
-+                control_attackers[i] = (0.0, 0.0)
-+                
-+        return control_attackers
-+    
-     
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -126,7 +197,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                                 break
- 
-             return new_status
--    ################################################################################
-+        
- 
-     def _check_area(self, state, area):
-         """Check if the state is inside the area.
-@@ -147,7 +218,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return False
-     
--    ################################################################################
- 
-     def _computeObs(self):
-         """Returns the current observation of the environment.
-@@ -162,16 +232,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return obs
-     
--    ################################################################################
-     
-     def _computeReward(self):
--        #TODO: Hanyang: not finished
-         """Computes the current reward value.
- 
--        One attacker is captured: +100
--        One attacker arrived at the goal: -100
-+        Once the attacker is captured: +100
-+        Once the attacker arrived at the goal: -100
-         The defender hits the obstacle: -100
--        One step and nothing happens: 
-+        One step and nothing happens: maybe use the distance between the attacker and the defender as a sign?
-         In status, 0 stands for free, -1 stands for captured, 1 stands for arrived
- 
-         Returns
-@@ -182,15 +250,24 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         """
-         last_attacker_status = self.attackers_status[-2]
-         current_attacker_status = self.attackers_status[-1]
--        reward = -1.0
-+        reward = 0.0
-+        # check the attacker status
-         for num in range(self.NUM_ATTACKERS):
--            reward += (current_attacker_status[num] - last_attacker_status[num]) * -10
--            
-+            reward += (current_attacker_status[num] - last_attacker_status[num]) * (-200)
-+        # check the defender status
-+        current_defender_state = self.defenders._get_state().copy()
-+        reward += -200 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-+        # check the relative distance difference or relative distance
-+        current_attacker_state = self.attackers._get_state().copy()
-+        current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
-+        last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
-+        # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
-+        reward += -current_relative_distance
-+        
-         return reward
- 
-     
-     def _computeTerminated(self):
--        #TODO: Hanyang: not finished
-         """Computes the current done value.
-         done = True if all attackers have arrived or been captured.
- 
-@@ -200,9 +277,15 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-             Whether the current episode is done.
- 
-         """
--        
-+        # defender hits the obstacle or the attacker is captured or the attacker has arrived or the attacker hits the obstacle
-+        # check the attacker status
-         current_attacker_status = self.attackers_status[-1]
--        done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        attacker_done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        # check the defender status: hit the obstacle, or the attacker is captured
-+        current_defender_state = self.defenders._get_state().copy()
-+        defender_done = self._check_area(current_defender_state[0], self.obstacles)
-+        # final done
-+        done = True if attacker_done or defender_done else False
-         
-         return done
-         
-@@ -223,7 +306,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-     
-     def _computeInfo(self):
--        #TODO: Hanyang: not finished
-         """Computes the current info dict(s).
- 
-         Unused.
-@@ -240,10 +322,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         
-         return info 
-     
--    ################################################################################
- 
-     def _computeAttackerActions(self):
--        #TODO: Hanyang: not finished
-         """Computes the current actions of the attackers.
- 
-         """
-@@ -296,9 +376,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_a1, opt_a2)
-     
--    ################################################################################
-     
--    def optCrtl_1vs1(self, spat_deriv):
-+    def optCtrl_1vs1(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 1 game.
-         
-         Parameters:
-@@ -329,7 +408,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_u1, opt_u2)
- 
--    ################################################################################
- 
-     def optCtrl_1vs0(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 0 game.
-@@ -360,50 +438,4 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                 opt_a1 = self.attackers.speed * deriv1 / ctrl_len
-                 opt_a2 = self.attackers.speed * deriv2 / ctrl_len
- 
--        return (opt_a1, opt_a2)
--        """Computes the optimal control (disturbance) for the attacker in a 1 vs. 2 game.
--        
--        Parameters:
--            spat_deriv (tuple): spatial derivative in all dimensions
--        
--        Returns:
--            tuple: a tuple of optimal control of the defender (disturbances)
--        """
--        opt_d1 = self.defenders.uMax
--        opt_d2 = self.defenders.uMax
--        opt_d3 = self.defenders.uMax
--        opt_d4 = self.defenders.uMax
--        deriv3 = spat_deriv[2]
--        deriv4 = spat_deriv[3]
--        deriv5 = spat_deriv[4]
--        deriv6 = spat_deriv[5]
--        distb_len1 = np.sqrt(deriv3*deriv3 + deriv4*deriv4)
--        distb_len2 = np.sqrt(deriv5*deriv5 + deriv6*deriv6)
--        if self.dMode == "max":
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = self.defenders.speed*deriv6 / distb_len2
--        else:
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = -self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = -self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = -self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = -self.defenders.speed*deriv6 / distb_len2
--
--        return (opt_d1, opt_d2, opt_d3, opt_d4)
-\ No newline at end of file
-+        return (opt_a1, opt_a2)
-\ No newline at end of file
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/events.out.tfevents.1718475019.cs-mars-14.28855.0 b/wandb/run-20240615_111015-lmlvxua0/files/events.out.tfevents.1718475019.cs-mars-14.28855.0
deleted file mode 120000
index 645bbd1..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/events.out.tfevents.1718475019.cs-mars-14.28855.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/2024.06.15_11:10/events.out.tfevents.1718475019.cs-mars-14.28855.0
\ No newline at end of file
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/requirements.txt b/wandb/run-20240615_111015-lmlvxua0/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/wandb-metadata.json b/wandb/run-20240615_111015-lmlvxua0/files/wandb-metadata.json
deleted file mode 100644
index 8da35f6..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-15T18:10:15.411813",
-    "startedAt": "2024-06-15T18:10:15.065480",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game_controller.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game_controller.py",
-    "codePath": "safe_control_gym/experiments/train_game_controller.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "47ee2a533ec86c45ae7cc45eb6c514cc4d3acdf7"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3101.206250000001,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3812.023,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3809.843,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3805.171,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4763.42,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3835.684,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3828.601,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3822.928,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3800.064,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2214.241,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2227.973,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2340.545,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2218.796,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.197,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2390.379,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2217.366,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2214.218,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3800.101,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3809.409,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3831.117,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4786.936,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3812.326,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3802.304,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3803.889,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3807.696,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2208.137,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.799,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2207.126,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2207.384,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2233.46,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2740.57,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2206.808,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2208.089,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26466751098633
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240615_111015-lmlvxua0/files/wandb-summary.json b/wandb/run-20240615_111015-lmlvxua0/files/wandb-summary.json
deleted file mode 100644
index 7fe6f13..0000000
--- a/wandb/run-20240615_111015-lmlvxua0/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb": {"runtime": 4}, "global_step": 0, "_timestamp": 1718475019.0054197, "_runtime": 3.935732841491699, "_step": 0}
\ No newline at end of file
diff --git a/wandb/run-20240615_111015-lmlvxua0/run-lmlvxua0.wandb b/wandb/run-20240615_111015-lmlvxua0/run-lmlvxua0.wandb
deleted file mode 100644
index a1b36b6..0000000
Binary files a/wandb/run-20240615_111015-lmlvxua0/run-lmlvxua0.wandb and /dev/null differ
diff --git a/wandb/run-20240615_111234-06vgglyi/files/code/safe_control_gym/experiments/train_game_controller.py b/wandb/run-20240615_111234-06vgglyi/files/code/safe_control_gym/experiments/train_game_controller.py
deleted file mode 100644
index d66c923..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/code/safe_control_gym/experiments/train_game_controller.py
+++ /dev/null
@@ -1,362 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e6
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{datetime.now().strftime("%Y.%m.%d_%H:%M")}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-        # from cleanrl_utils.evals.ppo_eval import evaluate
-
-        # episodic_returns = evaluate(
-        #     model_path,
-        #     make_env,
-        #     args.env_id,
-        #     eval_episodes=10,
-        #     run_name=f"{run_name}-eval",
-        #     Model=Agent,
-        #     device=device,
-        #     gamma=args.gamma,
-        # )
-        # for idx, episodic_return in enumerate(episodic_returns):
-        #     writer.add_scalar("eval/episodic_return", episodic_return, idx)
-
-        # if args.upload_model:
-        #     from cleanrl_utils.huggingface import push_to_hub
-
-        #     repo_name = f"{args.env_id}-{args.exp_name}-seed{args.seed}"
-        #     repo_id = f"{args.hf_entity}/{repo_name}" if args.hf_entity else repo_name
-        #     push_to_hub(args, episodic_returns, repo_id, "PPO", f"runs/{run_name}", f"videos/{run_name}-eval")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240615_111234-06vgglyi/files/conda-environment.yaml b/wandb/run-20240615_111234-06vgglyi/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240615_111234-06vgglyi/files/config.yaml b/wandb/run-20240615_111234-06vgglyi/files/config.yaml
deleted file mode 100644
index 3918f01..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game_controller
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 1000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 244
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game_controller.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718475154.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240615_111234-06vgglyi/files/diff.patch b/wandb/run-20240615_111234-06vgglyi/files/diff.patch
deleted file mode 100644
index 9ddbe93..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/diff.patch
+++ /dev/null
@@ -1,766 +0,0 @@
-diff --git a/ceshi_game.py b/ceshi_game.py
-index 5994a62..4d5bd4b 100644
---- a/ceshi_game.py
-+++ b/ceshi_game.py
-@@ -1,9 +1,65 @@
-+import  numpy as np
- from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
- 
-+initial_attacker = np.array([[-0.1, 0.0]])
-+initial_defender = np.array([[0.0, 0.0]])
-+env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
-+# print(f"The initial state is {env.}. \n")
-+# obs, info = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
- 
--env = ReachAvoidGameEnv()
--obs = env.reset()
--print(f"The state space of the env is {env.observation_space}. \n")
--print(f"The action space of the env is {env.action_space}. \n")
--print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
--print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-\ No newline at end of file
-+# obs = env.reset()
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The obs is {obs}. \n")
-+# print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+
-+# for i in range(10):
-+#     obs = env.reset()
-+#     print(f"The initial player seed is {env.initial_players_seed}. \n")
-+#     print(f"The obs is {obs}. \n")
-+
-+
-+action = np.array([-0.1, 0.0])
-+
-+for i in range(10):
-+    obs, reward, terminated, truncated, info = env.step(action)
-+    print(f"The obs is {obs}. \n")
-+    print(f"The reward is {reward}. \n")
-+    # print(f"The state of the attacker is {env.state[0]} and the state of the defender is {env.state[1]}. \n")
-+    if terminated or truncated:
-+        print(f"The game is terminated: {terminated} and the game is truncated: {truncated} at the step {i}. \n")
-+        break
-+
-+# current_attackers = np.array([[0.0, 0.0]])
-+# current_defenders = np.array([[3.0, 4.0]])
-+# distance = np.linalg.norm(current_attackers[0] - current_defenders[0])
-+# print(f"The distance between the attacker and the defender is {distance}. \n")
-+
-+# obstacles = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 0.6]}
-+
-+# def _check_area(state, area):
-+#         """Check if the state is inside the area.
-+
-+#         Parameters:
-+#             state (np.ndarray): the state to check
-+#             area (dict): the area dictionary to be checked.
-+        
-+#         Returns:
-+#             bool: True if the state is inside the area, False otherwise.
-+#         """
-+#         x, y = state  # Unpack the state assuming it's a 2D coordinate
-+
-+#         for bounds in area.values():
-+#             x_lower, x_upper, y_lower, y_upper = bounds
-+#             if x_lower <= x <= x_upper and y_lower <= y <= y_upper:
-+#                 return True
-+
-+#         return False
-+
-+# print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
-+# reward = 0.0
-+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
-index 5e7b3f0..d1032b2 100644
---- a/safe_control_gym/envs/__init__.py
-+++ b/safe_control_gym/envs/__init__.py
-@@ -61,4 +61,7 @@ register(idx='quadrotor_random',
- 
- register(idx='quadrotor_randomhj',
-          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
--         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-\ No newline at end of file
-+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-+
-+register(idx='reach_avoid',
-+         entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 0ea88d5..169382c 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -14,6 +14,7 @@ class Dynamics:
-     SIG = {'id': 'sig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.0}           # Base single integrator dynamics
-     FSIG = {'id': 'fsig', 'action_dim': 2, 'state_dim': 2, 'speed': 1.5}         # Faster single integrator dynamics with feedback
-     
-+    
- class BaseGameEnv(gym.Env):
-     """Base class for the multi-agent reach-avoid game Gym environments."""
-     
-@@ -26,6 +27,7 @@ class BaseGameEnv(gym.Env):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed: int = None,
-+                 random_init: bool = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +49,7 @@ class BaseGameEnv(gym.Env):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init : bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -67,15 +70,12 @@ class BaseGameEnv(gym.Env):
-         #### Input initial states ####################################
-         self.init_attackers = initial_attacker
-         self.init_defenders = initial_defender
--        #### Create action and observation spaces ##################
--        self.action_space = self._actionSpace()
--        self.observation_space = self._observationSpace()
-         #### Housekeeping ##########################################
-+        self.random_init = random_init
-         self._housekeeping()
-         #### Update and all players' information #####
-         self._updateAndLog()
-     
--    ################################################################################
- 
-     def _housekeeping(self):
-         """Housekeeping function.
-@@ -84,8 +84,10 @@ class BaseGameEnv(gym.Env):
-         in the `reset()` function.
- 
-         """
--        if self.init_attackers is None and self.init_defenders is None:
--            self.init_attackers, self.init_defenders = self.initial_players()            
-+        if self.random_init:
-+            self.init_attackers, self.init_defenders = self.initial_players()
-+        else:
-+            assert self.init_attackers is not None and self.init_defenders is not None, "Need to provide initial positions for all players."     
-         #### Set attackers and defenders ##########################
-         self.attackers = make_agents(self.ATTACKER_PHYSICS, self.NUM_ATTACKERS, self.init_attackers, self.CTRL_FREQ)
-         self.defenders = make_agents(self.DEFENDER_PHYSICS, self.NUM_DEFENDERS, self.init_defenders, self.CTRL_FREQ)
-@@ -96,21 +98,26 @@ class BaseGameEnv(gym.Env):
-         self.attackers_status = []  # 0 stands for free, -1 stands for captured, 1 stands for arrived 
-         self.attackers_actions = []
-         self.defenders_actions = []
-+        # self.last_relative_distance = np.zeros((self.NUM_ATTACKERS, self.NUM_DEFENDERS))
- 
--    ################################################################################
- 
-     def _updateAndLog(self):
-         """Update and log all players' information after inialization, reset(), or step.
- 
-         """
-         # Update the state
--        self.state = np.vstack([self.attackers._get_state().copy(), self.defenders._get_state().copy()])
-+        current_attackers = self.attackers._get_state().copy()
-+        current_defenders = self.defenders._get_state().copy()
-+        
-+        self.state = np.vstack([current_attackers, current_defenders])
-         # Log the state and trajectory information
--        self.attackers_traj.append(self.attackers._get_state().copy())
--        self.defenders_traj.append(self.defenders._get_state().copy())
-+        self.attackers_traj.append(current_attackers)
-+        self.defenders_traj.append(current_defenders)
-         self.attackers_status.append(self._getAttackersStatus().copy())
-+        # for i in range(self.NUM_ATTACKERS):
-+        #     for j in range(self.NUM_DEFENDERS):
-+        #         self.last_relative_distance[i, j] = np.linalg.norm(current_attackers[i] - current_defenders[j])
-     
--    ################################################################################
-     
-     def initial_players(self):
-         '''Set the initial positions for all players.
-@@ -122,7 +129,7 @@ class BaseGameEnv(gym.Env):
-         np.random.seed(self.initial_players_seed)
-     
-         # Map boundaries
--        min_val, max_val = -1.0, 1.0
-+        min_val, max_val = -0.99, 0.99
-         
-         # Obstacles and target areas
-         obstacles = [
-@@ -170,9 +177,9 @@ class BaseGameEnv(gym.Env):
-         
-         return np.array([attacker_pos]), np.array([defender_pos])
- 
--    ################################################################################
-     
--    def reset(self, seed : int = None):
-+    def reset(self, seed : int = None,
-+              options : dict = None):
-         """Resets the environment.
- 
-         Parameters
-@@ -198,65 +205,10 @@ class BaseGameEnv(gym.Env):
-         self._updateAndLog()
-         #### Prepare the observation #############################
-         obs = self._computeObs()
--        
--        return obs
--    
--    ################################################################################
--
--    def step(self,action):
--        #TODO: Hanyang: change the action only for the defender
--        """Advances the environment by one simulation step.
--
--        Parameters
--        ----------
--        action : ndarray | (dim_action, )
--            The input action for the defender.
--
--        Returns
--        -------
--        ndarray | dict[..]
--            The step's observation, check the specific implementation of `_computeObs()`
--            in each subclass for its format.
--        float | dict[..]
--            The step's reward value(s), check the specific implementation of `_computeReward()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
--            in each subclass for its format.
--        bool | dict[..]
--            Whether the current episode is trunacted, always false.
--        dict[..]
--            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
--            in each subclass for its format.
--
--        """
--        
--        #### Step the simulation using the desired physics update ##        
--        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
--        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
--        self.attackers.step(attackers_action)
--        self.defenders.step(defenders_action)
--        #### Update and all players' information #####
--        self._updateAndLog()
--        #### Prepare the return values #############################
--        obs = self._computeObs()
--        reward = self._computeReward()
--        terminated = self._computeTerminated()
--        truncated = self._computeTruncated()
-         info = self._computeInfo()
-         
--        #### Advance the step counter ##############################
--        self.step_counter += 1
--        #### Log the actions taken by the attackers and defenders ################
--        self.attackers_actions.append(attackers_action)
--        self.defenders_actions.append(defenders_action)
--        
--        return obs, reward, terminated, truncated, info
-+        return obs, info
-     
--    ################################################################################
- 
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -266,37 +218,15 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
--    
--    def _actionSpace(self):
--        """Returns the action space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--           
--    ################################################################################
--
--    def _observationSpace(self):
--        """Returns the observation space of the environment.
--
--        Must be implemented in a subclass.
--
--        """
--        raise NotImplementedError
--    
--    ################################################################################
-     
-     def _computeObs(self):
-         """Returns the current observation of the environment.
- 
--        Must be implemented in a subclass.
--
-         """
--        raise NotImplementedError
-+        obs = self.state.flatten()
-+        
-+        return obs
-     
--    ################################################################################
- 
-     def _computeReward(self):
-         """Computes the current reward value(s).
-@@ -311,7 +241,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeTerminated(self):
-         """Computes the current terminated value(s).
-@@ -321,7 +250,6 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
-     
--    ################################################################################
- 
-     def _computeTruncated(self):
-         """Computes the current truncated value(s).
-@@ -331,22 +259,11 @@ class BaseGameEnv(gym.Env):
-         """
-         raise NotImplementedError
- 
--    ################################################################################
- 
-     def _computeInfo(self):
-         """Computes the current info dict(s).
- 
-         Must be implemented in a subclass.
- 
--        """
--        raise NotImplementedError
--
--    ################################################################################
--
--    def _computeAttackerActions(self):
--        """Computes the current actions of the attackers.
--
--        Must be implemented in a subclass.
--
-         """
-         raise NotImplementedError
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseRLGame.py b/safe_control_gym/envs/gym_game/BaseRLGame.py
-index cdd42ca..c62dd6e 100644
---- a/safe_control_gym/envs/gym_game/BaseRLGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseRLGame.py
-@@ -26,6 +26,7 @@ class BaseRLGameEnv(BaseGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  output_folder='results',
-                  ):
-         """Initialization of a generic aviary environment.
-@@ -47,6 +48,7 @@ class BaseRLGameEnv(BaseGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         output_folder : str, optional
-             The folder where to save logs.
- 
-@@ -55,32 +57,33 @@ class BaseRLGameEnv(BaseGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-+        
-+        #### Create action and observation spaces ##################
-+        self.action_space = self._actionSpace()
-+        self.observation_space = self._observationSpace()
-        
--
--    ################################################################################
--
-    
-     def _actionSpace(self):
-         """Returns the action space of the environment.
--        Formulation: [attackers' action spaces, defenders' action spaces]
-+        Formulation: [defenders' action spaces]
-         Returns
-         -------
-         spaces.Box
--            A Box of size NUM_PLAYERS x 2, or 1, depending on the action type.
-+            A Box of size NUM_DEFENDERS x 2, or 1, depending on the action type.
- 
-         """
-         
--        if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
--            attacker_lower_bound = np.array([-1.0, -1.0])
--            attacker_upper_bound = np.array([+1.0, +1.0])
--        elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
--            attacker_lower_bound = np.array([-1.0])
--            attacker_upper_bound = np.array([+1.0])
--        else:
--            print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
--            exit()
-+        # if self.ATTACKER_PHYSICS == Dynamics.SIG or self.ATTACKER_PHYSICS == Dynamics.FSIG:
-+        #     attacker_lower_bound = np.array([-1.0, -1.0])
-+        #     attacker_upper_bound = np.array([+1.0, +1.0])
-+        # elif self.ATTACKER_PHYSICS == Dynamics.DUB3D:
-+        #     attacker_lower_bound = np.array([-1.0])
-+        #     attacker_upper_bound = np.array([+1.0])
-+        # else:
-+        #     print("[ERROR] in Attacker Action Space, BaseRLGameEnv._actionSpace()")
-+        #     exit()
-         
-         if self.DEFENDER_PHYSICS == Dynamics.SIG or self.DEFENDER_PHYSICS == Dynamics.FSIG:
-             defender_lower_bound = np.array([-1.0, -1.0])
-@@ -92,28 +95,28 @@ class BaseRLGameEnv(BaseGameEnv):
-             print("[ERROR] in Defender Action Space, BaseRLGameEnv._actionSpace()")
-             exit()
-         
--        attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
--        attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_lower_bound = np.array([attacker_lower_bound for i in range(self.NUM_ATTACKERS)])
-+        # attackers_upper_bound = np.array([attacker_upper_bound for i in range(self.NUM_ATTACKERS)])
- 
--        if self.NUM_DEFENDERS > 0:
--            defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
--            defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-+        # if self.NUM_DEFENDERS > 0:
-+        #     defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        #     defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-             
--            act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
--            act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
--        else:
--            act_lower_bound = attackers_lower_bound
--            act_upper_bound = attackers_upper_bound
--
-+        #     act_lower_bound = np.concatenate((attackers_lower_bound, defenders_lower_bound), axis=0)
-+        #     act_upper_bound = np.concatenate((attackers_upper_bound, defenders_upper_bound), axis=0)
-+        # else:
-+        #     act_lower_bound = attackers_lower_bound
-+        #     act_upper_bound = attackers_upper_bound
-+            
-+        defenders_lower_bound = np.array([defender_lower_bound for i in range(self.NUM_DEFENDERS)])
-+        defenders_upper_bound = np.array([defender_upper_bound for i in range(self.NUM_DEFENDERS)])
-         # Flatten the lower and upper bounds to ensure the action space shape is (4,)
--        act_lower_bound = act_lower_bound.flatten()
--        act_upper_bound = act_upper_bound.flatten()
-+        act_lower_bound = defenders_lower_bound.flatten()
-+        act_upper_bound = defenders_upper_bound.flatten()
- 
-         return spaces.Box(low=act_lower_bound, high=act_upper_bound, dtype=np.float32)
-  
- 
--    ################################################################################
--
-     def _observationSpace(self):
-         """Returns the observation space of the environment.
-         Formulation: [attackers' obs spaces, defenders' obs spaces]
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index b5344ff..0ea357c 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -24,13 +24,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                  initial_defender: np.ndarray=None,  # shape (num_defenders, state_dim)
-                  ctrl_freq: int = 200,
-                  seed = 42,
-+                 random_init = True,
-                  uMode="min", 
-                  dMode="max",
-                  output_folder='results',
-                  game_length_sec=20,
--                 map={'map': [-1., 1., -1., 1.]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-+                 map={'map': [-1.0, 1.0, -1.0, 1.0]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  des={'goal0': [0.6, 0.8, 0.1, 0.3]},  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
--                 obstacles: dict = None,  
-+                 obstacles: dict = {'obs1': [-0.1, 0.1, -1.0, -0.3], 'obs2': [-0.1, 0.1, 0.3, 1.0]}  # Hanyang: rectangele [xmin, xmax, ymin, ymax]
-                  ):
-         """Initialization of a generic aviary environment.
- 
-@@ -51,6 +52,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         ctrl_freq : int, optional
-             The control frequency of the environment.
-         seed : int, optional
-+        random_init: bool, optional
-         uMode : str, optional
-             The mode of the attacker, default is "min".
-         dMode : str, optional
-@@ -71,7 +73,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         super().__init__(num_attackers=num_attackers, num_defenders=num_defenders, 
-                          attackers_dynamics=attackers_dynamics, defenders_dynamics=defenders_dynamics, 
-                          initial_attacker=initial_attacker, initial_defender=initial_defender, 
--                         ctrl_freq=ctrl_freq, seed=seed, output_folder=output_folder
-+                         ctrl_freq=ctrl_freq, seed=seed, random_init=random_init, output_folder=output_folder
-                          )
-         
-         assert map is not None, "Map must be provided in the game."
-@@ -84,13 +86,82 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         self.uMode = uMode
-         self.dMode = dMode
-         # Load necessary values for the attacker control
--        #TODO: Hanyang: not finished
-         self.grid1vs0 = Grid(np.array([-1.0, -1.0]), np.array([1.0, 1.0]), 2, np.array([100, 100])) 
--        self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
--        self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-+        # self.grid1vs1 = Grid(np.array([-1.0, -1.0, -1.0, -1.0]), np.array([1.0, 1.0, 1.0, 1.0]), 4, np.array([45, 45, 45, 45]))
-+        # self.value1vs1 = np.load('safe_control_gym/envs/gym_game/values/1vs1Attacker.npy')
-         self.value1vs0 = np.load('safe_control_gym/envs/gym_game/values/1vs0Attacker.npy')
- 
--    ################################################################################
-+    
-+    def step(self, action):
-+        """Advances the environment by one simulation step.
-+
-+        Parameters
-+        ----------
-+        action : ndarray | (dim_action, )
-+            The input action for the defender.
-+
-+        Returns
-+        -------
-+        ndarray | dict[..]
-+            The step's observation, check the specific implementation of `_computeObs()`
-+            in each subclass for its format.
-+        float | dict[..]
-+            The step's reward value(s), check the specific implementation of `_computeReward()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is over, check the specific implementation of `_computeTerminated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is truncated, check the specific implementation of `_computeTruncated()`
-+            in each subclass for its format.
-+        bool | dict[..]
-+            Whether the current episode is trunacted, always false.
-+        dict[..]
-+            Additional information as a dictionary, check the specific implementation of `_computeInfo()`
-+            in each subclass for its format.
-+
-+        """
-+        
-+        #### Step the simulation using the desired physics update ##        
-+        attackers_action = self._computeAttackerActions()  # ndarray, shape (num_defenders, dim_action)
-+        defenders_action = action.copy().reshape(self.NUM_DEFENDERS, 2)  # ndarray, shape (num_defenders, dim_action)
-+        self.attackers.step(attackers_action)
-+        self.defenders.step(defenders_action)
-+        #### Update and all players' information #####
-+        self._updateAndLog()
-+        #### Prepare the return values #############################
-+        obs = self._computeObs()
-+        reward = self._computeReward()
-+        terminated = self._computeTerminated()
-+        truncated = self._computeTruncated()
-+        info = self._computeInfo()
-+        
-+        #### Advance the step counter ##############################
-+        self.step_counter += 1
-+        #### Log the actions taken by the attackers and defenders ################
-+        self.attackers_actions.append(attackers_action)
-+        self.defenders_actions.append(defenders_action)
-+        
-+        return obs, reward, terminated, truncated, info
-+    
-+    
-+    def _computeAttackerActions(self):
-+        """Computes the current actions of the attackers.
-+
-+        Must be implemented in a subclass.
-+
-+        """
-+        current_attacker_state = self.attackers._get_state().copy()
-+        control_attackers = np.zeros((self.NUM_ATTACKERS, 2))
-+        for i in range(self.NUM_ATTACKERS):
-+            neg2pos, pos2neg = find_sign_change1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i])
-+            if len(neg2pos):
-+                control_attackers[i] = self.attacker_control_1vs0(self.grid1vs0, self.value1vs0, current_attacker_state[i], neg2pos)
-+            else:
-+                control_attackers[i] = (0.0, 0.0)
-+                
-+        return control_attackers
-+    
-     
-     def _getAttackersStatus(self):
-         """Returns the current status of all attackers.
-@@ -126,7 +197,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                                 break
- 
-             return new_status
--    ################################################################################
-+        
- 
-     def _check_area(self, state, area):
-         """Check if the state is inside the area.
-@@ -147,7 +218,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return False
-     
--    ################################################################################
- 
-     def _computeObs(self):
-         """Returns the current observation of the environment.
-@@ -162,16 +232,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return obs
-     
--    ################################################################################
-     
-     def _computeReward(self):
--        #TODO: Hanyang: not finished
-         """Computes the current reward value.
- 
--        One attacker is captured: +100
--        One attacker arrived at the goal: -100
-+        Once the attacker is captured: +100
-+        Once the attacker arrived at the goal: -100
-         The defender hits the obstacle: -100
--        One step and nothing happens: 
-+        One step and nothing happens: maybe use the distance between the attacker and the defender as a sign?
-         In status, 0 stands for free, -1 stands for captured, 1 stands for arrived
- 
-         Returns
-@@ -182,15 +250,24 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         """
-         last_attacker_status = self.attackers_status[-2]
-         current_attacker_status = self.attackers_status[-1]
--        reward = -1.0
-+        reward = 0.0
-+        # check the attacker status
-         for num in range(self.NUM_ATTACKERS):
--            reward += (current_attacker_status[num] - last_attacker_status[num]) * -10
--            
-+            reward += (current_attacker_status[num] - last_attacker_status[num]) * (-200)
-+        # check the defender status
-+        current_defender_state = self.defenders._get_state().copy()
-+        reward += -200 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-+        # check the relative distance difference or relative distance
-+        current_attacker_state = self.attackers._get_state().copy()
-+        current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
-+        last_relative_distance = np.linalg.norm(self.attackers_traj[-2][0] - self.defenders_traj[-2][0])
-+        # reward += (current_relative_distance - last_relative_distance) * -1.0 / (2*np.sqrt(2))
-+        reward += -current_relative_distance
-+        
-         return reward
- 
-     
-     def _computeTerminated(self):
--        #TODO: Hanyang: not finished
-         """Computes the current done value.
-         done = True if all attackers have arrived or been captured.
- 
-@@ -200,9 +277,15 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-             Whether the current episode is done.
- 
-         """
--        
-+        # defender hits the obstacle or the attacker is captured or the attacker has arrived or the attacker hits the obstacle
-+        # check the attacker status
-         current_attacker_status = self.attackers_status[-1]
--        done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        attacker_done = np.all((current_attacker_status == 1) | (current_attacker_status == -1))
-+        # check the defender status: hit the obstacle, or the attacker is captured
-+        current_defender_state = self.defenders._get_state().copy()
-+        defender_done = self._check_area(current_defender_state[0], self.obstacles)
-+        # final done
-+        done = True if attacker_done or defender_done else False
-         
-         return done
-         
-@@ -223,7 +306,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-     
-     def _computeInfo(self):
--        #TODO: Hanyang: not finished
-         """Computes the current info dict(s).
- 
-         Unused.
-@@ -240,10 +322,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         
-         return info 
-     
--    ################################################################################
- 
-     def _computeAttackerActions(self):
--        #TODO: Hanyang: not finished
-         """Computes the current actions of the attackers.
- 
-         """
-@@ -296,9 +376,8 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_a1, opt_a2)
-     
--    ################################################################################
-     
--    def optCrtl_1vs1(self, spat_deriv):
-+    def optCtrl_1vs1(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 1 game.
-         
-         Parameters:
-@@ -329,7 +408,6 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
- 
-         return (opt_u1, opt_u2)
- 
--    ################################################################################
- 
-     def optCtrl_1vs0(self, spat_deriv):
-         """Computes the optimal control (disturbance) for the attacker in a 1 vs. 0 game.
-@@ -360,50 +438,4 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                 opt_a1 = self.attackers.speed * deriv1 / ctrl_len
-                 opt_a2 = self.attackers.speed * deriv2 / ctrl_len
- 
--        return (opt_a1, opt_a2)
--        """Computes the optimal control (disturbance) for the attacker in a 1 vs. 2 game.
--        
--        Parameters:
--            spat_deriv (tuple): spatial derivative in all dimensions
--        
--        Returns:
--            tuple: a tuple of optimal control of the defender (disturbances)
--        """
--        opt_d1 = self.defenders.uMax
--        opt_d2 = self.defenders.uMax
--        opt_d3 = self.defenders.uMax
--        opt_d4 = self.defenders.uMax
--        deriv3 = spat_deriv[2]
--        deriv4 = spat_deriv[3]
--        deriv5 = spat_deriv[4]
--        deriv6 = spat_deriv[5]
--        distb_len1 = np.sqrt(deriv3*deriv3 + deriv4*deriv4)
--        distb_len2 = np.sqrt(deriv5*deriv5 + deriv6*deriv6)
--        if self.dMode == "max":
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = self.defenders.speed*deriv6 / distb_len2
--        else:
--            if distb_len1 == 0:
--                opt_d1 = 0.0
--                opt_d2 = 0.0
--            else:
--                opt_d1 = -self.defenders.speed*deriv3 / distb_len1
--                opt_d2 = -self.defenders.speed*deriv4 / distb_len1
--            if distb_len2 == 0:
--                opt_d3 = 0.0
--                opt_d4 = 0.0
--            else:
--                opt_d3 = -self.defenders.speed*deriv5 / distb_len2
--                opt_d4 = -self.defenders.speed*deriv6 / distb_len2
--
--        return (opt_d1, opt_d2, opt_d3, opt_d4)
-\ No newline at end of file
-+        return (opt_a1, opt_a2)
-\ No newline at end of file
diff --git a/wandb/run-20240615_111234-06vgglyi/files/events.out.tfevents.1718475158.cs-mars-14.29295.0 b/wandb/run-20240615_111234-06vgglyi/files/events.out.tfevents.1718475158.cs-mars-14.29295.0
deleted file mode 120000
index 5ef99bf..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/events.out.tfevents.1718475158.cs-mars-14.29295.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/2024.06.15_11:12/events.out.tfevents.1718475158.cs-mars-14.29295.0
\ No newline at end of file
diff --git a/wandb/run-20240615_111234-06vgglyi/files/requirements.txt b/wandb/run-20240615_111234-06vgglyi/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240615_111234-06vgglyi/files/wandb-metadata.json b/wandb/run-20240615_111234-06vgglyi/files/wandb-metadata.json
deleted file mode 100644
index 8822999..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-15T18:12:35.074705",
-    "startedAt": "2024-06-15T18:12:34.727495",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game_controller.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game_controller.py",
-    "codePath": "safe_control_gym/experiments/train_game_controller.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "47ee2a533ec86c45ae7cc45eb6c514cc4d3acdf7"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 2793.9938437500005,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2411.432,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2883.065,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2616.756,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2241.476,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2891.817,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2879.89,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2932.998,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4760.212,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3154.31,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2555.351,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2651.072,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2892.504,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2801.956,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2899.68,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3034.837,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2715.639,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2715.909,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2725.843,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2729.281,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2727.63,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2750.819,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2561.968,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2736.637,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3766.529,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2300.065,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2248.132,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2759.131,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2286.8,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2993.508,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2488.769,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2567.433,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2726.354,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26470184326172
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240615_111234-06vgglyi/files/wandb-summary.json b/wandb/run-20240615_111234-06vgglyi/files/wandb-summary.json
deleted file mode 100644
index 8950c45..0000000
--- a/wandb/run-20240615_111234-06vgglyi/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 999424, "_timestamp": 1718476054.8928514, "_runtime": 900.162228345871, "_step": 6905, "charts/episodic_return": -247.1016387939453, "charts/episodic_length": 54.0, "charts/learning_rate": 1.2295081432966981e-06, "losses/value_loss": 0.022712096571922302, "losses/policy_loss": 0.0004767999053001404, "losses/entropy": 1.7101378440856934, "losses/old_approx_kl": 0.0009421189315617085, "losses/approx_kl": 1.4443416148424149e-05, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9153353571891785, "charts/SPS": 1116.0, "_wandb": {"runtime": 899}}
\ No newline at end of file
diff --git a/wandb/run-20240615_111234-06vgglyi/run-06vgglyi.wandb b/wandb/run-20240615_111234-06vgglyi/run-06vgglyi.wandb
deleted file mode 100644
index 694648c..0000000
Binary files a/wandb/run-20240615_111234-06vgglyi/run-06vgglyi.wandb and /dev/null differ
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/code/safe_control_gym/experiments/train_game_controller.py b/wandb/run-20240615_144011-5bzvjoay/files/code/safe_control_gym/experiments/train_game_controller.py
deleted file mode 100644
index da0dd2d..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/code/safe_control_gym/experiments/train_game_controller.py
+++ /dev/null
@@ -1,362 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-        # from cleanrl_utils.evals.ppo_eval import evaluate
-
-        # episodic_returns = evaluate(
-        #     model_path,
-        #     make_env,
-        #     args.env_id,
-        #     eval_episodes=10,
-        #     run_name=f"{run_name}-eval",
-        #     Model=Agent,
-        #     device=device,
-        #     gamma=args.gamma,
-        # )
-        # for idx, episodic_return in enumerate(episodic_returns):
-        #     writer.add_scalar("eval/episodic_return", episodic_return, idx)
-
-        # if args.upload_model:
-        #     from cleanrl_utils.huggingface import push_to_hub
-
-        #     repo_name = f"{args.env_id}-{args.exp_name}-seed{args.seed}"
-        #     repo_id = f"{args.hf_entity}/{repo_name}" if args.hf_entity else repo_name
-        #     push_to_hub(args, episodic_returns, repo_id, "PPO", f"runs/{run_name}", f"videos/{run_name}-eval")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/conda-environment.yaml b/wandb/run-20240615_144011-5bzvjoay/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/config.yaml b/wandb/run-20240615_144011-5bzvjoay/files/config.yaml
deleted file mode 100644
index 71a22b0..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game_controller
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game_controller.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718487611.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/diff.patch b/wandb/run-20240615_144011-5bzvjoay/files/diff.patch
deleted file mode 100644
index 7226b1c..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/diff.patch
+++ /dev/null
@@ -1,39 +0,0 @@
-diff --git a/safe_control_gym/experiments/train_game_controller.py b/safe_control_gym/experiments/train_game_controller.py
-index d66c923..da0dd2d 100644
---- a/safe_control_gym/experiments/train_game_controller.py
-+++ b/safe_control_gym/experiments/train_game_controller.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 1e6
-+    total_timesteps: int = 1e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-@@ -152,7 +152,7 @@ if __name__ == "__main__":
-     args.minibatch_size = int(args.batch_size // args.num_minibatches)
-     args.num_iterations = int(args.total_timesteps // args.batch_size)
-     # Hanyang: make saving directory
--    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{datetime.now().strftime("%Y.%m.%d_%H:%M")}' )
-+    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-     if not os.path.exists(run_name):
-         os.makedirs(run_name+'/')
-         
-diff --git a/training_results/game/ppo/2024/2024.06.15_11:12/events.out.tfevents.1718475158.cs-mars-14.29295.0 b/training_results/game/ppo/2024/2024.06.15_11:12/events.out.tfevents.1718475158.cs-mars-14.29295.0
-deleted file mode 100644
-index 4b365e0..0000000
-Binary files a/training_results/game/ppo/2024/2024.06.15_11:12/events.out.tfevents.1718475158.cs-mars-14.29295.0 and /dev/null differ
-diff --git a/training_results/game/ppo/2024/2024.06.15_11:12/train_game_controller.cleanrl_model b/training_results/game/ppo/2024/2024.06.15_11:12/train_game_controller.cleanrl_model
-deleted file mode 100644
-index e5958ff..0000000
-Binary files a/training_results/game/ppo/2024/2024.06.15_11:12/train_game_controller.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index dafbb13..5881952 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240615_111234-06vgglyi
-\ No newline at end of file
-+run-20240615_144011-5bzvjoay
-\ No newline at end of file
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/events.out.tfevents.1718487615.cs-mars-14.13248.0 b/wandb/run-20240615_144011-5bzvjoay/files/events.out.tfevents.1718487615.cs-mars-14.13248.0
deleted file mode 120000
index 50f9dbb..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/events.out.tfevents.1718487615.cs-mars-14.13248.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718487615.cs-mars-14.13248.0
\ No newline at end of file
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/requirements.txt b/wandb/run-20240615_144011-5bzvjoay/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/wandb-metadata.json b/wandb/run-20240615_144011-5bzvjoay/files/wandb-metadata.json
deleted file mode 100644
index fc3de8d..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-15T21:40:11.683686",
-    "startedAt": "2024-06-15T21:40:11.349680",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game_controller.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game_controller.py",
-    "codePath": "safe_control_gym/experiments/train_game_controller.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "3e2a3c6537f01b88400279a177ed10706c77fb01"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3117.6303437499996,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3908.836,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3900.656,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4873.891,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3879.943,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3885.557,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3912.281,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3895.253,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3880.433,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2207.377,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2215.25,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.157,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.822,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2209.416,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2200.16,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2223.355,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2315.132,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3886.994,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3879.389,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4906.418,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3900.993,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3889.569,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3908.372,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3881.608,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3880.076,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2212.426,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2212.151,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2211.476,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.174,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2201.675,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2207.29,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2201.904,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.137,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26449966430664
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240615_144011-5bzvjoay/files/wandb-summary.json b/wandb/run-20240615_144011-5bzvjoay/files/wandb-summary.json
deleted file mode 100644
index de7430d..0000000
--- a/wandb/run-20240615_144011-5bzvjoay/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1718496401.472506, "_runtime": 8790.12013411522, "_step": 95393, "charts/episodic_return": -282.69708251953125, "charts/episodic_length": 81.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.0016734156524762511, "losses/policy_loss": -4.269927740097046e-05, "losses/entropy": 4.605074882507324, "losses/old_approx_kl": -1.4230608940124512e-05, "losses/approx_kl": 3.073364496231079e-08, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9891042113304138, "charts/SPS": 1138.0, "_wandb": {"runtime": 8789}}
\ No newline at end of file
diff --git a/wandb/run-20240615_144011-5bzvjoay/run-5bzvjoay.wandb b/wandb/run-20240615_144011-5bzvjoay/run-5bzvjoay.wandb
deleted file mode 100644
index 9b63378..0000000
Binary files a/wandb/run-20240615_144011-5bzvjoay/run-5bzvjoay.wandb and /dev/null differ
diff --git a/wandb/run-20240617_191659-zknrsugm/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240617_191659-zknrsugm/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index 53325c0..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,341 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 2e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240617_191659-zknrsugm/files/conda-environment.yaml b/wandb/run-20240617_191659-zknrsugm/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240617_191659-zknrsugm/files/config.yaml b/wandb/run-20240617_191659-zknrsugm/files/config.yaml
deleted file mode 100644
index 943fb39..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 20000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 4882
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718677019.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240617_191659-zknrsugm/files/diff.patch b/wandb/run-20240617_191659-zknrsugm/files/diff.patch
deleted file mode 100644
index 4c6e52d..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/diff.patch
+++ /dev/null
@@ -1,30 +0,0 @@
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index a761901..53325c0 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 1e7
-+    total_timesteps: int = 2e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718487615.cs-mars-14.13248.0 b/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718487615.cs-mars-14.13248.0
-deleted file mode 100644
-index 795591d..0000000
-Binary files a/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718487615.cs-mars-14.13248.0 and /dev/null differ
-diff --git a/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model b/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index 7dc57b2..0000000
-Binary files a/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 5881952..3038406 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240615_144011-5bzvjoay
-\ No newline at end of file
-+run-20240617_191659-zknrsugm
-\ No newline at end of file
diff --git a/wandb/run-20240617_191659-zknrsugm/files/events.out.tfevents.1718677023.cs-mars-14.16320.0 b/wandb/run-20240617_191659-zknrsugm/files/events.out.tfevents.1718677023.cs-mars-14.16320.0
deleted file mode 120000
index a35990c..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/events.out.tfevents.1718677023.cs-mars-14.16320.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1718677023.cs-mars-14.16320.0
\ No newline at end of file
diff --git a/wandb/run-20240617_191659-zknrsugm/files/requirements.txt b/wandb/run-20240617_191659-zknrsugm/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240617_191659-zknrsugm/files/wandb-metadata.json b/wandb/run-20240617_191659-zknrsugm/files/wandb-metadata.json
deleted file mode 100644
index 9445106..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-18T02:16:59.706812",
-    "startedAt": "2024-06-18T02:16:59.317112",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "30c6ce606d1fa25c6c0b8365045249920e5179c4"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3391.2106874999995,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3780.44,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3705.22,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3810.687,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3757.907,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4762.267,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3813.849,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3752.935,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3750.002,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2294.184,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2948.383,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3437.843,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3017.48,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2723.864,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2883.982,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3548.066,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3167.062,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3742.234,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3745.366,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3753.873,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3806.238,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4767.275,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3747.719,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3750.624,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3739.542,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2969.566,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2325.844,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2717.313,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2651.118,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.886,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2810.78,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3713.461,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2885.732,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.27551651000977
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240617_191659-zknrsugm/files/wandb-summary.json b/wandb/run-20240617_191659-zknrsugm/files/wandb-summary.json
deleted file mode 100644
index a8adcf5..0000000
--- a/wandb/run-20240617_191659-zknrsugm/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 19996672, "_timestamp": 1718695015.8218813, "_runtime": 17996.50044631958, "_step": 193725, "charts/episodic_return": -250.8427734375, "charts/episodic_length": 67.0, "charts/learning_rate": 6.145022268810862e-08, "losses/value_loss": 0.009639453142881393, "losses/policy_loss": -1.993216574192047e-05, "losses/entropy": 5.636853218078613, "losses/old_approx_kl": -4.991888999938965e-07, "losses/approx_kl": 1.30385160446167e-08, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9587568640708923, "charts/SPS": 1111.0, "_wandb": {"runtime": 17996}}
\ No newline at end of file
diff --git a/wandb/run-20240618_074750-o4b743hf/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240618_074750-o4b743hf/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index a761901..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,341 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240618_074750-o4b743hf/files/conda-environment.yaml b/wandb/run-20240618_074750-o4b743hf/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240618_074750-o4b743hf/files/config.yaml b/wandb/run-20240618_074750-o4b743hf/files/config.yaml
deleted file mode 100644
index 5770b52..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718722070.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240618_074750-o4b743hf/files/diff.patch b/wandb/run-20240618_074750-o4b743hf/files/diff.patch
deleted file mode 100644
index 1d57940..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/diff.patch
+++ /dev/null
@@ -1,9 +0,0 @@
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 3038406..5a64040 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240617_191659-zknrsugm
-\ No newline at end of file
-+run-20240618_074750-o4b743hf
-\ No newline at end of file
diff --git a/wandb/run-20240618_074750-o4b743hf/files/events.out.tfevents.1718722074.cs-mars-14.26217.0 b/wandb/run-20240618_074750-o4b743hf/files/events.out.tfevents.1718722074.cs-mars-14.26217.0
deleted file mode 120000
index eb303f8..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/events.out.tfevents.1718722074.cs-mars-14.26217.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718722074.cs-mars-14.26217.0
\ No newline at end of file
diff --git a/wandb/run-20240618_074750-o4b743hf/files/requirements.txt b/wandb/run-20240618_074750-o4b743hf/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240618_074750-o4b743hf/files/wandb-metadata.json b/wandb/run-20240618_074750-o4b743hf/files/wandb-metadata.json
deleted file mode 100644
index 7be9ae4..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-18T14:47:51.019894",
-    "startedAt": "2024-06-18T14:47:50.631941",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "0a34c167d06d5751a9342252c04667e5cb29c8e4"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3609.8876562499995,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3183.934,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3276.711,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2890.436,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2880.734,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2955.011,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3287.725,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3129.329,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3624.791,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3665.281,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3680.012,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4587.706,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4493.356,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3667.909,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3656.343,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3678.668,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3671.32,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2901.341,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3541.026,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2980.506,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3567.031,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3640.798,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3656.281,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3645.323,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4138.656,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3665.326,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3673.203,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4597.075,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4509.17,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3645.408,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3675.589,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3677.636,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3672.77,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.28369140625
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240618_074750-o4b743hf/files/wandb-summary.json b/wandb/run-20240618_074750-o4b743hf/files/wandb-summary.json
deleted file mode 100644
index 4628ca5..0000000
--- a/wandb/run-20240618_074750-o4b743hf/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1718730947.603716, "_runtime": 8876.967581987381, "_step": 98253, "charts/episodic_return": -307.787109375, "charts/episodic_length": 190.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.006133183836936951, "losses/policy_loss": -1.2341886758804321e-05, "losses/entropy": 4.167597770690918, "losses/old_approx_kl": 3.137253224849701e-05, "losses/approx_kl": 1.210719347000122e-07, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9621642827987671, "charts/SPS": 1126.0, "_wandb": {"runtime": 8876}}
\ No newline at end of file
diff --git a/wandb/run-20240618_074750-o4b743hf/run-o4b743hf.wandb b/wandb/run-20240618_074750-o4b743hf/run-o4b743hf.wandb
deleted file mode 100644
index 755fd30..0000000
Binary files a/wandb/run-20240618_074750-o4b743hf/run-o4b743hf.wandb and /dev/null differ
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240618_104044-ngq8rkkn/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index f59782e..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/conda-environment.yaml b/wandb/run-20240618_104044-ngq8rkkn/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/config.yaml b/wandb/run-20240618_104044-ngq8rkkn/files/config.yaml
deleted file mode 100644
index 2ee5c60..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718732444.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/diff.patch b/wandb/run-20240618_104044-ngq8rkkn/files/diff.patch
deleted file mode 100644
index 3fc2509..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/diff.patch
+++ /dev/null
@@ -1,25 +0,0 @@
-diff --git a/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718722074.cs-mars-14.26217.0 b/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718722074.cs-mars-14.26217.0
-deleted file mode 100644
-index 8410d3c..0000000
-Binary files a/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718722074.cs-mars-14.26217.0 and /dev/null differ
-diff --git a/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model b/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index a37dfbe..0000000
-Binary files a/training_results/game/ppo/2024/10000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1718677023.cs-mars-14.16320.0 b/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1718677023.cs-mars-14.16320.0
-deleted file mode 100644
-index e5cddfc..0000000
-Binary files a/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1718677023.cs-mars-14.16320.0 and /dev/null differ
-diff --git a/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model b/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index a6af30a..0000000
-Binary files a/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 5a64040..ed21d8a 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240618_074750-o4b743hf
-\ No newline at end of file
-+run-20240618_104044-ngq8rkkn
-\ No newline at end of file
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/events.out.tfevents.1718732448.cs-mars-14.8777.0 b/wandb/run-20240618_104044-ngq8rkkn/files/events.out.tfevents.1718732448.cs-mars-14.8777.0
deleted file mode 120000
index 2c76616..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/events.out.tfevents.1718732448.cs-mars-14.8777.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718732448.cs-mars-14.8777.0
\ No newline at end of file
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/requirements.txt b/wandb/run-20240618_104044-ngq8rkkn/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/wandb-metadata.json b/wandb/run-20240618_104044-ngq8rkkn/files/wandb-metadata.json
deleted file mode 100644
index 2162438..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-18T17:40:44.758834",
-    "startedAt": "2024-06-18T17:40:44.432507",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "7ea98f27e1e8f4df95fe8108ca9f9e6ef58f1063"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3296.9981562499997,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3278.052,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2924.013,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2419.022,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3144.21,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2667.368,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2803.351,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2395.447,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2669.844,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3679.739,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3678.636,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3680.447,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3679.405,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3678.591,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4604.299,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3681.095,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3681.147,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2491.618,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2780.758,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2848.098,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2755.166,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2876.388,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3157.376,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2896.985,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2652.297,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3678.526,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3688.317,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3678.647,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3685.88,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3685.252,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4598.078,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3679.517,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3686.372,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.30339050292969
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240618_104044-ngq8rkkn/files/wandb-summary.json b/wandb/run-20240618_104044-ngq8rkkn/files/wandb-summary.json
deleted file mode 100644
index d4e8c59..0000000
--- a/wandb/run-20240618_104044-ngq8rkkn/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1718741719.088338, "_runtime": 9274.652072906494, "_step": 93512, "charts/episodic_return": 132.94561767578125, "charts/episodic_length": 98.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.014835280366241932, "losses/policy_loss": -5.872175097465515e-05, "losses/entropy": -0.1930447816848755, "losses/old_approx_kl": 9.782891720533371e-05, "losses/approx_kl": 1.5133991837501526e-06, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9819554686546326, "charts/SPS": 1078.0, "_wandb": {"runtime": 9274}}
\ No newline at end of file
diff --git a/wandb/run-20240618_104044-ngq8rkkn/run-ngq8rkkn.wandb b/wandb/run-20240618_104044-ngq8rkkn/run-ngq8rkkn.wandb
deleted file mode 100644
index f7f75f0..0000000
Binary files a/wandb/run-20240618_104044-ngq8rkkn/run-ngq8rkkn.wandb and /dev/null differ
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240618_152847-ymlnrp92/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index 3cf1fc6..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 2e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/conda-environment.yaml b/wandb/run-20240618_152847-ymlnrp92/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/config.yaml b/wandb/run-20240618_152847-ymlnrp92/files/config.yaml
deleted file mode 100644
index c6362c7..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 20000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 4882
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718749727.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/diff.patch b/wandb/run-20240618_152847-ymlnrp92/files/diff.patch
deleted file mode 100644
index 618e72a..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/diff.patch
+++ /dev/null
@@ -1,22 +0,0 @@
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index f59782e..3cf1fc6 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 1e7
-+    total_timesteps: int = 2e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/wandb/latest-run b/wandb/latest-run
-index ed21d8a..e39816a 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240618_104044-ngq8rkkn
-\ No newline at end of file
-+run-20240618_152847-ymlnrp92
-\ No newline at end of file
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/events.out.tfevents.1718749731.cs-mars-14.27595.0 b/wandb/run-20240618_152847-ymlnrp92/files/events.out.tfevents.1718749731.cs-mars-14.27595.0
deleted file mode 120000
index 5d67102..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/events.out.tfevents.1718749731.cs-mars-14.27595.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1718749731.cs-mars-14.27595.0
\ No newline at end of file
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/requirements.txt b/wandb/run-20240618_152847-ymlnrp92/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/wandb-metadata.json b/wandb/run-20240618_152847-ymlnrp92/files/wandb-metadata.json
deleted file mode 100644
index 6a04f3b..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-18T22:28:47.442153",
-    "startedAt": "2024-06-18T22:28:47.085007",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "4c6a4a6025e82e9942990baed1b2fceb7a4b79a0"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 2541.1145312500003,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2238.102,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2233.596,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2226.208,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2223.382,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2225.099,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2201.008,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.737,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.613,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3570.482,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2879.985,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2875.006,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2850.769,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2873.243,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2874.27,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2861.65,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2878.242,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2226.402,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2226.239,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2235.922,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2239.443,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.381,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2401.613,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2236.034,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.496,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3252.525,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2578.908,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2845.727,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2406.495,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2879.079,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2665.581,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2861.508,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2317.92,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.41812133789062
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240618_152847-ymlnrp92/files/wandb-summary.json b/wandb/run-20240618_152847-ymlnrp92/files/wandb-summary.json
deleted file mode 100644
index 89db1f1..0000000
--- a/wandb/run-20240618_152847-ymlnrp92/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 19996672, "_timestamp": 1718767904.4727309, "_runtime": 18177.384078979492, "_step": 181525, "charts/episodic_return": 123.5042495727539, "charts/episodic_length": 98.0, "charts/learning_rate": 6.145022268810862e-08, "losses/value_loss": 0.01609693467617035, "losses/policy_loss": -0.00033149123191833496, "losses/entropy": -0.33260130882263184, "losses/old_approx_kl": 8.003273978829384e-05, "losses/approx_kl": 6.384216248989105e-07, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9633419513702393, "charts/SPS": 1100.0, "_wandb": {"runtime": 18177}}
\ No newline at end of file
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240619_092047-3oqscu7m/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index e2174fe..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 3e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/conda-environment.yaml b/wandb/run-20240619_092047-3oqscu7m/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/config.yaml b/wandb/run-20240619_092047-3oqscu7m/files/config.yaml
deleted file mode 100644
index 7479534..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 30000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 7324
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718814047.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/diff.patch b/wandb/run-20240619_092047-3oqscu7m/files/diff.patch
deleted file mode 100644
index 9cea91b..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/diff.patch
+++ /dev/null
@@ -1,34 +0,0 @@
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index c296a1b..0ee7aaf 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -450,5 +450,5 @@ class ReachAvoidTestGame(ReachAvoidGameEnv):
-         kwargs['random_init'] = True
-         kwargs['initial_attacker'] = np.array([[-0.4, -0.8]])
-         kwargs['initial_defender'] = np.array([[0.0, 0.0]])
--        kwargs['seed'] = 2025
-+        kwargs['seed'] = 2024
-         super().__init__(*args, **kwargs)
-\ No newline at end of file
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index 3cf1fc6..e2174fe 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 2e7
-+    total_timesteps: int = 3e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/wandb/latest-run b/wandb/latest-run
-index e39816a..fe864e7 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240618_152847-ymlnrp92
-\ No newline at end of file
-+run-20240619_092047-3oqscu7m
-\ No newline at end of file
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/events.out.tfevents.1718814051.cs-mars-14.29201.0 b/wandb/run-20240619_092047-3oqscu7m/files/events.out.tfevents.1718814051.cs-mars-14.29201.0
deleted file mode 120000
index 32e9dc1..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/events.out.tfevents.1718814051.cs-mars-14.29201.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/30000000.0/events.out.tfevents.1718814051.cs-mars-14.29201.0
\ No newline at end of file
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/requirements.txt b/wandb/run-20240619_092047-3oqscu7m/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/wandb-metadata.json b/wandb/run-20240619_092047-3oqscu7m/files/wandb-metadata.json
deleted file mode 100644
index b66d0d6..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-19T16:20:47.454990",
-    "startedAt": "2024-06-19T16:20:47.107806",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "d9ab40809c668498e94a773457bb9f56f5f412ba"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3241.2677812499996,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3759.157,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3727.653,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3717.877,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3736.239,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3759.605,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3735.128,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3760.084,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4695.86,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2808.647,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2284.24,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2916.271,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2938.101,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2693.762,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.58,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3563.971,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2305.004,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3731.998,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3720.851,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3753.058,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3728.041,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3719.175,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3754.262,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3756.297,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4697.832,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2233.381,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2316.647,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2898.552,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2829.907,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2237.999,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2384.824,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2910.386,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2443.18,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.2363510131836
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240619_092047-3oqscu7m/files/wandb-summary.json b/wandb/run-20240619_092047-3oqscu7m/files/wandb-summary.json
deleted file mode 100644
index 06020bf..0000000
--- a/wandb/run-20240619_092047-3oqscu7m/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 29999104, "_timestamp": 1718841191.132162, "_runtime": 27144.021581172943, "_step": 281771, "charts/episodic_return": 95.24555206298828, "charts/episodic_length": 136.0, "charts/learning_rate": 4.0961221969837425e-08, "losses/value_loss": 0.004127713851630688, "losses/policy_loss": 5.8960169553756714e-05, "losses/entropy": -0.9296939373016357, "losses/old_approx_kl": -4.097120836377144e-05, "losses/approx_kl": 1.9650906324386597e-07, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9703482389450073, "charts/SPS": 1105.0, "_wandb": {"runtime": 27143}}
\ No newline at end of file
diff --git a/wandb/run-20240620_125324-8pcg39te/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240620_125324-8pcg39te/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index f59782e..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240620_125324-8pcg39te/files/conda-environment.yaml b/wandb/run-20240620_125324-8pcg39te/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240620_125324-8pcg39te/files/config.yaml b/wandb/run-20240620_125324-8pcg39te/files/config.yaml
deleted file mode 100644
index 6257ba5..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718913204.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240620_125324-8pcg39te/files/diff.patch b/wandb/run-20240620_125324-8pcg39te/files/diff.patch
deleted file mode 100644
index ab21f5a..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/diff.patch
+++ /dev/null
@@ -1,22 +0,0 @@
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index e2174fe..f59782e 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 3e7
-+    total_timesteps: int = 1e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/wandb/latest-run b/wandb/latest-run
-index fe864e7..50b2c50 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240619_092047-3oqscu7m
-\ No newline at end of file
-+run-20240620_125324-8pcg39te
-\ No newline at end of file
diff --git a/wandb/run-20240620_125324-8pcg39te/files/events.out.tfevents.1718913209.cs-mars-14.19614.0 b/wandb/run-20240620_125324-8pcg39te/files/events.out.tfevents.1718913209.cs-mars-14.19614.0
deleted file mode 120000
index 9e28f1d..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/events.out.tfevents.1718913209.cs-mars-14.19614.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718913209.cs-mars-14.19614.0
\ No newline at end of file
diff --git a/wandb/run-20240620_125324-8pcg39te/files/requirements.txt b/wandb/run-20240620_125324-8pcg39te/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240620_125324-8pcg39te/files/wandb-metadata.json b/wandb/run-20240620_125324-8pcg39te/files/wandb-metadata.json
deleted file mode 100644
index 9284d6c..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-20T19:53:24.680049",
-    "startedAt": "2024-06-20T19:53:24.248616",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "3cf3ba823410f3681f2c3f078527bd9bf3cf2de1"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 4023.7641562500007,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 4184.448,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4581.793,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4580.311,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4581.266,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4591.006,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3937.564,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4201.065,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4585.468,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3419.574,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3639.213,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3555.502,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3519.368,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3927.264,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4304.766,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3476.372,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3925.172,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3797.747,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4564.585,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4553.769,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4550.011,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4579.952,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4203.934,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4204.764,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4582.0,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3621.171,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3634.543,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3608.232,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3086.651,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3737.202,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3749.692,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3448.766,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3827.282,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.27205657958984
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240620_125324-8pcg39te/files/wandb-summary.json b/wandb/run-20240620_125324-8pcg39te/files/wandb-summary.json
deleted file mode 100644
index 0be8270..0000000
--- a/wandb/run-20240620_125324-8pcg39te/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1718923149.0887203, "_runtime": 9944.831291437149, "_step": 89009, "charts/episodic_return": 433.9998779296875, "charts/episodic_length": 104.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.007296780124306679, "losses/policy_loss": 0.00011004693806171417, "losses/entropy": 0.24574172496795654, "losses/old_approx_kl": -2.8088688850402832e-05, "losses/approx_kl": 1.4961697161197662e-06, "losses/clipfrac": 0.0, "losses/explained_variance": 0.975531280040741, "charts/SPS": 1006.0, "_wandb": {"runtime": 9944}}
\ No newline at end of file
diff --git a/wandb/run-20240620_125324-8pcg39te/run-8pcg39te.wandb b/wandb/run-20240620_125324-8pcg39te/run-8pcg39te.wandb
deleted file mode 100644
index 8fd412d..0000000
Binary files a/wandb/run-20240620_125324-8pcg39te/run-8pcg39te.wandb and /dev/null differ
diff --git a/wandb/run-20240620_175922-j13jmons/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240620_175922-j13jmons/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index f59782e..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240620_175922-j13jmons/files/conda-environment.yaml b/wandb/run-20240620_175922-j13jmons/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240620_175922-j13jmons/files/config.yaml b/wandb/run-20240620_175922-j13jmons/files/config.yaml
deleted file mode 100644
index da80670..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1718931562.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240620_175922-j13jmons/files/diff.patch b/wandb/run-20240620_175922-j13jmons/files/diff.patch
deleted file mode 100644
index d7e0383..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/diff.patch
+++ /dev/null
@@ -1,9 +0,0 @@
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 50b2c50..5633679 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240620_125324-8pcg39te
-\ No newline at end of file
-+run-20240620_175922-j13jmons
-\ No newline at end of file
diff --git a/wandb/run-20240620_175922-j13jmons/files/events.out.tfevents.1718931567.cs-mars-14.6864.0 b/wandb/run-20240620_175922-j13jmons/files/events.out.tfevents.1718931567.cs-mars-14.6864.0
deleted file mode 120000
index e3a810c..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/events.out.tfevents.1718931567.cs-mars-14.6864.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1718931567.cs-mars-14.6864.0
\ No newline at end of file
diff --git a/wandb/run-20240620_175922-j13jmons/files/requirements.txt b/wandb/run-20240620_175922-j13jmons/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240620_175922-j13jmons/files/wandb-metadata.json b/wandb/run-20240620_175922-j13jmons/files/wandb-metadata.json
deleted file mode 100644
index 00f440f..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-21T00:59:23.290975",
-    "startedAt": "2024-06-21T00:59:22.919334",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "bcce63d99aea1d548beb2dad212eb00a76d3dd13"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3074.8393749999996,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3780.124,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3779.886,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3776.757,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3771.354,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4159.801,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4724.395,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3778.221,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3767.781,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2200.333,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2218.225,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2210.232,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2232.48,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2207.54,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.385,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2443.414,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2219.734,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3773.82,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3765.81,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3772.246,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3792.477,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3802.493,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4724.37,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3776.901,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3759.557,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2215.546,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2206.716,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2238.566,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2213.409,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2219.716,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2206.288,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.84,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2219.443,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.26104354858398
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240620_175922-j13jmons/files/wandb-summary.json b/wandb/run-20240620_175922-j13jmons/files/wandb-summary.json
deleted file mode 100644
index be6a541..0000000
--- a/wandb/run-20240620_175922-j13jmons/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1718940503.2344148, "_runtime": 8940.307587862015, "_step": 104576, "charts/episodic_return": 68.9580078125, "charts/episodic_length": 76.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.0042949821799993515, "losses/policy_loss": -6.891787052154541e-07, "losses/entropy": -1.7126750946044922, "losses/old_approx_kl": 6.050802767276764e-05, "losses/approx_kl": 1.3560056686401367e-06, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9892204999923706, "charts/SPS": 1118.0, "_wandb": {"runtime": 8940}}
\ No newline at end of file
diff --git a/wandb/run-20240620_175922-j13jmons/run-j13jmons.wandb b/wandb/run-20240620_175922-j13jmons/run-j13jmons.wandb
deleted file mode 100644
index 7f0b0da..0000000
Binary files a/wandb/run-20240620_175922-j13jmons/run-j13jmons.wandb and /dev/null differ
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240621_143501-7pn2rhxs/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index f59782e..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 1e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/conda-environment.yaml b/wandb/run-20240621_143501-7pn2rhxs/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/config.yaml b/wandb/run-20240621_143501-7pn2rhxs/files/config.yaml
deleted file mode 100644
index 405def8..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1719005701.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/diff.patch b/wandb/run-20240621_143501-7pn2rhxs/files/diff.patch
deleted file mode 100644
index bf8e76f..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/diff.patch
+++ /dev/null
@@ -1,9 +0,0 @@
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 5633679..06ee675 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240620_175922-j13jmons
-\ No newline at end of file
-+run-20240621_143501-7pn2rhxs
-\ No newline at end of file
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/events.out.tfevents.1719005705.cs-mars-14.22502.0 b/wandb/run-20240621_143501-7pn2rhxs/files/events.out.tfevents.1719005705.cs-mars-14.22502.0
deleted file mode 120000
index 50e7fd9..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/events.out.tfevents.1719005705.cs-mars-14.22502.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/10000000.0/events.out.tfevents.1719005705.cs-mars-14.22502.0
\ No newline at end of file
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/requirements.txt b/wandb/run-20240621_143501-7pn2rhxs/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/wandb-metadata.json b/wandb/run-20240621_143501-7pn2rhxs/files/wandb-metadata.json
deleted file mode 100644
index 25bf292..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-21T21:35:01.523932",
-    "startedAt": "2024-06-21T21:35:01.203638",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "59d1b72f1c758dd3af5b51b677427e3e2b981b9c"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3033.302531250001,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2219.947,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.087,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.852,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2236.05,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.497,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2234.127,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2236.649,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2736.204,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3708.077,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3694.431,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3697.593,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3690.029,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3691.048,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4618.26,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3684.609,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3688.826,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.116,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.91,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2229.978,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2235.896,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2223.981,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2221.934,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2230.636,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.732,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3703.941,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3690.224,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3697.566,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3694.749,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3684.19,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4621.699,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3697.061,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3696.782,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.29506301879883
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240621_143501-7pn2rhxs/files/wandb-summary.json b/wandb/run-20240621_143501-7pn2rhxs/files/wandb-summary.json
deleted file mode 100644
index 407413f..0000000
--- a/wandb/run-20240621_143501-7pn2rhxs/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9998336, "_timestamp": 1719014631.5770488, "_runtime": 8930.370538711548, "_step": 89757, "charts/episodic_return": -51.84403991699219, "charts/episodic_length": 42.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.0021354404743760824, "losses/policy_loss": -0.0003701820969581604, "losses/entropy": -3.0200858116149902, "losses/old_approx_kl": 4.4228509068489075e-05, "losses/approx_kl": 7.195398211479187e-06, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9959277510643005, "charts/SPS": 1120.0, "_wandb": {"runtime": 8930}}
\ No newline at end of file
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/code/safe_control_gym/experiments/train_game.py b/wandb/run-20240622_093244-xrlwdxiv/files/code/safe_control_gym/experiments/train_game.py
deleted file mode 100644
index 3cf1fc6..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/code/safe_control_gym/experiments/train_game.py
+++ /dev/null
@@ -1,342 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
-import os
-import random
-import time
-from datetime import datetime
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import tyro
-from torch.distributions.normal import Normal
-from torch.utils.tensorboard import SummaryWriter
-
-from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 2024
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "ReachAvoidGame"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-    save_model: bool = True
-    """whether to save model into the `runs/{run_name}` folder"""
-    upload_model: bool = False
-    """whether to upload the saved model to huggingface"""
-    hf_entity: str = ""
-    """the user or org name of the model repository from the Hugging Face Hub"""
-
-    # Algorithm specific arguments
-    env_id: str = "reach_avoid"
-    """the id of the environment"""
-    total_timesteps: int = 2e7
-    """total timesteps of the experiments"""
-    learning_rate: float = 3e-4
-    """the learning rate of the optimizer"""
-    num_envs: int = 2
-    """the number of parallel game environments"""
-    num_steps: int = 2048
-    """the number of steps to run in each environment per policy rollout"""
-    anneal_lr: bool = True
-    """Toggle learning rate annealing for policy and value networks"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    gae_lambda: float = 0.95
-    """the lambda for the general advantage estimation"""
-    num_minibatches: int = 32
-    """the number of mini-batches"""
-    update_epochs: int = 10
-    """the K epochs to update the policy"""
-    norm_adv: bool = True
-    """Toggles advantages normalization"""
-    clip_coef: float = 0.2
-    """the surrogate clipping coefficient"""
-    clip_vloss: bool = True
-    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
-    ent_coef: float = 0.0
-    """coefficient of the entropy"""
-    vf_coef: float = 0.5
-    """coefficient of the value function"""
-    max_grad_norm: float = 0.5
-    """the maximum norm for the gradient clipping"""
-    target_kl: float = None
-    """the target KL divergence threshold"""
-
-    # to be filled in runtime
-    batch_size: int = 0
-    """the batch size (computed in runtime)"""
-    minibatch_size: int = 0
-    """the mini-batch size (computed in runtime)"""
-    num_iterations: int = 0
-    """the number of iterations (computed in runtime)"""
-
-
-def make_env(env_id, idx, capture_video, run_name, gamma):
-    def thunk():
-        if capture_video and idx == 0:
-            # env = gym.make(env_id, render_mode="rgb_array")
-            env = ReachAvoidGameEnv()
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            # env = gym.make(env_id)
-            env = ReachAvoidGameEnv()
-        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env = gym.wrappers.ClipAction(env)
-        env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
-        env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-        return env
-
-    return thunk
-
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-
-class Agent(nn.Module):
-    def __init__(self, envs):
-        super().__init__()
-        self.critic = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 1), std=1.0),
-        )
-        self.actor_mean = nn.Sequential(
-            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, 64)),
-            nn.Tanh(),
-            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),
-            nn.Tanh(),  # Hanyang: add tanh layer to clip the action space
-        )
-        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))
-
-    def get_value(self, x):
-        return self.critic(x)
-
-    def get_action_and_value(self, x, action=None):
-        action_mean = self.actor_mean(x)
-        action_logstd = self.actor_logstd.expand_as(action_mean)
-        action_std = torch.exp(action_logstd)
-        probs = Normal(action_mean, action_std)
-        if action is None:
-            action = probs.sample()
-        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
-
-
-if __name__ == "__main__":
-    args = tyro.cli(Args)
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = int(args.total_timesteps // args.batch_size)
-    # Hanyang: make saving directory
-    run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-    if not os.path.exists(run_name):
-        os.makedirs(run_name+'/')
-        
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv(
-        [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]
-    )
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    agent = Agent(envs).to(device)
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    # ALGO Logic: Storage setup
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    # TRY NOT TO MODIFY: start the game
-    global_step = 0
-    start_time = time.time()
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    for iteration in range(1, args.num_iterations + 1):
-        # Annealing the rate if instructed to do so.
-        if args.anneal_lr:
-            frac = 1.0 - (iteration - 1.0) / args.num_iterations
-            lrnow = frac * args.learning_rate
-            optimizer.param_groups[0]["lr"] = lrnow
-
-        for step in range(0, args.num_steps):
-            global_step += args.num_envs
-            obs[step] = next_obs
-            dones[step] = next_done
-
-            # ALGO LOGIC: action logic
-            with torch.no_grad():
-                action, logprob, _, value = agent.get_action_and_value(next_obs)
-                values[step] = value.flatten()
-            actions[step] = action
-            logprobs[step] = logprob
-
-            # TRY NOT TO MODIFY: execute the game and log data.
-            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).view(-1)
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-
-            if "final_info" in infos:
-                for info in infos["final_info"]:
-                    if info and "episode" in info:
-                        print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-
-        # bootstrap value if not done
-        with torch.no_grad():
-            next_value = agent.get_value(next_obs).reshape(1, -1)
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
-
-        # flatten the batch
-        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-        b_logprobs = logprobs.reshape(-1)
-        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-        b_advantages = advantages.reshape(-1)
-        b_returns = returns.reshape(-1)
-        b_values = values.reshape(-1)
-
-        # Optimizing the policy and value network
-        b_inds = np.arange(args.batch_size)
-        clipfracs = []
-        for epoch in range(args.update_epochs):
-            np.random.shuffle(b_inds)
-            for start in range(0, args.batch_size, args.minibatch_size):
-                end = start + args.minibatch_size
-                mb_inds = b_inds[start:end]
-
-                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
-                logratio = newlogprob - b_logprobs[mb_inds]
-                ratio = logratio.exp()
-
-                with torch.no_grad():
-                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
-                    old_approx_kl = (-logratio).mean()
-                    approx_kl = ((ratio - 1) - logratio).mean()
-                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
-                mb_advantages = b_advantages[mb_inds]
-                if args.norm_adv:
-                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-                # Policy loss
-                pg_loss1 = -mb_advantages * ratio
-                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-                # Value loss
-                newvalue = newvalue.view(-1)
-                if args.clip_vloss:
-                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                    v_clipped = b_values[mb_inds] + torch.clamp(
-                        newvalue - b_values[mb_inds],
-                        -args.clip_coef,
-                        args.clip_coef,
-                    )
-                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                    v_loss = 0.5 * v_loss_max.mean()
-                else:
-                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-                entropy_loss = entropy.mean()
-                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-                optimizer.zero_grad()
-                loss.backward()
-                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-                optimizer.step()
-
-            if args.target_kl is not None and approx_kl > args.target_kl:
-                break
-
-        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
-        var_y = np.var(y_true)
-        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
-        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
-        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
-        writer.add_scalar("losses/explained_variance", explained_var, global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-    if args.save_model:
-        model_path = f"{run_name}/{args.exp_name}.cleanrl_model"
-        torch.save(agent.state_dict(), model_path)
-        print(f"model saved to {model_path}")
-
-    envs.close()
-    writer.close()
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/conda-environment.yaml b/wandb/run-20240622_093244-xrlwdxiv/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/config.yaml b/wandb/run-20240622_093244-xrlwdxiv/files/config.yaml
deleted file mode 100644
index 74f7b99..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/config.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 2024
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 20000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 2
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 4096
-minibatch_size:
-  desc: null
-  value: 128
-num_iterations:
-  desc: null
-  value: 4882
-_wandb:
-  desc: null
-  value:
-    code_path: code/safe_control_gym/experiments/train_game.py
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1719073964.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/diff.patch b/wandb/run-20240622_093244-xrlwdxiv/files/diff.patch
deleted file mode 100644
index 63b0f79..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/diff.patch
+++ /dev/null
@@ -1,123 +0,0 @@
-diff --git a/WorkingLogs.md b/WorkingLogs.md
-index e4beb62..6322b21 100644
---- a/WorkingLogs.md
-+++ b/WorkingLogs.md
-@@ -87,6 +87,15 @@ python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrot
- python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_boltz --algo ppo --task quadrotor_fixed --test_distb_level 1.0 --seed 2024
- python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_null --algo ppo --task quadrotor_fixed --test_distb_level 1.0  --seed 42
- 
--## Env Info
-+### test in the quadrotor_wind
-+python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_boltz --algo ppo --task quadrotor_wind --seed 2024
-+python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_null --algo ppo --task quadrotor_wind --seed 42
-+
-+### test in the quadrotor_random_hj
-+python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_boltz --algo ppo --task quadrotor_randomhj --seed 2024
-+python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_null --algo ppo --task quadrotor_randomhj --seed 42
-+python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_null --algo rarl --task quadrotor_randomhj  --seed 42
- 
-+
-+## Env Info
- python safe_control_gym/experiments/test_rl_controller.py --trained_task quadrotor_boltz --algo ppo --task quadrotor_random --seed 42  --render
-diff --git a/safe_control_gym/envs/__init__.py b/safe_control_gym/envs/__init__.py
-index 7a1345b..a1a6e06 100644
---- a/safe_control_gym/envs/__init__.py
-+++ b/safe_control_gym/envs/__init__.py
-@@ -63,6 +63,10 @@ register(idx='quadrotor_randomhj',
-          entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorRandomHJDistb',
-          config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
- 
-+register(idx='quadrotor_wind',
-+         entry_point='safe_control_gym.envs.gym_pybullet_drones.quadrotor_distb:QuadrotorWindDistb',
-+         config_entry_point='safe_control_gym.envs.gym_pybullet_drones:quadrotor_distb.yaml')
-+
- register(idx='reach_avoid',
-          entry_point='safe_control_gym.envs.gym_game.ReachAvoidGame:ReachAvoidGameEnv')
- 
-diff --git a/safe_control_gym/envs/gym_pybullet_drones/base_distb_aviary.py b/safe_control_gym/envs/gym_pybullet_drones/base_distb_aviary.py
-index 61367d8..cea448b 100644
---- a/safe_control_gym/envs/gym_pybullet_drones/base_distb_aviary.py
-+++ b/safe_control_gym/envs/gym_pybullet_drones/base_distb_aviary.py
-@@ -161,7 +161,7 @@ class BaseDistbAviary(BenchmarkEnv):
-         # Hanyang: initialize the disturbance parameters and the initial state randomization parameters here.
-         self.distb_type = distb_type
-         self.distb_level = distb_level
--        assert self.distb_type in ['fixed', 'boltzmann', 'random_hj', 'random', None], f"[ERROR] The disturbance type '{self.distb_type}' is not supported now. \n"
-+        assert self.distb_type in ['fixed', 'boltzmann', 'random_hj', 'random', 'wind', None], f"[ERROR] The disturbance type '{self.distb_type}' is not supported now. \n"
-         self.init_xy_lim = 0.25
-         self.init_z_lim = 0.1
-         self.init_rp_lim = np.pi/6
-@@ -364,12 +364,15 @@ class BaseDistbAviary(BenchmarkEnv):
-                     high = np.array([6e-3, 6e-3, 1.5e-4])
-                     # Generate a random sample
-                     hj_distbs = np.random.uniform(low, high)
-+                elif self.distb_type == 'wind':  # contant wind disturbances
-+                    hj_distbs = np.array([0.0, 0.0037099999999999998, 0.0])
-+                    # print(f"[INFO] The disturbance in the wind distb is {hj_distbs}. \n")
-                 else: # HJ based fixed, random_hj or boltzmann disturbances
-                     current_angles = quat2euler(self._get_drone_state_vector(i)[3:7])  # convert quaternion to eulers
-                     current_angle_rates = self._get_drone_state_vector(i)[13:16]
-                     current_state = np.concatenate((current_angles, current_angle_rates), axis=0)
-                     _, hj_distbs = distur_gener_quadrotor(current_state, self.distb_level)
--                    print(f"[INFO] The disturbance for drone {i} is {hj_distbs}. \n")
-+                    # print(f"[INFO] The disturbance for drone {i} is {hj_distbs}. \n")
-                 
-                 if self.PHYSICS == Physics.PYB:
-                     # self._physics(clipped_action[i, :], i)
-diff --git a/safe_control_gym/envs/gym_pybullet_drones/quadrotor_distb.py b/safe_control_gym/envs/gym_pybullet_drones/quadrotor_distb.py
-index 0940a45..4ae05e6 100644
---- a/safe_control_gym/envs/gym_pybullet_drones/quadrotor_distb.py
-+++ b/safe_control_gym/envs/gym_pybullet_drones/quadrotor_distb.py
-@@ -757,3 +757,18 @@ class QuadrotorRandomDistb(QuadrotorDistb):
-         kwargs['seed'] = 2024
-         kwargs['adversary_disturbance'] = 'dynamics'  # TODO: for rarl test, but not sure whether it has influences on the performances or not
-         super().__init__(*args, **kwargs)  # distb_level=distb_level, randomization_reset=randomization_reset,
-+
-+
-+class QuadrotorWindDistb(QuadrotorDistb):
-+    NAME = 'quadrotor_wind'
-+    #TODO: Hanyang: add contant wind torque disturbance
-+    # Hanyang: add contant wind torque disturbance
-+    def __init__(self, *args,  **kwargs):  # distb_level=1.0, randomization_reset=False,
-+        # Set disturbance_type to 'fixed' regardless of the input
-+        kwargs['distb_type'] = 'wind'
-+        kwargs['distb_level'] = 0.0
-+        kwargs['randomized_init'] = True
-+        kwargs['record'] = False
-+        kwargs['seed'] = 2024
-+        kwargs['adversary_disturbance'] = 'dynamics'  # TODO: for rarl test, but not sure whether it has influences on the performances or not
-+        super().__init__(*args, **kwargs)  # distb_level=distb_level, randomization_reset=randomization_reset,
-diff --git a/safe_control_gym/experiments/test_game1.py b/safe_control_gym/experiments/test_game1.py
-index 96263d5..c6072af 100644
---- a/safe_control_gym/experiments/test_game1.py
-+++ b/safe_control_gym/experiments/test_game1.py
-@@ -182,6 +182,7 @@ if __name__ == "__main__":
-     args.exp_name = "train_game.cleanrl_model"
-     run_name = os.path.join('training_results/' + 'game/ppo/' +f'{args.seed}/' + f'{args.total_timesteps}' )
-     model_path = f"{run_name}/{args.exp_name}"
-+    print(f"The loaded model is {model_path}. \n")
-     assert os.path.exists(model_path), f"Model path {model_path} does not exist."
-     
-     episodic_returns, envs = evaluate(
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index f59782e..3cf1fc6 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 1e7
-+    total_timesteps: int = 2e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 5633679..6eb3ab1 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240620_175922-j13jmons
-\ No newline at end of file
-+run-20240622_093244-xrlwdxiv
-\ No newline at end of file
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/events.out.tfevents.1719073968.cs-mars-14.2743.0 b/wandb/run-20240622_093244-xrlwdxiv/files/events.out.tfevents.1719073968.cs-mars-14.2743.0
deleted file mode 120000
index ab301f5..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/events.out.tfevents.1719073968.cs-mars-14.2743.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1719073968.cs-mars-14.2743.0
\ No newline at end of file
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/requirements.txt b/wandb/run-20240622_093244-xrlwdxiv/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/wandb-metadata.json b/wandb/run-20240622_093244-xrlwdxiv/files/wandb-metadata.json
deleted file mode 100644
index cc8a961..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/wandb-metadata.json
+++ /dev/null
@@ -1,208 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-22T16:32:44.764627",
-    "startedAt": "2024-06-22T16:32:44.325186",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "safe_control_gym/experiments/train_game.py",
-    "codePathLocal": "safe_control_gym/experiments/train_game.py",
-    "codePath": "safe_control_gym/experiments/train_game.py",
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "59d1b72f1c758dd3af5b51b677427e3e2b981b9c"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3793.6212499999997,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3746.925,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3742.182,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3817.891,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3944.798,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3835.918,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3799.307,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4705.991,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3699.448,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4533.039,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4583.239,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4486.266,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3232.286,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3476.332,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3701.151,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3694.286,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3200.841,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3718.331,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3784.968,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3775.025,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3770.287,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3831.321,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3759.702,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4744.174,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3716.659,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3825.111,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3304.839,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3755.31,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2381.248,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3707.645,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3711.106,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3708.091,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3702.163,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 85.27167510986328
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240622_093244-xrlwdxiv/files/wandb-summary.json b/wandb/run-20240622_093244-xrlwdxiv/files/wandb-summary.json
deleted file mode 100644
index 4ae9c9a..0000000
--- a/wandb/run-20240622_093244-xrlwdxiv/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 19996672, "_timestamp": 1719091859.9498143, "_runtime": 17895.620077371597, "_step": 230226, "charts/episodic_return": -30.493053436279297, "charts/episodic_length": 74.0, "charts/learning_rate": 6.145022268810862e-08, "losses/value_loss": 0.003181756939738989, "losses/policy_loss": 6.309151649475098e-05, "losses/entropy": -3.6453371047973633, "losses/old_approx_kl": -0.0002756686881184578, "losses/approx_kl": 6.156042218208313e-07, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9956687688827515, "charts/SPS": 1117.0, "_wandb": {"runtime": 17895}}
\ No newline at end of file
diff --git a/wandb/run-20240624_111121-7fza559v/files/conda-environment.yaml b/wandb/run-20240624_111121-7fza559v/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240624_111121-7fza559v/files/config.yaml b/wandb/run-20240624_111121-7fza559v/files/config.yaml
deleted file mode 100644
index 6b5af02..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/config.yaml
+++ /dev/null
@@ -1,121 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 42
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 4
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 8192
-minibatch_size:
-  desc: null
-  value: 256
-num_iterations:
-  desc: null
-  value: 1220
-_wandb:
-  desc: null
-  value:
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1719252681.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240624_111121-7fza559v/files/diff.patch b/wandb/run-20240624_111121-7fza559v/files/diff.patch
deleted file mode 100644
index 1fe4c1a..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/diff.patch
+++ /dev/null
@@ -1,281 +0,0 @@
-diff --git a/ceshi_game.py b/ceshi_game.py
-index 7aa8f7e..9f804c3 100644
---- a/ceshi_game.py
-+++ b/ceshi_game.py
-@@ -32,7 +32,7 @@ args = tyro.cli(Args)
- #     [make_env(args.env_id, i, args.capture_video, "ceshi_game", args.gamma) for i in range(1)])
- 
- # env = gym.vector.SyncVectorEnv([make_env(args.env_id, 0, args.capture_video, "ceshi_game", args.gamma)])
--env = ReachAvoidTestGame()
-+# env = ReachAvoidTestGame()
- 
- # obs, info = env.reset()
- # print(f"The obs is {obs}. \n")
-@@ -42,8 +42,8 @@ env = ReachAvoidTestGame()
- # env = ReachAvoidGameEnv(initial_attacker=initial_attacker, initial_defender=initial_defender, random_init=False)
- # print(f"The initial state is {env.}. \n")
- # obs, info = env.reset()
--print(f"The state space of the env is {env.observation_space}. \n")
--print(f"The action space of the env is {env.action_space}. \n")
-+# print(f"The state space of the env is {env.observation_space}. \n")
-+# print(f"The action space of the env is {env.action_space}. \n")
- # print(f"The {envs.state}")
- 
- # print(f"The obs is {obs} and the shape of the obs is {obs.shape}. \n")
-@@ -99,4 +99,75 @@ print(f"The action space of the env is {env.action_space}. \n")
- 
- # print(f"The defender is in the obstacle area: {_check_area(current_defenders[0], obstacles)}. \n")
- # reward = 0.0
--# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-\ No newline at end of file
-+# reward += -1.0 if _check_area(current_defenders[0], obstacles) else 0.0
-+
-+
-+
-+ # Map boundaries
-+min_val, max_val = -0.9, 0.9
-+
-+# Obstacles and target areas
-+obstacles = [
-+([-0.1, 0.1], [-1.0, -0.3]),  # First obstacle
-+([-0.1, 0.1], [0.3, 0.6])     # Second obstacle
-+]
-+target = ([0.6, 0.8], [0.1, 0.3])
-+
-+def is_valid_position(pos):
-+    x, y = pos
-+    # Check boundaries
-+    if not (min_val <= x <= max_val and min_val <= y <= max_val):
-+        return False
-+    # Check obstacles
-+    for (ox, oy) in obstacles:
-+        if ox[0] <= x <= ox[1] and oy[0] <= y <= oy[1]:
-+            return False
-+    # Check target
-+    if target[0][0] <= x <= target[0][1] and target[1][0] <= y <= target[1][1]:
-+        return False
-+    return True
-+
-+def generate_position(current_seed):
-+    np.random.seed(current_seed)
-+    while True:
-+        pos = np.round(np.random.uniform(min_val, max_val, 2), 1)
-+        if is_valid_position(pos):
-+            return pos
-+
-+def generate_neighborpoint(position, distance, radius, seed):
-+    """
-+    Generate a random point within a circle whose center is a specified distance away from a given position.
-+
-+    Parameters:
-+    position (tuple): The (x, y) coordinates of the initial position.
-+    distance (float): The distance from the initial position to the center of the circle.
-+    radius (float): The radius of the circle.
-+    seed (int): The random seed.
-+
-+    Returns:
-+    tuple: A random (x, y) point within the specified circle.
-+    """
-+    np.random.seed(seed)
-+    while True:
-+        # Randomly choose an angle to place the circle's center
-+        angle = np.random.uniform(0, 2 * np.pi)
-+        
-+        # Determine the center of the circle
-+        center_x = position[0] + distance * np.cos(angle)
-+        center_y = position[1] + distance * np.sin(angle)
-+        
-+        # Generate a random point within the circle
-+        point_angle = np.random.uniform(0, 2 * np.pi)
-+        point_radius = np.sqrt(np.random.uniform(0, 1)) * radius
-+        point_x = center_x + point_radius * np.cos(point_angle)
-+        point_y = center_y + point_radius * np.sin(point_angle)
-+
-+        if is_valid_position((point_x, point_y)):
-+            return (point_x, point_y)
-+
-+# attacker = np.array([[0.0, 0.0]])
-+attacker = np.round(np.random.uniform(min_val, max_val, 2), 1)
-+print(f"The attacker is {attacker}. \n")
-+defender = generate_neighborpoint(attacker, 0.5, 0.1, 0)
-+print(f"The defender is {defender}. \n")
-+print(f"The distance between the attacker and the defender is {np.linalg.norm(attacker - defender)}. \n")
-\ No newline at end of file
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 64f6d62..68db391 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -71,6 +71,7 @@ class BaseGameEnv(gym.Env):
-         self.init_attackers = initial_attacker
-         self.init_defenders = initial_defender
-         #### Housekeeping ##########################################
-+        self.call_counter = 0
-         self.random_init = random_init
-         self._housekeeping()
-         #### Update and all players' information #####
-@@ -129,7 +130,7 @@ class BaseGameEnv(gym.Env):
-         np.random.seed(self.initial_players_seed)
-     
-         # Map boundaries
--        min_val, max_val = -0.99, 0.99
-+        min_val, max_val = -0.9, 0.9
-         
-         # Obstacles and target areas
-         obstacles = [
-@@ -159,21 +160,59 @@ class BaseGameEnv(gym.Env):
-                 if is_valid_position(pos):
-                     return pos
-         
--        def distance(pos1, pos2):
--            return np.sqrt((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2)
-+        def generate_neighborpoint(point, distance, radius, seed):
-+            """
-+            Generate a random point within a circle whose center is a specified distance away from a given position.
-+
-+            Parameters:
-+            position (tuple): The (x, y) coordinates of the initial position.
-+            distance (float): The distance from the initial position to the center of the circle.
-+            radius (float): The radius of the circle.
-+            seed (int): The random seed.
-+
-+            Returns:
-+            tuple: A random (x, y) point whose relative distance between the input position is .
-+            """
-+            np.random.seed(seed)
-+            while True:
-+                # Randomly choose an angle to place the circle's center
-+                angle = np.random.uniform(0, 2 * np.pi)
-+                
-+                # Determine the center of the circle
-+                center_x = point[0] + distance * np.cos(angle)
-+                center_y = point[1] + distance * np.sin(angle)
-+                
-+                # Generate a random point within the circle
-+                point_angle = np.random.uniform(0, 2 * np.pi)
-+                point_radius = np.sqrt(np.random.uniform(0, 1)) * radius
-+                new_point_x = center_x + point_radius * np.cos(point_angle)
-+                new_point_y = center_y + point_radius * np.sin(point_angle)
-+
-+                if is_valid_position((new_point_x, new_point_y)):
-+                    return (new_point_x, new_point_y)
-+        
-+        # Calculate desired distance based on the counter
-+        if self.call_counter < 128:  # 128 episodes
-+            distance = 0.15
-+            r = 0.05
-+        elif self.call_counter < 256:
-+            distance = 0.35
-+            r = 0.15
-+        elif self.call_counter < 512:
-+            distance = 0.75
-+            r = 0.25
-+        else:
-+            distance = 1.45
-+            r = 1.35
-         
-         attacker_seed = self.initial_players_seed
-         defender_seed = self.initial_players_seed + 1
-         
--        while True:
--            attacker_pos = generate_position(attacker_seed)
--            defender_pos = generate_position(defender_seed)
--            
--            if distance(attacker_pos, defender_pos) > 1.0:
--                break
--            defender_seed += 1  # Change the seed for the defender until a valid position is found
-+        attacker_pos = generate_position(attacker_seed)
-+        defender_pos = generate_neighborpoint(attacker_pos, distance, r, defender_seed)
-         
-         self.initial_players_seed += 1
-+        self.call_counter += 1  # Increment the call counter
-         
-         return np.array([attacker_pos]), np.array([defender_pos])
- 
-diff --git a/safe_control_gym/envs/gym_game/ReachAvoidGame.py b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-index 0ec1234..8123b66 100644
---- a/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-+++ b/safe_control_gym/envs/gym_game/ReachAvoidGame.py
-@@ -183,7 +183,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-             current_defender_state = self.defenders._get_state()
- 
-             for num in range(self.NUM_ATTACKERS):
--                if last_status[num]:  # attacker has arrived or been captured
-+                if last_status[num]:  # attacker has arrived(+1) or been captured(-1)
-                     new_status[num] = last_status[num]
-                 else: # attacker is free last time
-                     # check if the attacker arrive at the des this time
-@@ -196,7 +196,7 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-                     else:
-                         # check if the attacker is captured
-                         for j in range(self.NUM_DEFENDERS):
--                            if np.linalg.norm(current_attacker_state[num] - current_defender_state[j]) <= 0.1:
-+                            if np.linalg.norm(current_attacker_state[num] - current_defender_state[j]) <= 0.1:  # Hanyang: 0.1 is the threshold
-                                 new_status[num] = -1
-                                 break
- 
-@@ -261,14 +261,14 @@ class ReachAvoidGameEnv(BaseRLGameEnv):
-         # reward 2:
-         status_change = current_attacker_status[0] - last_attacker_status[0]
-         if status_change == 1:  # attacker arrived
--            reward += -5
-+            reward += -50
-         elif status_change == -1:  # attacker is captured
--            reward += 10
-+            reward += 50
-         else:  # attacker is free
-             reward += 0.0
-         # check the defender status
-         current_defender_state = self.defenders._get_state().copy()
--        reward += -10 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-+        reward += -50 if self._check_area(current_defender_state[0], self.obstacles) else 0.0
-         # check the relative distance difference or relative distance
-         current_attacker_state = self.attackers._get_state().copy()
-         current_relative_distance = np.linalg.norm(current_attacker_state[0] - current_defender_state[0])
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index 3cf1fc6..23c923e 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -21,7 +21,7 @@ from safe_control_gym.envs.gym_game.ReachAvoidGame import ReachAvoidGameEnv
- class Args:
-     exp_name: str = os.path.basename(__file__)[: -len(".py")]
-     """the name of this experiment"""
--    seed: int = 2024
-+    seed: int = 42
-     """seed of the experiment"""
-     torch_deterministic: bool = True
-     """if toggled, `torch.backends.cudnn.deterministic=False`"""
-@@ -45,11 +45,11 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 2e7
-+    total_timesteps: int = 1e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
--    num_envs: int = 2
-+    num_envs: int = 4
-     """the number of parallel game environments"""
-     num_steps: int = 2048
-     """the number of steps to run in each environment per policy rollout"""
-diff --git "a/training_results/game/ppo/2024/20000000.0/Screenshot 2024-06-23 at 11.25.14\342\200\257AM.png" "b/training_results/game/ppo/2024/20000000.0/Screenshot 2024-06-23 at 11.25.14\342\200\257AM.png"
-deleted file mode 100644
-index 7aecba0..0000000
-Binary files "a/training_results/game/ppo/2024/20000000.0/Screenshot 2024-06-23 at 11.25.14\342\200\257AM.png" and /dev/null differ
-diff --git a/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1719073968.cs-mars-14.2743.0 b/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1719073968.cs-mars-14.2743.0
-deleted file mode 100644
-index d422b35..0000000
-Binary files a/training_results/game/ppo/2024/20000000.0/events.out.tfevents.1719073968.cs-mars-14.2743.0 and /dev/null differ
-diff --git a/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model b/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index 968c322..0000000
-Binary files a/training_results/game/ppo/2024/20000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index 6eb3ab1..b5cf86c 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240622_093244-xrlwdxiv
-\ No newline at end of file
-+run-20240624_111121-7fza559v
-\ No newline at end of file
diff --git a/wandb/run-20240624_111121-7fza559v/files/events.out.tfevents.1719252685.cs-mars-14.23354.0 b/wandb/run-20240624_111121-7fza559v/files/events.out.tfevents.1719252685.cs-mars-14.23354.0
deleted file mode 120000
index 76791d1..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/events.out.tfevents.1719252685.cs-mars-14.23354.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0
\ No newline at end of file
diff --git a/wandb/run-20240624_111121-7fza559v/files/requirements.txt b/wandb/run-20240624_111121-7fza559v/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240624_111121-7fza559v/files/wandb-metadata.json b/wandb/run-20240624_111121-7fza559v/files/wandb-metadata.json
deleted file mode 100644
index 3f69b6e..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/wandb-metadata.json
+++ /dev/null
@@ -1,207 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-24T18:11:21.514264",
-    "startedAt": "2024-06-24T18:11:21.201040",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/localhome/hha160/projects/safe-control-gym/safe_control_gym/experiments/train_game.py",
-    "codePathLocal": null,
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "ad556ebbb3ed82c14f6dbc13949f53ed4a3db37e"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 2595.30140625,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2346.638,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2270.82,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2987.925,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2897.576,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2863.187,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2881.463,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4568.687,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3277.68,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2197.789,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.666,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.896,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2202.218,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.835,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.394,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.115,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.531,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2852.336,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2907.606,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2870.009,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2886.659,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2863.95,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2883.005,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3614.135,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2895.442,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2197.397,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.908,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.745,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.816,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.884,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2197.837,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.978,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.518,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 83.82525253295898
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240624_111121-7fza559v/files/wandb-summary.json b/wandb/run-20240624_111121-7fza559v/files/wandb-summary.json
deleted file mode 100644
index 6798994..0000000
--- a/wandb/run-20240624_111121-7fza559v/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 9994240, "_timestamp": 1719258620.952173, "_runtime": 5939.748265028, "_step": 134542, "charts/episodic_return": 44.78354263305664, "charts/episodic_length": 23.0, "charts/learning_rate": 2.459016457123653e-07, "losses/value_loss": 0.004303186200559139, "losses/policy_loss": -0.00015458650887012482, "losses/entropy": -1.4508497714996338, "losses/old_approx_kl": -1.8899329006671906e-05, "losses/approx_kl": 2.910848706960678e-06, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9842564463615417, "charts/SPS": 1683.0, "_wandb": {"runtime": 5939}}
\ No newline at end of file
diff --git a/wandb/run-20240624_111121-7fza559v/run-7fza559v.wandb b/wandb/run-20240624_111121-7fza559v/run-7fza559v.wandb
deleted file mode 100644
index 5250db1..0000000
Binary files a/wandb/run-20240624_111121-7fza559v/run-7fza559v.wandb and /dev/null differ
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/conda-environment.yaml b/wandb/run-20240624_130556-v2gygcr9/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/config.yaml b/wandb/run-20240624_130556-v2gygcr9/files/config.yaml
deleted file mode 100644
index d685791..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/config.yaml
+++ /dev/null
@@ -1,121 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 42
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 10000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 4
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 8192
-minibatch_size:
-  desc: null
-  value: 256
-num_iterations:
-  desc: null
-  value: 1220
-_wandb:
-  desc: null
-  value:
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1719259556.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/diff.patch b/wandb/run-20240624_130556-v2gygcr9/files/diff.patch
deleted file mode 100644
index eddf927..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/diff.patch
+++ /dev/null
@@ -1,38 +0,0 @@
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 68db391..2b992cf 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -128,7 +128,7 @@ class BaseGameEnv(gym.Env):
-             defenders (np.ndarray): the initial positions of the defenders
-         '''
-         np.random.seed(self.initial_players_seed)
--    
-+        print(f"========== self.call_counter: {self.call_counter} in BaseGame.py. ==========\n")
-         # Map boundaries
-         min_val, max_val = -0.9, 0.9
-         
-diff --git a/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0 b/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0
-deleted file mode 100644
-index 0f43e0a..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0 and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/reward5.png b/training_results/game/ppo/42/10000000.0/reward5.png
-deleted file mode 100644
-index be3f8ec..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/reward5.png and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/reward5_init.png b/training_results/game/ppo/42/10000000.0/reward5_init.png
-deleted file mode 100644
-index 4dd0892..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/reward5_init.png and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model b/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index afe5179..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index b5cf86c..05f0a22 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240624_111121-7fza559v
-\ No newline at end of file
-+run-20240624_130556-v2gygcr9
-\ No newline at end of file
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/events.out.tfevents.1719259560.cs-mars-14.14461.0 b/wandb/run-20240624_130556-v2gygcr9/files/events.out.tfevents.1719259560.cs-mars-14.14461.0
deleted file mode 120000
index af91e13..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/events.out.tfevents.1719259560.cs-mars-14.14461.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719259560.cs-mars-14.14461.0
\ No newline at end of file
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/requirements.txt b/wandb/run-20240624_130556-v2gygcr9/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/wandb-metadata.json b/wandb/run-20240624_130556-v2gygcr9/files/wandb-metadata.json
deleted file mode 100644
index 9c96ee4..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/wandb-metadata.json
+++ /dev/null
@@ -1,207 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-24T20:05:56.614700",
-    "startedAt": "2024-06-24T20:05:56.293897",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/localhome/hha160/projects/safe-control-gym/safe_control_gym/experiments/train_game.py",
-    "codePathLocal": null,
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "80efbfd459cf287f905f642c8b57d99949a3c732"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3058.6544062499997,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3763.479,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3759.093,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3774.244,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4706.936,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4289.976,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3756.615,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3763.178,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3759.072,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.244,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.231,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.059,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.953,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.582,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.19,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.867,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2197.595,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3758.995,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3757.763,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3763.996,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4708.466,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3825.752,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3758.029,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3779.746,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3764.684,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.267,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.956,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.545,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.609,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2198.312,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.96,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.916,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 2199.631,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 83.83418273925781
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240624_130556-v2gygcr9/files/wandb-summary.json b/wandb/run-20240624_130556-v2gygcr9/files/wandb-summary.json
deleted file mode 100644
index 0f363ca..0000000
--- a/wandb/run-20240624_130556-v2gygcr9/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 135192, "_timestamp": 1719259637.4929032, "_runtime": 81.19318723678589, "_step": 777, "charts/episodic_return": -736.0281982421875, "charts/episodic_length": 161.0, "charts/learning_rate": 0.00029631148208864033, "losses/value_loss": 0.06622284650802612, "losses/policy_loss": 0.008421849459409714, "losses/entropy": 2.6486940383911133, "losses/old_approx_kl": 0.010105331428349018, "losses/approx_kl": 0.005730172619223595, "losses/clipfrac": 0.04906005784869194, "losses/explained_variance": 0.7073330283164978, "charts/SPS": 1766.0}
\ No newline at end of file
diff --git a/wandb/run-20240624_130921-tkjintz8/files/conda-environment.yaml b/wandb/run-20240624_130921-tkjintz8/files/conda-environment.yaml
deleted file mode 100644
index eb32aec..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/conda-environment.yaml
+++ /dev/null
@@ -1,356 +0,0 @@
-name: base
-channels:
-  - plotly
-  - conda-forge
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=conda_forge
-  - _openmp_mutex=4.5=2_kmp_llvm
-  - absl-py=1.4.0=pyhd8ed1ab_0
-  - aiohttp=3.8.4=py38h01eb140_1
-  - aiosignal=1.3.1=pyhd8ed1ab_0
-  - alsa-lib=1.2.8=h166bdaf_0
-  - async-timeout=4.0.2=pyhd8ed1ab_0
-  - attr=2.5.1=h166bdaf_1
-  - attrs=23.1.0=pyh71513ae_1
-  - backports=1.0=pyhd8ed1ab_3
-  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
-  - blinker=1.6.2=pyhd8ed1ab_0
-  - brotli=1.0.9=h166bdaf_8
-  - brotli-bin=1.0.9=h166bdaf_8
-  - brotlipy=0.7.0=py38h0a891b7_1005
-  - bullet-cpp=3.21=h8f669ce_4
-  - bzip2=1.0.8=h7f98852_4
-  - c-ares=1.18.1=h7f98852_0
-  - ca-certificates=2024.2.2=hbcca054_0
-  - cachetools=5.3.1=pyhd8ed1ab_0
-  - cairo=1.16.0=ha61ee94_1014
-  - certifi=2024.2.2=pyhd8ed1ab_0
-  - cffi=1.15.1=py38h4a40e3a_3
-  - charset-normalizer=2.1.1=pyhd8ed1ab_0
-  - click=8.1.5=unix_pyh707e725_0
-  - colorama=0.4.6=pyhd8ed1ab_0
-  - conda=22.11.1=py38h578d9bd_1
-  - conda-package-handling=2.0.2=pyh38be061_0
-  - conda-package-streaming=0.7.0=pyhd8ed1ab_1
-  - contourpy=1.0.7=py38hfbd4bf9_0
-  - cryptography=39.0.0=py38h3d167d9_0
-  - cycler=0.11.0=pyhd8ed1ab_0
-  - dbus=1.13.6=h5008d03_3
-  - decorator=5.1.1=pyhd3eb1b0_0
-  - expat=2.5.0=h27087fc_0
-  - fftw=3.3.10=nompi_hf0379b8_106
-  - fmt=9.1.0=h924138e_0
-  - font-ttf-dejavu-sans-mono=2.37=hab24e00_0
-  - font-ttf-inconsolata=3.000=h77eed37_0
-  - font-ttf-source-code-pro=2.038=h77eed37_0
-  - font-ttf-ubuntu=0.83=hab24e00_0
-  - fontconfig=2.14.2=h14ed4e7_0
-  - fonts-conda-ecosystem=1=0
-  - fonts-conda-forge=1=0
-  - fonttools=4.38.0=py38h0a891b7_1
-  - freetype=2.12.1=hca18f0e_1
-  - frozenlist=1.4.0=py38h01eb140_0
-  - future=0.18.2=py38_1
-  - gettext=0.21.1=h27087fc_0
-  - glib=2.74.1=h6239696_1
-  - glib-tools=2.74.1=h6239696_1
-  - google-auth=2.22.0=pyh1a96a4e_0
-  - google-auth-oauthlib=1.0.0=pyhd8ed1ab_1
-  - graphite2=1.3.13=h58526e2_1001
-  - grpcio=1.54.2=py38h8dc9893_2
-  - gst-plugins-base=1.21.3=h4243ec0_1
-  - gstreamer=1.21.3=h25f0c4b_1
-  - gstreamer-orc=0.4.33=h166bdaf_0
-  - harfbuzz=6.0.0=h8e241bc_0
-  - icu=70.1=h27087fc_0
-  - idna=3.4=pyhd8ed1ab_0
-  - importlib-metadata=6.8.0=pyha770c72_0
-  - jack=1.9.22=h11f4161_0
-  - joblib=1.3.0=pyhd8ed1ab_1
-  - jpeg=9e=h166bdaf_2
-  - keyutils=1.6.1=h166bdaf_0
-  - kiwisolver=1.4.4=py38h43d8883_1
-  - krb5=1.20.1=h81ceb04_0
-  - lame=3.100=h166bdaf_1003
-  - lcms2=2.14=hfd0df8a_1
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=4.0.0=h27087fc_0
-  - libabseil=20230125.3=cxx17_h59595ed_0
-  - libarchive=3.6.2=h3d51595_0
-  - libblas=3.9.0=16_linux64_openblas
-  - libbrotlicommon=1.0.9=h166bdaf_8
-  - libbrotlidec=1.0.9=h166bdaf_8
-  - libbrotlienc=1.0.9=h166bdaf_8
-  - libcap=2.66=ha37c62d_0
-  - libcblas=3.9.0=16_linux64_openblas
-  - libclang=15.0.7=default_had23c3d_0
-  - libclang13=15.0.7=default_h3e3d535_0
-  - libcups=2.3.3=h36d4200_3
-  - libcurl=7.87.0=hdc1c0ab_0
-  - libdb=6.2.32=h9c3ff4c_0
-  - libdeflate=1.17=h0b41bf4_0
-  - libedit=3.1.20191231=he28a2e2_2
-  - libev=4.33=h516909a_1
-  - libevent=2.1.10=h28343ad_4
-  - libffi=3.4.2=h6a678d5_6
-  - libflac=1.4.2=h27087fc_0
-  - libgcc-ng=12.2.0=h65d4601_19
-  - libgcrypt=1.10.1=h166bdaf_0
-  - libgfortran-ng=12.2.0=h69a702a_19
-  - libgfortran5=12.2.0=h337968e_19
-  - libglib=2.74.1=h606061b_1
-  - libgpg-error=1.46=h620e276_0
-  - libgrpc=1.54.2=hb20ce57_2
-  - libiconv=1.17=h166bdaf_0
-  - libjpeg-turbo=2.1.4=h166bdaf_0
-  - liblapack=3.9.0=16_linux64_openblas
-  - libllvm15=15.0.7=hadd5161_0
-  - libmamba=1.2.0=hcea66bb_0
-  - libmambapy=1.2.0=py38h7fa060d_0
-  - libnghttp2=1.51.0=hff17c54_0
-  - libnsl=2.0.0=h7f98852_0
-  - libogg=1.3.4=h7f98852_1
-  - libopenblas=0.3.21=pthreads_h78a6416_3
-  - libopus=1.3.1=h7f98852_1
-  - libpng=1.6.39=h753d276_0
-  - libpq=15.1=hb675445_3
-  - libprotobuf=3.21.12=h3eb15da_0
-  - libsndfile=1.2.0=hb75c966_0
-  - libsolv=0.7.23=h3eb15da_0
-  - libsqlite=3.40.0=h753d276_0
-  - libssh2=1.10.0=hf14f497_3
-  - libstdcxx-ng=12.2.0=h46fd767_19
-  - libsystemd0=252=h2a991cd_0
-  - libtiff=4.5.0=h6adf6a1_2
-  - libtool=2.4.7=h27087fc_0
-  - libudev1=252=h166bdaf_0
-  - libuuid=2.32.1=h7f98852_1000
-  - libvorbis=1.3.7=h9c3ff4c_0
-  - libwebp-base=1.2.4=h166bdaf_0
-  - libxcb=1.13=h7f98852_1004
-  - libxkbcommon=1.0.3=he3ba5ed_0
-  - libxml2=2.10.3=h7463322_0
-  - libzlib=1.2.13=h166bdaf_4
-  - llvm-openmp=15.0.7=h0cdce71_0
-  - llvmdev=6.0.0=h1a6f6a4_4
-  - lz4-c=1.9.4=hcb278e6_0
-  - lzo=2.10=h516909a_1000
-  - mamba=1.2.0=py38haad2881_0
-  - markdown=3.4.3=pyhd8ed1ab_0
-  - markupsafe=2.1.3=py38h01eb140_0
-  - matplotlib=3.6.3=py38h578d9bd_0
-  - matplotlib-base=3.6.3=py38hd6c3c57_0
-  - mpg123=1.31.2=hcb278e6_0
-  - mpi=1.0=mpich
-  - mpi4py=3.1.4=py38h97ac3a3_0
-  - mpich=4.1.1=h846660c_100
-  - multidict=6.0.4=py38h1de0b5d_0
-  - munkres=1.1.4=pyh9f0ad1d_0
-  - mysql-common=8.0.32=ha901b37_0
-  - mysql-libs=8.0.32=hd7da12d_0
-  - ncurses=6.4=h6a678d5_0
-  - networkx=2.8.8=pyhd8ed1ab_0
-  - nspr=4.35=h27087fc_0
-  - nss=3.82=he02c5a1_0
-  - oauthlib=3.2.2=pyhd8ed1ab_0
-  - openjpeg=2.5.0=hfec8fc6_2
-  - openssl=3.2.1=hd590300_0
-  - ordered-set=4.1.0=pyhd8ed1ab_0
-  - packaging=23.0=pyhd8ed1ab_0
-  - pandas=2.0.3=py38h01efb38_1
-  - pcre2=10.40=hc3806b6_0
-  - pillow=9.4.0=py38hb32c036_0
-  - pip=22.3.1=py38h06a4308_0
-  - pixman=0.40.0=h36c2ea0_0
-  - plotly=5.13.0=py_0
-  - pluggy=1.0.0=pyhd8ed1ab_5
-  - ply=3.11=py_1
-  - pooch=1.7.0=pyha770c72_3
-  - protobuf=4.21.12=py38h8dc9893_0
-  - psutil=5.9.5=py38h1de0b5d_0
-  - pthread-stubs=0.4=h36c2ea0_1001
-  - pulseaudio=16.1=ha8d29e2_1
-  - pyasn1=0.4.8=py_0
-  - pyasn1-modules=0.2.7=py_0
-  - pybind11-abi=4=hd8ed1ab_3
-  - pybullet=3.21=py38h8f669ce_4
-  - pycosat=0.6.4=py38h0a891b7_1
-  - pycparser=2.21=pyhd8ed1ab_0
-  - pyjwt=2.7.0=pyhd8ed1ab_0
-  - pyopenssl=23.0.0=pyhd8ed1ab_0
-  - pyparsing=3.0.9=pyhd8ed1ab_0
-  - pyqt=5.15.7=py38ha0d8c90_3
-  - pyqt5-sip=12.11.0=py38h8dc9893_3
-  - pysocks=1.7.1=pyha2e5f31_6
-  - python=3.8.16=he550d4f_1_cpython
-  - python-dateutil=2.8.2=pyhd8ed1ab_0
-  - python-tzdata=2023.3=pyhd8ed1ab_0
-  - python_abi=3.8=2_cp38
-  - pytz=2023.3=pyhd8ed1ab_0
-  - pyu2f=0.1.5=pyhd8ed1ab_0
-  - qt-main=5.15.6=h602db52_6
-  - re2=2023.03.02=h8c504da_0
-  - readline=8.2=h5eee18b_0
-  - reproc=14.2.4=h0b41bf4_0
-  - reproc-cpp=14.2.4=hcb278e6_0
-  - requests=2.28.2=pyhd8ed1ab_0
-  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
-  - rsa=4.9=pyhd8ed1ab_0
-  - ruamel.yaml=0.17.21=py38h0a891b7_2
-  - ruamel.yaml.clib=0.2.7=py38h1de0b5d_1
-  - scipy=1.10.1=py38h59b608b_3
-  - setuptools=65.6.3=py38h06a4308_0
-  - sip=6.7.6=py38h8dc9893_0
-  - six=1.16.0=pyh6c4a22f_0
-  - sqlite=3.40.1=h5082296_0
-  - tabulate=0.9.0=pyhd8ed1ab_1
-  - tenacity=8.0.1=py38h06a4308_1
-  - tensorboard=2.13.0=pyhd8ed1ab_0
-  - tensorboard-data-server=0.7.0=py38h3d167d9_0
-  - tk=8.6.12=h1ccaba5_0
-  - toml=0.10.2=pyhd8ed1ab_0
-  - toolz=0.12.0=pyhd8ed1ab_0
-  - tornado=6.2=py38h0a891b7_1
-  - tqdm=4.64.1=pyhd8ed1ab_0
-  - typing-extensions=4.7.1=hd8ed1ab_0
-  - typing_extensions=4.7.1=pyha770c72_0
-  - unicodedata2=15.0.0=py38h0a891b7_0
-  - urllib3=1.26.14=pyhd8ed1ab_0
-  - werkzeug=2.3.6=pyhd8ed1ab_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xcb-util=0.4.0=h516909a_0
-  - xcb-util-image=0.4.0=h166bdaf_0
-  - xcb-util-keysyms=0.4.0=h516909a_0
-  - xcb-util-renderutil=0.3.9=h166bdaf_0
-  - xcb-util-wm=0.4.1=h516909a_0
-  - xmltodict=0.13.0=pyhd8ed1ab_0
-  - xorg-kbproto=1.0.7=h7f98852_1002
-  - xorg-libice=1.0.10=h7f98852_0
-  - xorg-libsm=1.2.3=hd9c2040_1000
-  - xorg-libx11=1.7.2=h7f98852_0
-  - xorg-libxau=1.0.9=h7f98852_0
-  - xorg-libxdmcp=1.1.3=h7f98852_0
-  - xorg-libxext=1.3.4=h7f98852_1
-  - xorg-libxrender=0.9.10=h7f98852_1003
-  - xorg-renderproto=0.11.1=h7f98852_1002
-  - xorg-xextproto=7.3.0=h7f98852_1002
-  - xorg-xproto=7.0.31=h7f98852_1007
-  - xz=5.2.10=h5eee18b_1
-  - yaml-cpp=0.7.0=h27087fc_2
-  - yarl=1.9.2=py38h01eb140_0
-  - zipp=3.16.0=pyhd8ed1ab_1
-  - zlib=1.2.13=h166bdaf_4
-  - zstandard=0.19.0=py38h5945529_1
-  - zstd=1.5.2=h3eb15da_6
-  - pip:
-      - appdirs==1.4.4
-      - casadi==3.6.5
-      - cfgv==3.4.0
-      - clarabel==0.7.1
-      - cloudpickle==1.6.0
-      - cmake==3.26.4
-      - cvxpy==1.4.3
-      - dict-deep==4.1.2
-      - distlib==0.3.8
-      - dm-tree==0.1.8
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - ecos==2.0.13
-      - exceptiongroup==1.2.0
-      - farama-notifications==0.0.4
-      - filelock==3.12.2
-      - fsspec==2024.2.0
-      - gitdb==4.0.11
-      - gitpython==3.1.42
-      - gputil==1.4.0
-      - gpytorch==1.11
-      - gym==0.19.0
-      - gymnasium==0.28.1
-      - heterocl==0.1
-      - identify==2.5.35
-      - imageio==2.31.2
-      - importlib-resources==6.1.3
-      - iniconfig==2.0.0
-      - jax-jumpy==1.0.0
-      - jaxtyping==0.2.19
-      - jinja2==3.1.2
-      - jsonschema==4.21.1
-      - jsonschema-specifications==2023.12.1
-      - lazy-loader==0.3
-      - linear-operator==0.5.2
-      - lit==16.0.6
-      - lz4==4.3.3
-      - markdown-it-py==3.0.0
-      - mdurl==0.1.2
-      - mosek==10.1.28
-      - mpmath==1.3.0
-      - msgpack==1.0.8
-      - munch==2.5.0
-      - mxnet-mkl==1.6.0
-      - nodeenv==1.8.0
-      - numpy==1.23.1
-      - nvidia-cublas-cu11==11.10.3.66
-      - nvidia-cuda-cupti-cu11==11.7.101
-      - nvidia-cuda-nvrtc-cu11==11.7.99
-      - nvidia-cuda-runtime-cu11==11.7.99
-      - nvidia-cudnn-cu11==8.5.0.96
-      - nvidia-cufft-cu11==10.9.0.58
-      - nvidia-curand-cu11==10.2.10.91
-      - nvidia-cusolver-cu11==11.4.0.1
-      - nvidia-cusparse-cu11==11.7.4.91
-      - nvidia-nccl-cu11==2.14.3
-      - nvidia-nvtx-cu11==11.7.91
-      - opencv-python-headless==4.8.1.78
-      - osqp==0.6.5
-      - path==16.10.0
-      - pkgutil-resolve-name==1.3.10
-      - platformdirs==4.2.0
-      - pre-commit==3.5.0
-      - pyaml==23.12.0
-      - pyarrow==15.0.1
-      - pybind11==2.12.0
-      - pycddlib==2.1.7
-      - pyglet==2.0.9
-      - pygments==2.17.2
-      - pyprind==2.11.3
-      - pytest==7.4.4
-      - python-graphviz==0.8.4
-      - pytope==0.0.4
-      - pyvirtualdisplay==3.0
-      - pywavelets==1.4.1
-      - pyyaml==6.0.1
-      - qdldl==0.1.7.post1
-      - ray==2.9.3
-      - referencing==0.33.0
-      - rich==13.7.1
-      - rpds-py==0.18.0
-      - safe-control-gym==2.0.0
-      - scikit-image==0.21.0
-      - scikit-learn==1.3.2
-      - scikit-optimize==0.9.0
-      - scs==3.2.4.post1
-      - sentry-sdk==1.43.0
-      - setproctitle==1.3.3
-      - shimmy==1.2.1
-      - shtab==1.7.1
-      - smmap==5.0.1
-      - stable-baselines3==2.1.0
-      - sympy==1.12
-      - tensorboardx==2.6.2.2
-      - termcolor==1.1.0
-      - theano==1.0.5
-      - threadpoolctl==3.4.0
-      - tifffile==2023.7.10
-      - tomli==2.0.1
-      - torch==1.13.1
-      - torchaudio==2.0.2
-      - torchvision==0.15.2
-      - transforms3d==0.4.1
-      - triton==2.0.0
-      - typeguard==2.13.3
-      - typer==0.9.0
-      - tyro==0.7.3
-      - virtualenv==20.25.1
-      - wandb==0.16.4
-prefix: /localhome/hha160/anaconda3/envs/quadrotor
diff --git a/wandb/run-20240624_130921-tkjintz8/files/config.yaml b/wandb/run-20240624_130921-tkjintz8/files/config.yaml
deleted file mode 100644
index b8acfce..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/config.yaml
+++ /dev/null
@@ -1,121 +0,0 @@
-wandb_version: 1
-
-exp_name:
-  desc: null
-  value: train_game
-seed:
-  desc: null
-  value: 42
-torch_deterministic:
-  desc: null
-  value: true
-cuda:
-  desc: null
-  value: true
-track:
-  desc: null
-  value: true
-wandb_project_name:
-  desc: null
-  value: ReachAvoidGame
-wandb_entity:
-  desc: null
-  value: null
-capture_video:
-  desc: null
-  value: false
-save_model:
-  desc: null
-  value: true
-upload_model:
-  desc: null
-  value: false
-hf_entity:
-  desc: null
-  value: ''
-env_id:
-  desc: null
-  value: reach_avoid
-total_timesteps:
-  desc: null
-  value: 20000000.0
-learning_rate:
-  desc: null
-  value: 0.0003
-num_envs:
-  desc: null
-  value: 4
-num_steps:
-  desc: null
-  value: 2048
-anneal_lr:
-  desc: null
-  value: true
-gamma:
-  desc: null
-  value: 0.99
-gae_lambda:
-  desc: null
-  value: 0.95
-num_minibatches:
-  desc: null
-  value: 32
-update_epochs:
-  desc: null
-  value: 10
-norm_adv:
-  desc: null
-  value: true
-clip_coef:
-  desc: null
-  value: 0.2
-clip_vloss:
-  desc: null
-  value: true
-ent_coef:
-  desc: null
-  value: 0.0
-vf_coef:
-  desc: null
-  value: 0.5
-max_grad_norm:
-  desc: null
-  value: 0.5
-target_kl:
-  desc: null
-  value: null
-batch_size:
-  desc: null
-  value: 8192
-minibatch_size:
-  desc: null
-  value: 256
-num_iterations:
-  desc: null
-  value: 2441
-_wandb:
-  desc: null
-  value:
-    python_version: 3.8.16
-    cli_version: 0.16.4
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    start_time: 1719259761.0
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.16
-      5: 0.16.4
-      8:
-      - 5
-      13: linux-x86_64
diff --git a/wandb/run-20240624_130921-tkjintz8/files/diff.patch b/wandb/run-20240624_130921-tkjintz8/files/diff.patch
deleted file mode 100644
index 18b40d4..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/diff.patch
+++ /dev/null
@@ -1,68 +0,0 @@
-diff --git a/safe_control_gym/envs/gym_game/BaseGame.py b/safe_control_gym/envs/gym_game/BaseGame.py
-index 68db391..4dc12e1 100644
---- a/safe_control_gym/envs/gym_game/BaseGame.py
-+++ b/safe_control_gym/envs/gym_game/BaseGame.py
-@@ -128,7 +128,7 @@ class BaseGameEnv(gym.Env):
-             defenders (np.ndarray): the initial positions of the defenders
-         '''
-         np.random.seed(self.initial_players_seed)
--    
-+        print(f"========== self.call_counter: {self.call_counter} in BaseGame.py. ==========\n")
-         # Map boundaries
-         min_val, max_val = -0.9, 0.9
-         
-@@ -192,13 +192,13 @@ class BaseGameEnv(gym.Env):
-                     return (new_point_x, new_point_y)
-         
-         # Calculate desired distance based on the counter
--        if self.call_counter < 128:  # 128 episodes
-+        if self.call_counter < 500:  # 2e5 steps 
-             distance = 0.15
-             r = 0.05
--        elif self.call_counter < 256:
-+        elif self.call_counter < 1000:  # around 4e5 steps
-             distance = 0.35
-             r = 0.15
--        elif self.call_counter < 512:
-+        elif self.call_counter < 1800:
-             distance = 0.75
-             r = 0.25
-         else:
-diff --git a/safe_control_gym/experiments/train_game.py b/safe_control_gym/experiments/train_game.py
-index 23c923e..e991d35 100644
---- a/safe_control_gym/experiments/train_game.py
-+++ b/safe_control_gym/experiments/train_game.py
-@@ -45,7 +45,7 @@ class Args:
-     # Algorithm specific arguments
-     env_id: str = "reach_avoid"
-     """the id of the environment"""
--    total_timesteps: int = 1e7
-+    total_timesteps: int = 2e7
-     """total timesteps of the experiments"""
-     learning_rate: float = 3e-4
-     """the learning rate of the optimizer"""
-diff --git a/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0 b/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0
-deleted file mode 100644
-index 0f43e0a..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/events.out.tfevents.1719252685.cs-mars-14.23354.0 and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/reward5.png b/training_results/game/ppo/42/10000000.0/reward5.png
-deleted file mode 100644
-index be3f8ec..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/reward5.png and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/reward5_init.png b/training_results/game/ppo/42/10000000.0/reward5_init.png
-deleted file mode 100644
-index 4dd0892..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/reward5_init.png and /dev/null differ
-diff --git a/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model b/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model
-deleted file mode 100644
-index afe5179..0000000
-Binary files a/training_results/game/ppo/42/10000000.0/train_game.cleanrl_model and /dev/null differ
-diff --git a/wandb/latest-run b/wandb/latest-run
-index b5cf86c..b7e7f2f 120000
---- a/wandb/latest-run
-+++ b/wandb/latest-run
-@@ -1 +1 @@
--run-20240624_111121-7fza559v
-\ No newline at end of file
-+run-20240624_130921-tkjintz8
-\ No newline at end of file
diff --git a/wandb/run-20240624_130921-tkjintz8/files/events.out.tfevents.1719259765.cs-mars-14.16617.0 b/wandb/run-20240624_130921-tkjintz8/files/events.out.tfevents.1719259765.cs-mars-14.16617.0
deleted file mode 120000
index 7877cb7..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/events.out.tfevents.1719259765.cs-mars-14.16617.0
+++ /dev/null
@@ -1 +0,0 @@
-/local-scratch/localhome/hha160/projects/safe-control-gym/training_results/game/ppo/42/20000000.0/events.out.tfevents.1719259765.cs-mars-14.16617.0
\ No newline at end of file
diff --git a/wandb/run-20240624_130921-tkjintz8/files/requirements.txt b/wandb/run-20240624_130921-tkjintz8/files/requirements.txt
deleted file mode 100644
index cd9ac8b..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/requirements.txt
+++ /dev/null
@@ -1,207 +0,0 @@
-Farama-Notifications==0.0.4
-GPUtil==1.4.0
-GitPython==3.1.42
-Jinja2==3.1.2
-Markdown==3.4.3
-MarkupSafe==2.1.3
-Mosek==10.1.28
-Pillow==9.4.0
-PyJWT==2.7.0
-PyPrind==2.11.3
-PyQt5-sip==12.11.0
-PyQt5==5.15.7
-PySocks==1.7.1
-PyVirtualDisplay==3.0
-PyWavelets==1.4.1
-PyYAML==6.0.1
-Pygments==2.17.2
-Shimmy==1.2.1
-Theano==1.0.5
-Werkzeug==2.3.6
-absl-py==1.4.0
-aiohttp==3.8.4
-aiosignal==1.3.1
-appdirs==1.4.4
-async-timeout==4.0.2
-attrs==23.1.0
-backports.functools-lru-cache==1.6.4
-blinker==1.6.2
-brotlipy==0.7.0
-cachetools==5.3.1
-casadi==3.6.5
-certifi==2024.2.2
-cffi==1.15.1
-cfgv==3.4.0
-charset-normalizer==2.1.1
-clarabel==0.7.1
-click==8.1.5
-cloudpickle==1.6.0
-cmake==3.26.4
-colorama==0.4.6
-conda-package-handling==2.0.2
-conda==22.11.1
-conda_package_streaming==0.7.0
-contourpy==1.0.7
-cryptography==39.0.0
-cvxpy==1.4.3
-cycler==0.11.0
-decorator==5.1.1
-dict-deep==4.1.2
-distlib==0.3.8
-disturbance-CrazyFlie-simulation==1.0
-dm-tree==0.1.8
-docker-pycreds==0.4.0
-docstring_parser==0.16
-ecos==2.0.13
-exceptiongroup==1.2.0
-filelock==3.12.2
-fonttools==4.38.0
-frozenlist==1.4.0
-fsspec==2024.2.0
-future==0.18.2
-gitdb==4.0.11
-google-auth-oauthlib==1.0.0
-google-auth==2.22.0
-gpytorch==1.11
-graphviz==0.8.4
-grpcio==1.54.2
-gym-pybullet-drones==1.0
-gym==0.19.0
-gymnasium==0.28.1
-heterocl==0.1
-hlib==0.1
-identify==2.5.35
-idna==3.4
-imageio==2.31.2
-importlib-metadata==6.8.0
-importlib_resources==6.1.3
-iniconfig==2.0.0
-jax-jumpy==1.0.0
-jaxtyping==0.2.19
-joblib==1.3.0
-jsonschema-specifications==2023.12.1
-jsonschema==4.21.1
-kiwisolver==1.4.4
-lazy_loader==0.3
-libmambapy==1.2.0
-linear-operator==0.5.2
-lit==16.0.6
-lz4==4.3.3
-mamba==1.2.0
-markdown-it-py==3.0.0
-matplotlib==3.6.3
-mdurl==0.1.2
-mpi4py==3.1.4
-mpmath==1.3.0
-msgpack==1.0.8
-multidict==6.0.4
-munch==2.5.0
-munkres==1.1.4
-mxnet-mkl==1.6.0
-networkx==2.8.8
-nodeenv==1.8.0
-numpy==1.23.1
-numpy==1.24.4
-nvidia-cublas-cu11==11.10.3.66
-nvidia-cuda-cupti-cu11==11.7.101
-nvidia-cuda-nvrtc-cu11==11.7.99
-nvidia-cuda-runtime-cu11==11.7.99
-nvidia-cudnn-cu11==8.5.0.96
-nvidia-cufft-cu11==10.9.0.58
-nvidia-curand-cu11==10.2.10.91
-nvidia-cusolver-cu11==11.4.0.1
-nvidia-cusparse-cu11==11.7.4.91
-nvidia-nccl-cu11==2.14.3
-nvidia-nvtx-cu11==11.7.91
-oauthlib==3.2.2
-odp==0.0
-opencv-python-headless==4.8.1.78
-ordered-set==4.1.0
-osqp==0.6.5
-packaging==23.0
-pandas==2.0.3
-path==16.10.0
-pip==22.3.1
-pkgutil_resolve_name==1.3.10
-platformdirs==4.2.0
-plotly==5.13.0
-pluggy==1.0.0
-ply==3.11
-pooch==1.7.0
-pre-commit==3.5.0
-protobuf==4.21.12
-psutil==5.9.5
-pyOpenSSL==23.0.0
-pyaml==23.12.0
-pyarrow==15.0.1
-pyasn1-modules==0.2.7
-pyasn1==0.4.8
-pybind11==2.12.0
-pybullet==3.2.0
-pycddlib==2.1.7
-pycosat==0.6.4
-pycparser==2.21
-pyglet==2.0.9
-pyparsing==3.0.9
-pytest==7.4.4
-python-dateutil==2.8.2
-pytope==0.0.4
-pytz==2023.3
-pyu2f==0.1.5
-qdldl==0.1.7.post1
-ray==2.9.3
-referencing==0.33.0
-requests-oauthlib==1.3.1
-requests==2.28.2
-rich==13.7.1
-rpds-py==0.18.0
-rsa==4.9
-ruamel.yaml.clib==0.2.7
-ruamel.yaml==0.17.21
-safe-control-gym==2.0.0
-scikit-image==0.21.0
-scikit-learn==1.3.2
-scikit-optimize==0.9.0
-scipy==1.10.1
-scs==3.2.4.post1
-sentry-sdk==1.43.0
-setproctitle==1.3.3
-setuptools==65.6.3
-shtab==1.7.1
-sip==6.7.6
-six==1.16.0
-smmap==5.0.1
-stable-baselines3==2.1.0
-sympy==1.12
-tabulate==0.9.0
-tenacity==8.0.1
-tensorboard-data-server==0.7.0
-tensorboard==2.13.0
-tensorboardX==2.6.2.2
-termcolor==1.1.0
-threadpoolctl==3.4.0
-tifffile==2023.7.10
-toml==0.10.2
-tomli==2.0.1
-toolz==0.12.0
-torch==1.13.1
-torchaudio==2.0.2
-torchvision==0.15.2
-tornado==6.2
-tqdm==4.64.1
-transforms3d==0.4.1
-triton==2.0.0
-typeguard==2.13.3
-typer==0.9.0
-typing_extensions==4.7.1
-tyro==0.7.3
-tzdata==2023.3
-unicodedata2==15.0.0
-urllib3==1.26.14
-virtualenv==20.25.1
-wandb==0.16.4
-wheel==0.37.1
-xmltodict==0.13.0
-yarl==1.9.2
-zipp==3.16.0
-zstandard==0.19.0
\ No newline at end of file
diff --git a/wandb/run-20240624_130921-tkjintz8/files/wandb-metadata.json b/wandb/run-20240624_130921-tkjintz8/files/wandb-metadata.json
deleted file mode 100644
index 089214b..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/wandb-metadata.json
+++ /dev/null
@@ -1,207 +0,0 @@
-{
-    "os": "Linux-5.4.0-146-generic-x86_64-with-glibc2.10",
-    "python": "3.8.16",
-    "heartbeatAt": "2024-06-24T20:09:21.367909",
-    "startedAt": "2024-06-24T20:09:21.020399",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/localhome/hha160/projects/safe-control-gym/safe_control_gym/experiments/train_game.py",
-    "codePathLocal": null,
-    "git": {
-        "remote": "git@github.com:Hu-Hanyang/safe-control-gym.git",
-        "commit": "80efbfd459cf287f905f642c8b57d99949a3c732"
-    },
-    "email": "HuHY97@outlook.com",
-    "root": "/local-scratch/localhome/hha160/projects/safe-control-gym",
-    "host": "cs-mars-14",
-    "username": "hha160",
-    "executable": "/localhome/hha160/anaconda3/envs/quadrotor/bin/python",
-    "cpu_count": 16,
-    "cpu_count_logical": 32,
-    "cpu_freq": {
-        "current": 3902.9811249999993,
-        "min": 2200.0,
-        "max": 3400.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 3598.601,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3598.899,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3604.982,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3594.706,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3598.468,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3595.523,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4501.681,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3589.069,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.651,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4493.325,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4504.945,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4459.204,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.513,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3598.274,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4501.212,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3931.792,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3587.285,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3585.399,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.868,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3595.703,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.013,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3594.025,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4511.768,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3616.209,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3600.113,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4501.606,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4501.184,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4497.823,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.924,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 3599.807,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4485.068,
-            "min": 2200.0,
-            "max": 3400.0
-        },
-        {
-            "current": 4050.756,
-            "min": 2200.0,
-            "max": 3400.0
-        }
-    ],
-    "disk": {
-        "/": {
-            "total": 116.77696228027344,
-            "used": 83.83425521850586
-        }
-    },
-    "gpu": "NVIDIA GeForce RTX 3080",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 3080",
-            "memory_total": 10737418240
-        }
-    ],
-    "memory": {
-        "total": 31.33232879638672
-    }
-}
diff --git a/wandb/run-20240624_130921-tkjintz8/files/wandb-summary.json b/wandb/run-20240624_130921-tkjintz8/files/wandb-summary.json
deleted file mode 100644
index 215a175..0000000
--- a/wandb/run-20240624_130921-tkjintz8/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 19996672, "_timestamp": 1719270967.8693922, "_runtime": 11206.846627235413, "_step": 121020, "charts/episodic_return": -120.29470825195312, "charts/episodic_length": 178.0, "charts/learning_rate": 1.2290044537621725e-07, "losses/value_loss": 0.016744464635849, "losses/policy_loss": 0.00015100650489330292, "losses/entropy": -4.457248687744141, "losses/old_approx_kl": -0.0007113930769264698, "losses/approx_kl": 1.191697083413601e-05, "losses/clipfrac": 0.0, "losses/explained_variance": 0.9652240872383118, "charts/SPS": 1785.0, "_wandb": {"runtime": 11206}}
\ No newline at end of file
